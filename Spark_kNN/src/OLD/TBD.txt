package org.cusp.bdi.sknn

import org.apache.spark.Partitioner
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel
import org.apache.spark.util.SizeEstimator
import org.cusp.bdi.ds.SpatialIndex.buildRectBounds
import org.cusp.bdi.ds._
import org.cusp.bdi.ds.geom.{Circle, Geom2D, Point, Rectangle}
import org.cusp.bdi.ds.kdt.{KdTree, KdtBranchRootNode, KdtLeafNode, KdtNode}
import org.cusp.bdi.ds.qt.QuadTree
import org.cusp.bdi.ds.sortset.{Node, SortedLinkedList}
import org.cusp.bdi.sknn.SparkKnn.{INITIAL_GRID_WIDTH, SHUFFLE_PARTITION_MAX_BYTE_SIZE}
import org.cusp.bdi.sknn.ds.util.{GlobalIndexPointUserData, SpatialIdxOperations, SupportedSpatialIndexes}
import org.cusp.bdi.util.Helper

import scala.collection.mutable
import scala.collection.mutable.{ArrayBuffer, ListBuffer}
import scala.util.Random

object SparkKnn extends Serializable {

  val INITIAL_GRID_WIDTH = 1e2
  val SHUFFLE_PARTITION_MAX_BYTE_SIZE: Double = 2e9
  //  val EXECUTOR_MEM_CACHE_RATE: Double = 0.50
  val COST_INT: Long = 4

  //  def firstIdxGT(arrCounts: Array[Int], prevMinVal: Int): Int = {
  //
  //    var minIdx = 0
  //
  //    for (i <- arrCounts.indices) {
  //      if (arrCounts(i) <= prevMinVal)
  //        return i
  //      else if (arrCounts(i) < arrCounts(minIdx))
  //        minIdx = i
  //    }
  //
  //    minIdx
  //  }

  def getSparkKNNClasses: Array[Class[_]] =
    Array(
      Helper.getClass,
      SupportedKnnOperations.getClass,
      classOf[SortedLinkedList[_]],
      classOf[Rectangle],
      classOf[Circle],
      classOf[Point],
      classOf[MBRInfo],
      classOf[Geom2D],
      classOf[GlobalIndexPointUserData],
      classOf[KdTree],
      classOf[QuadTree],
      KdTree.getClass,
      classOf[KdtNode],
      classOf[KdtBranchRootNode],
      classOf[KdtLeafNode],
      classOf[SparkKnn],
      SparkKnn.getClass,
      SupportedSpatialIndexes.getClass,
      SpatialIndex.getClass,
      classOf[SpatialIndex],
      classOf[Node[_]],
      classOf[ListBuffer[_]],
      classOf[mutable.Stack[MBRInfo]])
}

case class SparkKnn(debugMode: Boolean, spatialIndexType: SupportedSpatialIndexes.Value, rddLeft: RDD[Point], rddRight: RDD[Point], k: Int /*, gridWidth: Int*/) extends Serializable {

  val lstDebugInfo: ListBuffer[String] = ListBuffer()

  def knnJoin(): RDD[(Point, Iterable[(Double, Point)])] = {

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>knnJoin", lstDebugInfo)

    val (partObjCapacityRight, mbrInfoRight, gridWidthRight) = computeCapacity(rddRight, rddLeft, isAllKnn = false)

    knnJoinExecute(rddLeft, rddRight, mbrInfoRight, partObjCapacityRight, gridWidthRight)
  }

  def allKnnJoin(): RDD[(Point, Iterable[(Double, Point)])] = {

    val (partObjCapacityRight, mbrInfoRight, gridWidthRight) = computeCapacity(rddRight, rddLeft, isAllKnn = true)
    val (partObjCapacityLeft, mbrInfoLeft, gridWidthLeft) = computeCapacity(rddLeft, rddRight, isAllKnn = true)

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>All kNN partObjCapacityLeft=%s partObjCapacityRight=%s".format(partObjCapacityLeft, partObjCapacityRight), lstDebugInfo)

    knnJoinExecute(rddRight, rddLeft, mbrInfoLeft, partObjCapacityLeft, gridWidthLeft)
      .union(knnJoinExecute(rddLeft, rddRight, mbrInfoRight, partObjCapacityRight, gridWidthRight))
  }

  private def knnJoinExecute(rddActiveLeft: RDD[Point],
                             rddActiveRight: RDD[Point], mbrDS_ActiveRight: MBRInfo,
                             partObjCapacity: (Long, Long), gridWidthRightActive: Int): RDD[(Point, Iterable[(Double, Point)])] = {

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>knnJoinExecute", lstDebugInfo)

    def computeSquareXY_Coord(x: Double, y: Double): (Double, Double) = (((x - mbrDS_ActiveRight.left) / gridWidthRightActive).toInt, ((y - mbrDS_ActiveRight.bottom) / gridWidthRightActive).toInt)

    def computeSquareXY_Point(point: Point): (Double, Double) = computeSquareXY_Coord(point.x, point.y)

    var startTime = System.currentTimeMillis
    var bvGlobalIndexRight: Broadcast[SpatialIndex] = null
    var bvArrPartitionMBRs: Broadcast[Array[MBRInfo]] = null

    {
      val stackRangeInfo = mutable.Stack[MBRInfo]()
      var totalWeight = 0L
      var partCounter = -1

      // build range info
      val iterGlobalIndexObjects = rddActiveRight
        .mapPartitions(_.map(point => (computeSquareXY_Point(point), 1L))) // grid assignment
        .reduceByKey(_ + _) // summarize
        .sortByKey() // sorts by (x, y)
        .mapPartitions(_.map(row => new Point(row._1, new GlobalIndexPointUserData(row._2))))
        .toLocalIterator
        .map(point => { // group cells on partitions

          val globalIndexPointUserData = point.userData match {
            case globalIndexPointUserData: GlobalIndexPointUserData => globalIndexPointUserData
          }

          val newWeight = totalWeight + globalIndexPointUserData.numPoints

          if (totalWeight == 0 || totalWeight > partObjCapacity._1 || newWeight > partObjCapacity._2) {

            partCounter += 1
            totalWeight = globalIndexPointUserData.numPoints
            stackRangeInfo.push(new MBRInfo(point.x, point.y))
          }
          else {

            totalWeight = newWeight
            stackRangeInfo.top.right = point.x

            if (point.y < stackRangeInfo.top.bottom)
              stackRangeInfo.top.bottom = point.y
            else if (point.y > stackRangeInfo.top.top)
              stackRangeInfo.top.top = point.y
          }

          globalIndexPointUserData.partitionIdx = partCounter

          point
        })

      Helper.loggerSLf4J(debugMode, SparkKnn, ">>rangeInfo time in %,d MS.".format(System.currentTimeMillis - startTime), lstDebugInfo)

      startTime = System.currentTimeMillis

      // create global index
      val glbIdx_ActiveRight: SpatialIndex = SupportedSpatialIndexes(spatialIndexType)
      glbIdx_ActiveRight.insert(buildRectBounds(computeSquareXY_Coord(mbrDS_ActiveRight.left, mbrDS_ActiveRight.bottom), computeSquareXY_Coord(mbrDS_ActiveRight.right, mbrDS_ActiveRight.top)), iterGlobalIndexObjects, 1)

      Helper.loggerSLf4J(debugMode, SparkKnn, ">>GlobalIndex insert time in %,d MS. Grid size: (%,d X %,d)\tIndex: %s\tIndex Size: %,d".format(System.currentTimeMillis - startTime, gridWidthRightActive, gridWidthRightActive, glbIdx_ActiveRight, -1 /*SizeEstimator.estimate(glbIdx_ActiveRight)*/), lstDebugInfo)

      bvGlobalIndexRight = rddActiveRight.context.broadcast(glbIdx_ActiveRight)
      bvArrPartitionMBRs = rddActiveRight.context.broadcast(stackRangeInfo.map(_.stretch()).toArray)

      stackRangeInfo.foreach(row =>
        Helper.loggerSLf4J(debugMode, SparkKnn, ">>\t%s".format(row.toString), lstDebugInfo))
    }

    startTime = System.currentTimeMillis

    val numRounds = rddActiveLeft
      .mapPartitions(_.map(point => (computeSquareXY_Point(point), null)))
      .reduceByKey((_, _) => null)
      .mapPartitions(iter => Iterator(iter.map(row => SpatialIdxOperations.extractLstPartition(bvGlobalIndexRight.value, row._1, k).length).max))
      .max

    //    val arrPartObjCounts = rddActiveLeft
    //      .mapPartitions(_.map(point => (computeSquareXY_Point(point), 1L)))
    //      .reduceByKey(_ + _)
    //      .mapPartitions(_.map(row => {
    //
    //        val arrPartitionId = SpatialIdxOperations.extractLstPartition(bvGlobalIndexRight.value, row._1, k).sorted
    //println(">>>\t"+row._2 + "\t"+ arrPartitionId.mkString("\t"))
    //        // map to indexes for counting
    //        val arrPartitionIdCount = ArrayBuffer.fill(arrPartitionId(arrPartitionId.length - 1) + 1)(0L)
    //
    //        arrPartitionId.foreach(pId => arrPartitionIdCount(pId) = row._2)
    //
    //        (arrPartitionId.length, arrPartitionIdCount)
    //      }))
    //      .treeReduce((row1, row2) => {
    //
    //        row1._2.sizeHint(row2._2.length)
    //
    //        for (i <- 0 until row2._2.length)
    //          if (i < row1._2.length)
    //            row1._2(i) += row2._2(i)
    //          else
    //            row1._2 += row2._2(i)
    //
    //        (Helper.max(row1._1, row2._1), row1._2)
    //      })
    //
    //    val numRounds = arrPartObjCounts._1

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>LeftDS numRounds done. numRounds: %,d time in %,d MS".format(numRounds, System.currentTimeMillis - startTime), lstDebugInfo)

    val actualNumPartitions = bvArrPartitionMBRs.value.length

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>Actual number of partitions: %,d".format(actualNumPartitions), lstDebugInfo)

    // build a spatial index on each partition
    val rddSpIdx = rddActiveRight
      .mapPartitions(_.map(point => (bvGlobalIndexRight.value.findExact(computeSquareXY_Point(point)).userData match {
        case globalIndexPoint: GlobalIndexPointUserData => globalIndexPoint.partitionIdx
      }, point)))
      .partitionBy(new Partitioner() {

        override def numPartitions: Int = actualNumPartitions

        override def getPartition(key: Any): Int =
          key match {
            case pIdx: Int =>
              //              if (pIdx < 0) -pIdx - 1 else pIdx // -1 to counterpart the +1 for cases of -0
              if (pIdx == -1) Random.nextInt(numPartitions) else pIdx
          }
      })
      .mapPartitionsWithIndex((pIdx, iter) => { // build spatial index

        val startTime = System.currentTimeMillis
        val mbrInfo = bvArrPartitionMBRs.value(actualNumPartitions - pIdx - 1)

        val minX = mbrInfo.left * gridWidthRightActive + mbrDS_ActiveRight.left
        val minY = mbrInfo.bottom * gridWidthRightActive + mbrDS_ActiveRight.bottom
        val maxX = mbrInfo.right * gridWidthRightActive + mbrDS_ActiveRight.right + gridWidthRightActive
        val maxY = mbrInfo.top * gridWidthRightActive + mbrDS_ActiveRight.top + gridWidthRightActive

        val spatialIndex = SupportedSpatialIndexes(spatialIndexType)

        spatialIndex.insert(buildRectBounds(minX, minY, maxX, maxY), iter.map(_._2), gridWidthRightActive)

        Helper.loggerSLf4J(debugMode, SparkKnn, ">>SpatialIndex on partition %,d time in %,d MS. Index: %s\tTotal Size: %,d".format(pIdx, System.currentTimeMillis - startTime, spatialIndex, -1 /*SizeEstimator.estimate(spatialIndex)*/), lstDebugInfo)

        val anyRef: AnyRef = spatialIndex

        Iterator((pIdx, anyRef))
      }, preservesPartitioning = true)
      .persist(StorageLevel.MEMORY_ONLY)

    var rddPoint: RDD[(Int, AnyRef)] = rddActiveLeft
      .mapPartitions(_.map(point => {

        var arrPartitionId = SpatialIdxOperations.extractLstPartition(bvGlobalIndexRight.value, computeSquareXY_Point(point), k)

        if (arrPartitionId.length < numRounds) {

          arrPartitionId.sizeHint(numRounds)

          // insures that the first round doesn't perform any kNN computation unless the object needs all rounds
          // this minimizes memory usage while the global index is present
          //            arrPartitionId.insert(0, -1)

          //          random - space the list but maintain order
          while (arrPartitionId.length < numRounds)
            arrPartitionId.insert(Random.nextInt(arrPartitionId.length + 1), -1)

          // trim trailing -1s
          arrPartitionId = arrPartitionId.slice(0, arrPartitionId.lastIndexWhere(_ != -1) + 1)
        }

        (arrPartitionId.head, new RowData(point, new SortedLinkedList[Point](k), arrPartitionId.tail))

        //          val rowData = new RowData(point, new SortedLinkedList[Point](k), arrPartitionId)
        //
        //          if (rowData.arrPartitionId.length == numRounds) {
        //
        //            val nextParIdX = rowData.nextPartId
        //
        //            arrCounts(nextParIdX) += 1
        //
        //            (nextParIdX, rowData)
        //          } else {
        //
        //            val nextParIdX = firstIdxGT(arrCounts, prevMin)
        //
        //            prevMin = arrCounts(nextParIdX)
        //
        //            arrCounts(nextParIdX) += 1
        //
        //            (-(nextParIdX + 1), rowData) // +1 since there is no -0
        //          }
      }))

    (1 to numRounds).foreach(currRoundNum => {

      rddPoint = (rddSpIdx ++ rddPoint.partitionBy(rddSpIdx.partitioner.get))
        .mapPartitionsWithIndex((pIdx, iter) => {

          var counter = 0L

          // first entry is always the spatial index
          var spatialIndex: SpatialIndex = iter.next._2 match {
            case spIdx: SpatialIndex => spIdx
          }

          iter.map(row =>
            row._2 match {
              case rowData: RowData =>

                counter += 1

                if (row._1 != -1)
                  spatialIndex.nearestNeighbor(rowData.point, rowData.sortedList)

                if (!iter.hasNext) {

                  spatialIndex = null // GC the index

                  Helper.loggerSLf4J(debugMode, SparkKnn, ">>kNN done index: %,d roundNum: %,d numPoints: %,d".format(pIdx, currRoundNum, counter), lstDebugInfo)
                }

                (rowData.nextPartId, rowData)

              //                if (roundsRemain == 0)
              //                  row
              //                else if (rowData.arrPartitionId.isEmpty || rowData.arrPartitionId.length < roundsRemain) {
              //
              //                  val nextParIdX = firstIdxGT(arrCounts, prevMin)
              //
              //                  prevMin = arrCounts(nextParIdX)
              //                  arrCounts(nextParIdX) += 1
              //
              //                  (-(nextParIdX + 1), rowData) // +1 since there is no -0
              //                }
              //                else {
              //
              //                  val nextParIdX = rowData.nextPartId
              //
              //                  arrCounts(nextParIdX) += 1
              //
              //                  (nextParIdX, rowData)
              //                }

              //                (rowData.nextPartId, rowData)
            })
        })

      if (currRoundNum == 1) {

        bvGlobalIndexRight.unpersist(true)
        bvArrPartitionMBRs.unpersist(true)

        //        bvGlobalIndexRight = null
        //        bvArrPartitionMBRs = null
      }
    })

    rddPoint
      .mapPartitions(_.map(_._2 match {
        case rowData: RowData => (rowData.point, rowData.sortedList.map(nd => (math.sqrt(nd.distance), nd.data)))
      }))
  }

  private def computeCapacity(rddRight: RDD[Point], rddLeft: RDD[Point], isAllKnn: Boolean): ((Long, Long), MBRInfo, Int) = {

    val numExecutors = rddRight.context.getConf.get("spark.executor.instances").toInt
    val execAssignedMem = Helper.toByte(rddRight.context.getConf.get("spark.executor.memory"))
    val execOverheadMem = Helper.max(384e6, 0.1 * execAssignedMem).toLong // 10% reduction in memory to account for yarn overhead
    val numCoresPerExecutor = rddRight.context.getConf.get("spark.executor.cores").toInt
    val coreAvailMem = (execAssignedMem - execOverheadMem) /* * EXECUTOR_MEM_CACHE_RATE */ / numCoresPerExecutor
    val partitionAssignedMem = Helper.min(coreAvailMem, SHUFFLE_PARTITION_MAX_BYTE_SIZE)
    val totalAvailCores = numExecutors * numCoresPerExecutor

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>numExecutors: %,d execAssignedMem: %,d numCoresPerExecutor: %,d totalAvailCores: %,d coreAvailMem: %,d partitionAssignedMem: %,.2f"
      .format(numExecutors, execAssignedMem, numCoresPerExecutor, totalAvailCores, coreAvailMem, partitionAssignedMem), lstDebugInfo)

    var startTime = System.currentTimeMillis

    val (costMaxPointRight, mbrInfoRight, maxGridCellCount, rowCountRight) = rddRight
      .mapPartitions(_.map(point => ((math.floor(point.x / INITIAL_GRID_WIDTH).toInt, math.floor(point.y / INITIAL_GRID_WIDTH).toInt), (SizeEstimator.estimate(point), new MBRInfo(point.x.toFloat, point.y.toFloat), 1L))))
      .reduceByKey((row1, row2) => (Helper.max(row1._1, row2._1), row1._2.merge(row2._2), row1._3 + row2._3))
      .mapPartitions(_.map(row => (row._2._1, row._2._2, row._2._3, row._2._3)))
      .treeReduce((row1, row2) => (Helper.max(row1._1, row2._1), row1._2.merge(row2._2), Helper.max(row1._3, row2._3), row1._4 + row2._4)
      )

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>Right DS info done costMaxPointRight: %,d mbrInfoRight: %s maxGridCellCount: %,d rowCountRight: %,d Time: %,d MS"
      .format(costMaxPointRight, mbrInfoRight, maxGridCellCount, rowCountRight, System.currentTimeMillis - startTime), lstDebugInfo)

    val costRect = SizeEstimator.estimate(Rectangle(new Geom2D(), new Geom2D()))

    /*
     * Right dataset (Spatial Indexes) size estimate. Every row in the right RDD contains:
     * 1. partition ID (int) <- insignificant here, so it's not accounted for.
     * 2. spatial index: # of nodes with objects
    */
    val spatialIndexMockRight = SupportedSpatialIndexes(spatialIndexType)
    val costSpatialIndexMockRight = SizeEstimator.estimate(spatialIndexMockRight)
    spatialIndexMockRight.insert(new Rectangle(new Geom2D(Double.MaxValue / 2)), (0 until spatialIndexMockRight.nodeCapacity).map(i => new Point(i, i)).iterator, 1)
    val costSpIdxInsertOverhead = (SizeEstimator.estimate(spatialIndexMockRight) - costSpatialIndexMockRight - costRect) / spatialIndexMockRight.nodeCapacity.toDouble

    val estimateNodeCount = spatialIndexMockRight.estimateNodeCount(rowCountRight)

    val costRightRDD = (rowCountRight * costMaxPointRight) + (estimateNodeCount * (costSpatialIndexMockRight + costRect + costSpIdxInsertOverhead))

    val numPartitionsRight = math.ceil(costRightRDD / partitionAssignedMem).toInt

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>Right DS costSpatialIndexMockRight: %,d costSpIdxInsertOverhead: %,.2f costRect: %,d estimateNodeCount: %,d costRightRDD: %,.2f numPartitionsRight: %,d"
      .format(costSpatialIndexMockRight, costSpIdxInsertOverhead, costRect, estimateNodeCount, costRightRDD, numPartitionsRight), lstDebugInfo)

    /*
     * every row in the left RDD contains:
     *   1. partition ID (int)
     *   2. point info (RowData -> (point, SortedLinkedList, list of partitions to visit))
     *
     *   point: coordinates + userData <- from raw file
     *   SortedLinkedList: point matches from the right RDD of size up to k
     */

    startTime = System.currentTimeMillis

    val (costMaxPointLeft, rowCountLeft) = rddLeft.mapPartitions(_.map(point => (SizeEstimator.estimate(point), 1L)))
      .treeReduce((row1, row2) => (Helper.max(row1._1, row2._1), row1._2 + row2._2))

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>Left DS info done costMaxPointLeft: %,d rowCountLeft: %,d Time: %,d MS"
      .format(costMaxPointLeft, rowCountLeft, System.currentTimeMillis - startTime), lstDebugInfo)

    val arrPartIdMockLeft = ArrayBuffer.fill[Int](Helper.max(numPartitionsRight, totalAvailCores))(0)
    val costArrPartIdLeft = SizeEstimator.estimate(arrPartIdMockLeft)

    val sortLstMockLeft = new SortedLinkedList[Point](k) // at the end of the process, the SortedLinkedList contains k points from the right DS
    var costSortLstLeft = SizeEstimator.estimate(sortLstMockLeft)

    sortLstMockLeft.add(0, Point())
    val costSortLstInsertOverhead = SizeEstimator.estimate(sortLstMockLeft) - SizeEstimator.estimate(Point()) - costSortLstLeft + costMaxPointRight
    costSortLstLeft += k * costSortLstInsertOverhead

    val tupleMockLeft = (0, new RowData())
    val costTupleObjLeft = SizeEstimator.estimate(tupleMockLeft) + costMaxPointLeft + costArrPartIdLeft + costSortLstLeft

    val costLeftRDD = rowCountLeft * costTupleObjLeft
    val numPartitionsLeft = math.ceil(costLeftRDD / partitionAssignedMem)

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>Left DS costMaxPointLeft: %,d costSortLstLeft: %,d costSortLstInsertOverhead: %,d costTupleObjLeft: %,d costLeftRDD: %,d numPartitionsLeft: %,.2f"
      .format(costMaxPointLeft, costSortLstLeft, costSortLstInsertOverhead, costTupleObjLeft, costLeftRDD, numPartitionsLeft), lstDebugInfo)

    val numPartitionsMin = Helper.max(numPartitionsRight, numPartitionsLeft).toInt
    val numPartitionsPreferred =
      if (numPartitionsMin % totalAvailCores == 0)
        numPartitionsMin
      else
        math.ceil(numPartitionsMin / totalAvailCores.toDouble) * totalAvailCores

    val coreObjCapacityRight = (math.ceil(rowCountRight / numPartitionsPreferred).toLong, rowCountRight / numPartitionsMin)

    val rate = maxGridCellCount / coreObjCapacityRight._1.toDouble

    val gridWidth = math.ceil(if (rate > 1) INITIAL_GRID_WIDTH / rate else INITIAL_GRID_WIDTH).toInt

    Helper.loggerSLf4J(debugMode, SparkKnn, ">>costLeftRDD: %,d numPartitionsMin: %,d numPartitionsPreferred: %,.2f coreObjCapacityRight: %s rate: %,.4f gridWidth: %,d"
      .format(costLeftRDD, numPartitionsMin, numPartitionsPreferred, coreObjCapacityRight, rate, gridWidth), lstDebugInfo)

    //        System.exit(-555)
    (coreObjCapacityRight, mbrInfoRight.stretch(), gridWidth)
  }
}