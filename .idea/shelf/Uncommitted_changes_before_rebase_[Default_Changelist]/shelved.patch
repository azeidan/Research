Index: FW_GeoMatch/src/main/scala/GitHub_Example.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import org.cusp.bdi.gm.GeoMatch\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.serializer.KryoSerializer\nimport org.cusp.bdi.gm.geom.GMPoint\nimport org.cusp.bdi.gm.geom.GMLineString\n\nobject GitHub_Example {\n    def main(args: Array[String]): Unit = {\n\n        // spark initialization including Kryo\n        val sparkConf = new SparkConf()\n            .setAppName(\"GeoMatch_Test\")\n            .set(\"spark.serializer\", classOf[KryoSerializer].getName)\n            .registerKryoClasses(GeoMatch.getGeoMatchClasses())\n\n        // assuming local run\n        sparkConf.setMaster(\"local[*]\")\n\n        val sparkContext = new SparkContext(sparkConf)\n\n        // The first dataset.\n        val rddFirstSet = sparkContext.textFile(\"firstDataset.csv\")\n            .mapPartitions(_.map(line => {\n\n                // parse the line and form the spatial object\n                val parts = line.split(',')\n\n                val arrCoords = parts.slice(1, parts.length)\n                    .map(xyStr => {\n                        val xy = xyStr.split(' ')\n\n                        (xy(0).toDouble.toInt, xy(1).toDouble.toInt)\n                    })\n\n                // create the spatial object GMLineString. The first parameter is the payload\n                // the second parameter is the list of coordinates that form the LineString\n                new GMLineString(parts(0), arrCoords)\n            }))\n\n        // The second dataset.\n        val rddSecondSet = sparkContext.textFile(\"secondDataset.csv\")\n            .mapPartitions(_.map(line => {\n\n                // parse the line and form the spatial object\n                val parts = line.split(',')\n\n                // create the spatial object GMPoint. The first parameter is the payload\n                // the second parameter is the point's coordinates\n                new GMPoint(parts(0), (parts(1).toDouble.toInt, parts(2).toDouble.toInt))\n            }))\n\n        // initialize GeoMatch\n        val geoMatch = new GeoMatch(false, 256, 150, (-1, -1, -1, -1))\n\n        // perform map-matching\n        val resultRDD = geoMatch.spatialJoinKNN(rddFirstSet, rddSecondSet, 3, false)\n\n        // print the results\n        resultRDD.mapPartitions(_.map(row => println(\"%-10s%s\".format(row._1.payload, row._2.map(_.payload).mkString(\",\")))))\n            .collect()\n    }\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- FW_GeoMatch/src/main/scala/GitHub_Example.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ FW_GeoMatch/src/main/scala/GitHub_Example.scala	(date 1594705270491)
@@ -1,9 +1,4 @@
-import org.cusp.bdi.gm.GeoMatch
-import org.apache.spark.SparkConf
-import org.apache.spark.SparkContext
-import org.apache.spark.serializer.KryoSerializer
-import org.cusp.bdi.gm.geom.GMPoint
-import org.cusp.bdi.gm.geom.GMLineString
+
 
 object GitHub_Example {
     def main(args: Array[String]): Unit = {
Index: SpatialBenchmark/pom.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/pom.xml	(date 1594757046958)
+++ SpatialBenchmark/pom.xml	(date 1594757046958)
@@ -0,0 +1,43 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+  <groupId>SpatialBenchmark</groupId>
+  <artifactId>SpatialBenchmark</artifactId>
+  <version>0.0.1-SNAPSHOT</version>
+  <name>SpatialBenchmark</name>
+  <description>SpatialBenchmark</description>
+
+  <dependencies>
+
+    <dependency>
+      <groupId>Common</groupId>
+      <artifactId>Common</artifactId>
+      <version>0.2</version>
+      <scope>compile</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>LocalRunFiles</groupId>
+      <artifactId>LocalRunFiles</artifactId>
+      <version>0.0.1-SNAPSHOT</version>
+      <scope>compile</scope>
+    </dependency>
+
+  </dependencies>
+
+  <build>
+    <sourceDirectory>src/main/scala</sourceDirectory>
+    <plugins>
+      <plugin>
+        <artifactId>maven-compiler-plugin</artifactId>
+        <version>3.3</version>
+        <configuration>
+          <source>1.8</source>
+          <target>1.8</target>
+        </configuration>
+      </plugin>
+    </plugins>
+  </build>
+</project>
\ No newline at end of file
Index: Spark_kNN/src/main/scala/org/cusp/bdi/sknn/util/AssignToPartitions.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.cusp.bdi.sknn.util\n\nimport scala.collection.mutable.ListBuffer\n\nimport org.cusp.bdi.sknn.QTPartId\n\n/**\n  * Based on Maximum Subarray algorithm (Kadane's)\n  */\ncase class AssignToPartitions(arrAttrQT: Array[QTPartId], maxSum: Long) {\n\n    private var partCounter = 0\n\n    // group of quadtrees adding to the maximum sum\n    private val lstGroup = ListBuffer[QTPartId]()\n\n    def getPartitionCount() = partCounter\n\n    // 1st, assign quadtrees with maximum total points\n    arrAttrQT.foreach(qtInf =>\n        if (qtInf.totalPoints >= maxSum) {\n\n            qtInf.assignedPart = partCounter\n            partCounter += 1\n        })\n\n    // remaining unassigned quadtrees\n    var arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)\n\n    while (!arrUnassigned.isEmpty) {\n\n        var maxSoFar = 0L\n        var maxSumEndingHere = 0L\n        var idx = 0\n\n        while (idx < arrUnassigned.size) {\n\n            if (maxSumEndingHere + arrUnassigned(idx).totalPoints <= maxSum && lstGroup.size != arrUnassigned.size) {\n\n                maxSumEndingHere += arrUnassigned(idx).totalPoints\n\n                lstGroup += arrUnassigned(idx)\n\n                // TODO: remove, for negative values\n                if (maxSumEndingHere < 0) {\n\n                    lstGroup.clear()\n                    maxSumEndingHere = 0\n                }\n                else if (maxSoFar < maxSumEndingHere)\n                    maxSoFar = maxSumEndingHere\n            }\n            else {\n\n                lstGroup.foreach(_.assignedPart = partCounter)\n\n                lstGroup.clear()\n\n                partCounter += 1\n\n                idx = arrUnassigned.size\n            }\n\n            idx += 1\n        }\n\n        arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)\n    }\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Spark_kNN/src/main/scala/org/cusp/bdi/sknn/util/AssignToPartitions.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ Spark_kNN/src/main/scala/org/cusp/bdi/sknn/util/AssignToPartitions.scala	(date 1594794771030)
@@ -2,68 +2,68 @@
 
 import scala.collection.mutable.ListBuffer
 
-import org.cusp.bdi.sknn.QTPartId
+import org.cusp.bdi.sknn.PartitionInfo
 
 /**
-  * Based on Maximum Subarray algorithm (Kadane's)
-  */
-case class AssignToPartitions(arrAttrQT: Array[QTPartId], maxSum: Long) {
+ * Based on Maximum Subarray algorithm (Kadane's)
+ */
+case class AssignToPartitions(arrAttrQT: Array[PartitionInfo], maxSum: Long) {
 
-    private var partCounter = 0
+  private var partCounter = 0
 
-    // group of quadtrees adding to the maximum sum
-    private val lstGroup = ListBuffer[QTPartId]()
+  // group of quadtrees adding to the maximum sum
+  private val lstGroup = ListBuffer[PartitionInfo]()
 
-    def getPartitionCount() = partCounter
+  def getPartitionCount() = partCounter
 
-    // 1st, assign quadtrees with maximum total points
-    arrAttrQT.foreach(qtInf =>
-        if (qtInf.totalPoints >= maxSum) {
+  // 1st, assign quadtrees with maximum total points
+  arrAttrQT.foreach(qtInf =>
+    if (qtInf.totalPoints >= maxSum) {
 
-            qtInf.assignedPart = partCounter
-            partCounter += 1
-        })
+      qtInf.assignedPart = partCounter
+      partCounter += 1
+    })
 
-    // remaining unassigned quadtrees
-    var arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)
+  //  unassigned regions
+  var arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)
 
-    while (!arrUnassigned.isEmpty) {
+  while (!arrUnassigned.isEmpty) {
 
-        var maxSoFar = 0L
-        var maxSumEndingHere = 0L
-        var idx = 0
+    var maxSoFar = 0L
+    var maxSumEndingHere = 0L
+    var idx = 0
 
-        while (idx < arrUnassigned.size) {
+    while (idx < arrUnassigned.size) {
 
-            if (maxSumEndingHere + arrUnassigned(idx).totalPoints <= maxSum && lstGroup.size != arrUnassigned.size) {
+      if (maxSumEndingHere + arrUnassigned(idx).totalPoints <= maxSum && lstGroup.size != arrUnassigned.size) {
 
-                maxSumEndingHere += arrUnassigned(idx).totalPoints
+        maxSumEndingHere += arrUnassigned(idx).totalPoints
 
-                lstGroup += arrUnassigned(idx)
+        lstGroup += arrUnassigned(idx)
 
-                // TODO: remove, for negative values
-                if (maxSumEndingHere < 0) {
+        // TODO: remove, for negative values
+        if (maxSumEndingHere < 0) {
 
-                    lstGroup.clear()
-                    maxSumEndingHere = 0
-                }
-                else if (maxSoFar < maxSumEndingHere)
-                    maxSoFar = maxSumEndingHere
-            }
-            else {
+          lstGroup.clear()
+          maxSumEndingHere = 0
+        }
+        else if (maxSoFar < maxSumEndingHere)
+          maxSoFar = maxSumEndingHere
+      }
+      else {
 
-                lstGroup.foreach(_.assignedPart = partCounter)
+        lstGroup.foreach(_.assignedPart = partCounter)
 
-                lstGroup.clear()
+        lstGroup.clear()
 
-                partCounter += 1
+        partCounter += 1
 
-                idx = arrUnassigned.size
-            }
+        idx = arrUnassigned.size
+      }
 
-            idx += 1
-        }
+      idx += 1
+    }
 
-        arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)
-    }
+    arrUnassigned = arrAttrQT.filter(_.assignedPart == -1).sortBy(_.totalPoints)
+  }
 }
\ No newline at end of file
Index: FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/SIM_Example.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/SIM_Example.scala	(date 1594963700041)
+++ FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/SIM_Example.scala	(date 1594963700041)
@@ -0,0 +1,129 @@
+package org.cusp.bdi.fw.simba
+
+import org.apache.hadoop.io.compress.GzipCodec
+import org.apache.spark.sql.Row
+import org.apache.spark.sql.simba.SimbaSession
+import org.cusp.bdi.util.{Helper, LocalRunConsts}
+
+import scala.collection.mutable.SortedSet
+
+object SIM_Example extends Serializable {
+
+  case class PointData(x: Double, y: Double, other: String)
+
+  //    case class PointData(x: Double, y: Double)
+  //    case class LineSegmentData(start: Point, end: Point, other: String)
+
+  private final def getDS(simbaSession: SimbaSession, fileName: String, objType: String) = {
+
+    //        import simbaSession.implicits._
+    //        import simbaSession.simbaImplicits._
+
+    import simbaSession.implicits._
+
+    simbaSession.read.textFile(fileName)
+      .map(SimbaLineParser.lineParser(objType))
+      .filter(_ != null)
+      .map(row => PointData(row._2._1.toDouble, row._2._2.toDouble, row._1))
+  }
+
+  def main(args: Array[String]): Unit = {
+
+    val startTime = System.currentTimeMillis()
+
+    val clArgs = SIM_CLArgs.taxi_taxi_1M_No_Trip
+    //        val clArgs = SIM_CLArgs.taxi_taxi_rand_samp
+    //        val clArgs = SIM_CLArgs.randomPoints_randomPoints
+
+    //        val clArgs = SIM_CLArgs.busPoint_busPointShift
+    //        val clArgs = SIM_CLArgs.TPEP_Point_TPEP_Point
+    //        val clArgs = SIM_CLArgs.lion_PolyRect_TPEP_Point
+    //        val clArgs = SIM_CLArgs.lion_LStr_TPEP_Point
+    //        val clArgs = SIM_CLArgs.lion_LStr_Taxi_Point
+    //        val clArgs = SIM_CLArgs.lion_PolyRect_Taxi_Point
+    //        val clArgs = SIM_CLArgs.OSM_Point_OSM_Point
+    //        val clArgs = CLArgsParser(args, SIM_Arguments())
+
+    var simbaBuilder = SimbaSession.builder()
+      .appName(StringBuilder.newBuilder
+        .append("SIM_Example_")
+        .append("_")
+        .append(clArgs.getParamValueString(SIM_Arguments.queryType))
+        .append("_")
+        .append(clArgs.getParamValueString(SIM_Arguments.firstSetObj))
+        .append("_")
+        .append(clArgs.getParamValueString(SIM_Arguments.secondSetObj))
+        .toString)
+      .config("simba.join.partitions", "50")
+      .config("simba.index.partitions", "50")
+      .config("spark.local.dir", LocalRunConsts.sparkWorkDir)
+    //            .getOrCreate()
+
+    if (clArgs.getParamValueBoolean(SIM_Arguments.local))
+      simbaBuilder.master("local[*]")
+
+    val simbaSession = simbaBuilder.getOrCreate()
+
+    // delete output dir if exists
+    Helper.delDirHDFS(simbaSession.sparkContext, clArgs.getParamValueString(SIM_Arguments.outDir))
+
+    val DS1 = getDS(simbaSession, clArgs.getParamValueString(SIM_Arguments.firstSet), clArgs.getParamValueString(SIM_Arguments.firstSetObj))
+      .repartition(1024)
+
+    val DS2 = getDS(simbaSession, clArgs.getParamValueString(SIM_Arguments.secondSet), clArgs.getParamValueString(SIM_Arguments.secondSetObj))
+      .repartition(1024)
+
+    //        import simbaSession.implicits._
+    //        import simbaSession.simbaImplicits._
+
+    val res1 = DS1.knnJoin(DS2, Array("x", "y"), Array("x", "y"), 10)
+      .rdd
+      .mapPartitions(_.map(processRow))
+      .reduceByKey(_ ++ _)
+      .mapPartitions(_.map(rowToString))
+
+    val res2 = DS2.knnJoin(DS1, Array("x", "y"), Array("x", "y"), 10)
+      .rdd
+      .mapPartitions(_.map(processRow))
+      .reduceByKey(_ ++ _)
+      .mapPartitions(_.map(rowToString))
+
+    res1.union(res2).saveAsTextFile(clArgs.getParamValueString(SIM_Arguments.outDir), classOf[GzipCodec])
+
+    simbaSession.stop()
+
+    if (clArgs.getParamValueBoolean(SIM_Arguments.local)) {
+
+      printf("Total Time: %,.2f Sec%n", (System.currentTimeMillis() - startTime) / 1000.0)
+      println("Output idr: " + clArgs.getParamValueString(SIM_Arguments.outDir))
+    }
+  }
+
+  def rowToString(row: (String, SortedSet[(Double, String)])) = {
+    val sb = StringBuilder.newBuilder
+      .append(row._1)
+
+    (SortedSet[(Double, String)]() ++ row._2)
+      .foreach(matches => sb.append(";%.8f,%s".format(matches._1, matches._2)))
+
+    sb.toString()
+  }
+
+  def processRow(row: Row) = {
+
+    val x = row(0).toString.toDouble
+    val y = row(1).toString.toDouble
+
+    val dist = Helper.euclideanDist((x, y), (row(3) match {
+      case d: Double => d
+    }, row(4) match {
+      case d: Double => d
+    }))
+
+    val sSet = SortedSet((dist, row(5).toString))
+
+    val pointInfo = "%s,%.8f,%.8f".format(row.get(2).toString, x, y)
+
+    (pointInfo, sSet)
+  }
+}
\ No newline at end of file
Index: FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/HelperObjects.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/HelperObjects.scala	(date 1588317540376)
+++ FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/HelperObjects.scala	(date 1588317540376)
@@ -0,0 +1,78 @@
+package org.cusp.bdi.fw.simba
+
+import org.cusp.bdi.util.Arguments_QueryType
+import org.cusp.bdi.util.InputFileParsers
+import org.cusp.bdi.util.LocalRunArgs
+import org.cusp.bdi.util.LocalRunConsts
+
+object SIM_Arguments extends Arguments_QueryType {
+
+    override def apply() =
+        super.apply()
+}
+
+object SimbaLineParser {
+
+    def lineParser(objType: String) = objType match {
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_TPEP_POINT_WGS84 => InputFileParsers.tpepPoints_WGS84
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_OSM_POINT_WGS84 => InputFileParsers.osmPoints_WGS84
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_TPEP_POINT => InputFileParsers.tpepPoints
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_TAXI_POINT => InputFileParsers.taxiPoints
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_THREE_PART_LINE => InputFileParsers.threePartLine
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_BUS_POINT => InputFileParsers.busPoints
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_BUS_POINT_SHIFTED => InputFileParsers.busPoints
+        case s if s matches "(?i)" + LocalRunConsts.DS_CODE_RAND_POINT => InputFileParsers.randPoints
+    }
+}
+
+object SIM_CLArgs {
+
+    val TPEP_Point_TPEP_Point = SIM_CLArgs("join", LocalRunConsts.pathTPEP_NAD83, "TPEP_Point_WGS84", LocalRunConsts.pathTPEP_NAD83, "TPEP_Point_WGS84")
+    val OSM_Point_OSM_Point = SIM_CLArgs("join", LocalRunConsts.pathOSM_Point_WGS84, "OSM_Point_WGS84", LocalRunConsts.pathOSM_Point_WGS84, "OSM_Point_WGS84")
+
+    private def apply(queryType: String, firstSet: String, firstSetObj: String, secondSet: String, secondSetObj: String) = {
+
+        val additionalParams = StringBuilder.newBuilder
+            .append(" -queryType ").append(queryType)
+            .append(" -matchCount 3")
+            .append(" -errorRange 150")
+            .append(" -matchDist 150")
+
+        LocalRunArgs(firstSet, firstSetObj, secondSet, secondSetObj, additionalParams, SIM_Arguments())
+    }
+
+    def taxi_taxi_1M =
+        apply("kNNJoin",
+            LocalRunConsts.pathTaxi1M_NAD83,
+            LocalRunConsts.DS_CODE_TAXI_POINT,
+            LocalRunConsts.pathTaxi1M_NAD83,
+            LocalRunConsts.DS_CODE_TAXI_POINT)
+
+    def taxi_taxi_1M_No_Trip =
+        apply("kNNJoin",
+            LocalRunConsts.pathTaxi1M_No_Trip_NAD83_A,
+            LocalRunConsts.DS_CODE_THREE_PART_LINE,
+            LocalRunConsts.pathTaxi1M_No_Trip_NAD83_B,
+            LocalRunConsts.DS_CODE_THREE_PART_LINE)
+
+    def taxi_taxi_rand_samp =
+        apply("kNNJoin",
+            LocalRunConsts.pathRandSample_A_NAD83,
+            LocalRunConsts.DS_CODE_THREE_PART_LINE,
+            LocalRunConsts.pathRandSample_B_NAD83,
+            LocalRunConsts.DS_CODE_THREE_PART_LINE)
+
+    def busPoint_busPointShift =
+        apply("kNNJoin",
+            LocalRunConsts.pathBus_NAD83_SMALL,
+            LocalRunConsts.DS_CODE_BUS_POINT,
+            LocalRunConsts.pathBus_NAD83_SMALL_SHIFTED,
+            LocalRunConsts.DS_CODE_BUS_POINT_SHIFTED)
+
+    def randomPoints_randomPoints =
+        apply("kNNJoin",
+            LocalRunConsts.pathRandomPointsNonUniformPart1,
+            LocalRunConsts.DS_CODE_RAND_POINT,
+            LocalRunConsts.pathRandomPointsNonUniformPart2,
+            LocalRunConsts.DS_CODE_RAND_POINT)
+}
\ No newline at end of file
Index: .idea/Research.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module type=\"JAVA_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\" inherit-compiler-output=\"true\">\n    <exclude-output />\n    <content url=\"file://$MODULE_DIR$\" />\n    <orderEntry type=\"inheritedJdk\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/Research.iml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ .idea/Research.iml	(date 1594705152099)
@@ -5,5 +5,6 @@
     <content url="file://$MODULE_DIR$" />
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="library" name="scala-sdk-2.11.11" level="application" />
   </component>
 </module>
\ No newline at end of file
Index: FW_GeoMatch/src/main/scala/org/cusp/bdi/gm/examples/GM_Example.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.cusp.bdi.gm.examples\n\nimport org.apache.hadoop.io.compress.GzipCodec\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.serializer.KryoSerializer\nimport org.cusp.bdi.gm.GeoMatch\nimport org.cusp.bdi.gm.geom.GMGeomBase\nimport org.cusp.bdi.gm.geom.GMLineString\nimport org.cusp.bdi.gm.geom.GMPoint\nimport org.cusp.bdi.gm.geom.GMPolygon\nimport org.cusp.bdi.gm.geom.GMRectangle\nimport org.cusp.bdi.util.Arguments_QueryType\nimport org.cusp.bdi.util.CLArgsParser\nimport org.cusp.bdi.util.Helper\nimport org.cusp.bdi.util.InputFileParsers\n\nobject QueryType {\n    val distance = \"distance\"\n    val kNN = \"knn\"\n    val range = \"range\"\n}\n\nobject GM_Arguments extends Arguments_QueryType {\n\n    val hilbertN = (\"hilbertN\", \"Int\", 256, \"The size of the hilbert curve (i.e. n)\")\n    val adjustForBoundCross = (\"adjustForBoundCross\", \"Boolean\", false, \"Set to true to adust search to account for second dataset objects that cross partition boundries. This will increase 1st set obj duplication.\")\n    val searchGridMinX = (\"searchGridMinX\", \"Int\", -1, \"Search grid's minimum X\")\n    val searchGridMinY = (\"searchGridMinY\", \"Int\", -1, \"Search grid's minimum Y\")\n    val searchGridMaxX = (\"searchGridMaxX\", \"Int\", -1, \"Search grid's maximum X\")\n    val searchGridMaxY = (\"searchGridMaxY\", \"Int\", -1, \"Search grid's maximum Y\")\n\n    override def apply() =\n        super.apply() ++ List(queryType, hilbertN, adjustForBoundCross, searchGridMinX, searchGridMinY, searchGridMaxX, searchGridMaxY)\n}\n\nobject GM_Example extends Serializable {\n\n    private def RDD_Point(sparkContext: SparkContext, objType: String, fileName: String) = {\n\n        val lineParser = objType match {\n            case s if s matches \"(?i)TPEP_Point\" => InputFileParsers.tpepPoints\n            case s if s matches \"(?i)Taxi_Point\" => InputFileParsers.taxiPoints\n            case s if s matches \"(?i)Bus_Point\" => InputFileParsers.busPoints\n        }\n\n        sparkContext.textFile(fileName)\n            .mapPartitions(_.map(lineParser))\n            .filter(_ != null)\n            .mapPartitions(_.map(parts => new GMPoint(parts._1, (parts._2._1.toInt + 1, parts._2._2.toInt + 1))))\n    }\n    private def getRDD(sparkContext: SparkContext, objType: String, fileName: String, errorRange: Int): RDD[_ <: GMGeomBase] = {\n\n        if (objType.matches(\"(?i)TPEP_Point|(?i)Taxi_Point|(?i)Bus_Point\"))\n            RDD_Point(sparkContext, objType, fileName)\n        else if (objType.matches(\"(?i)LION_LineString|(?i)LION_Polygon|(?i)LION_Rectangle\"))\n            sparkContext.textFile(fileName)\n                .mapPartitions(_.map(InputFileParsers.nycLION))\n                .filter(_ != null)\n                .mapPartitions(_.map(parts => {\n\n                    val arrCoords = parts._2.map(coords => {\n\n                        val x = coords._1.substring(0, coords._1.indexOf('.')).toInt + 1 // ceil\n                        val y = coords._2.substring(0, coords._2.indexOf('.')).toInt + 1 // ceil\n\n                        (x, y)\n                    })\n\n                    (parts._1, arrCoords)\n                }))\n                .mapPartitions(_.map(row =>\n                    objType match {\n                        case s if s matches \"(?i)LION_LineString\" =>\n                            new GMLineString(row._1, row._2)\n                        case s if s matches \"(?i)LION_Polygon\" =>\n                            new GMPolygon(row._1, row._2)\n                        case s if s matches \"(?i)LION_Rectangle\" => {\n\n                            val mbr = Helper.getMBREnds(row._2, errorRange)\n\n                            new GMRectangle(row._1, mbr(0), mbr(1))\n                        }\n                    }))\n        else if (objType.matches(\"(?i)TPEP_Point|(?i)Taxi_Point|(?i)Bus_Point\"))\n            RDD_Point(sparkContext, objType, fileName)\n        else\n            throw new Exception(objType + \" Is not a valid object type\")\n    }\n\n    def main(args: Array[String]): Unit = {\n\n        val startTime = System.currentTimeMillis()\n\n        // change the match expression based on the values to run. 0 is for running on the cluster\n        //        val clArgs = GM_LocalRunArgs.lion_LineStr_TPEP_Point\n        //                val clArgs = GM_LocalRunArgs.TPEP_Point_TPEP_Point\n        //        val clArgs = GM_LocalRunArgs.lion_Poly_TPEP_Point\n        //        val clArgs = GM_LocalRunrgs.lion_PolyRect_TPEP_Point\n        //        val clArgs = GM_LocalRunArgs.lion_LineStr_Taxi_Point\n        //        val clArgs = GM_LocalRunArgs.lion_Poly_Taxi_Point\n        //        val clArgs = GM_LocalRunArgs.lion_LineStr_Bus_Point\n        val clArgs = CLArgsParser(args, GM_Arguments())\n\n        val sparkConf = new SparkConf()\n            .setAppName(StringBuilder\n                .newBuilder\n                .append(\"GM_Example_\")\n                .append(\"_\")\n                .append(clArgs.getParamValueString(GM_Arguments.queryType))\n                .append(\"_\")\n                .append(clArgs.getParamValueString(GM_Arguments.firstSetObj))\n                .append(\"_\")\n                .append(clArgs.getParamValueString(GM_Arguments.secondSetObj))\n                .toString)\n\n        if (clArgs.getParamValueBoolean(GM_Arguments.local))\n            sparkConf.setMaster(\"local[*]\")\n\n        sparkConf.set(\"spark.serializer\", classOf[KryoSerializer].getName)\n        sparkConf.registerKryoClasses(GeoMatch.getGeoMatchClasses())\n\n        val sparkContext = new SparkContext(sparkConf)\n\n        // delete output dir if exists\n        Helper.delDirHDFS(sparkContext, clArgs.getParamValueString(GM_Arguments.outDir))\n\n        val searchGridMBR = (clArgs.getParamValueInt(GM_Arguments.searchGridMinX), clArgs.getParamValueInt(GM_Arguments.searchGridMinY), clArgs.getParamValueInt(GM_Arguments.searchGridMaxX), clArgs.getParamValueInt(GM_Arguments.searchGridMaxY))\n\n        val geoMatch = new GeoMatch(clArgs.getParamValueBoolean(GM_Arguments.debug),\n                                    clArgs.getParamValueInt(GM_Arguments.hilbertN),\n                                    clArgs.getParamValueDouble(GM_Arguments.errorRange),\n                                    searchGridMBR)\n\n        val rdd1 = getRDD(sparkContext, clArgs.getParamValueString(GM_Arguments.firstSetObj), clArgs.getParamValueString(GM_Arguments.firstSet), clArgs.getParamValueDouble(GM_Arguments.errorRange).toInt)\n        val rdd2 = getRDD(sparkContext, clArgs.getParamValueString(GM_Arguments.secondSetObj), clArgs.getParamValueString(GM_Arguments.secondSet), clArgs.getParamValueDouble(GM_Arguments.errorRange).toInt)\n        val matchCount = clArgs.getParamValueInt(GM_Arguments.matchCount)\n        val matchDist = clArgs.getParamValueDouble(GM_Arguments.matchDist)\n        val adjustForBoundCross = clArgs.getParamValueBoolean(GM_Arguments.adjustForBoundCross)\n\n        val rddResult = clArgs.getParamValueString(GM_Arguments.queryType).toLowerCase() match {\n            case QueryType.distance =>\n                geoMatch.spatialJoinDistance(rdd1, rdd2, matchCount, matchDist, adjustForBoundCross)\n            case QueryType.kNN =>\n                geoMatch.spatialJoinKNN(rdd1, rdd2, matchCount, adjustForBoundCross)\n            case QueryType.range =>\n                geoMatch.spatialJoinRange(rdd1, rdd2, adjustForBoundCross)\n        }\n\n        rddResult.mapPartitions(_.map(x => {\n\n            StringBuilder\n                .newBuilder\n                .append(x._1.payload)\n                .append(',')\n                .append(x._2.map(_.payload).mkString(\",\"))\n        }))\n            .saveAsTextFile(clArgs.getParamValueString(GM_Arguments.outDir), classOf[GzipCodec])\n\n        if (clArgs.getParamValueBoolean(GM_Arguments.local)) {\n\n            println(\"Total Time: \" + (System.currentTimeMillis() - startTime))\n            println(\"Output idr: \" + clArgs.getParamValueString(GM_Arguments.outDir))\n        }\n    }\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- FW_GeoMatch/src/main/scala/org/cusp/bdi/gm/examples/GM_Example.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ FW_GeoMatch/src/main/scala/org/cusp/bdi/gm/examples/GM_Example.scala	(date 1594705270535)
@@ -1,21 +1,5 @@
 package org.cusp.bdi.gm.examples
 
-import org.apache.hadoop.io.compress.GzipCodec
-import org.apache.spark.SparkConf
-import org.apache.spark.SparkContext
-import org.apache.spark.rdd.RDD
-import org.apache.spark.serializer.KryoSerializer
-import org.cusp.bdi.gm.GeoMatch
-import org.cusp.bdi.gm.geom.GMGeomBase
-import org.cusp.bdi.gm.geom.GMLineString
-import org.cusp.bdi.gm.geom.GMPoint
-import org.cusp.bdi.gm.geom.GMPolygon
-import org.cusp.bdi.gm.geom.GMRectangle
-import org.cusp.bdi.util.Arguments_QueryType
-import org.cusp.bdi.util.CLArgsParser
-import org.cusp.bdi.util.Helper
-import org.cusp.bdi.util.InputFileParsers
-
 object QueryType {
     val distance = "distance"
     val kNN = "knn"
Index: .idea/compiler.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"CompilerConfiguration\">\n    <annotationProcessing>\n      <profile name=\"Maven default annotation processors profile\" enabled=\"true\">\n        <sourceOutputDir name=\"target/generated-sources/annotations\" />\n        <sourceTestOutputDir name=\"target/generated-test-sources/test-annotations\" />\n        <outputRelativeToContentRoot value=\"true\" />\n        <module name=\"FW_GeoMatch\" />\n        <module name=\"FW_Simba\" />\n        <module name=\"FW_Simba_Source\" />\n        <module name=\"Common\" />\n        <module name=\"GeoMatch\" />\n        <module name=\"Spark_kNN\" />\n        <module name=\"LocalRunFiles\" />\n      </profile>\n    </annotationProcessing>\n    <bytecodeTargetLevel>\n      <module name=\"Common\" target=\"1.8\" />\n      <module name=\"FW_GeoMatch\" target=\"1.8\" />\n      <module name=\"FW_Simba\" target=\"1.8\" />\n      <module name=\"FW_Simba_Source\" target=\"1.8\" />\n      <module name=\"GeoMatch\" target=\"1.8\" />\n      <module name=\"LocalRunFiles\" target=\"1.8\" />\n      <module name=\"Spark_kNN\" target=\"1.8\" />\n    </bytecodeTargetLevel>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/compiler.xml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ .idea/compiler.xml	(date 1594757152726)
@@ -6,13 +6,13 @@
         <sourceOutputDir name="target/generated-sources/annotations" />
         <sourceTestOutputDir name="target/generated-test-sources/test-annotations" />
         <outputRelativeToContentRoot value="true" />
-        <module name="FW_GeoMatch" />
         <module name="FW_Simba" />
         <module name="FW_Simba_Source" />
         <module name="Common" />
         <module name="GeoMatch" />
         <module name="Spark_kNN" />
         <module name="LocalRunFiles" />
+        <module name="SpatialBenchmark" />
       </profile>
     </annotationProcessing>
     <bytecodeTargetLevel>
@@ -23,6 +23,7 @@
       <module name="GeoMatch" target="1.8" />
       <module name="LocalRunFiles" target="1.8" />
       <module name="Spark_kNN" target="1.8" />
+      <module name="SpatialBenchmark" target="1.8" />
     </bytecodeTargetLevel>
   </component>
 </project>
\ No newline at end of file
Index: Spark_kNN/src/test/scala/org/cusp/bdi/sknn/test/TestAllKNN.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.cusp.bdi.sknn.test\n\nimport org.apache.hadoop.io.compress.GzipCodec\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.serializer.KryoSerializer\nimport org.cusp.bdi.gm.GeoMatch\nimport org.cusp.bdi.sknn.SparkKNN\nimport org.cusp.bdi.sknn.util.QuadTreeInfo\nimport org.cusp.bdi.sknn.util.RDD_Store\nimport org.cusp.bdi.sknn.util.SparkKNN_Arguments\nimport org.cusp.bdi.util.Helper\nimport org.cusp.bdi.util.LocalRunConsts\nimport org.cusp.bdi.util.sknn.SparkKNN_Local_CLArgs\n\nimport com.insightfullogic.quad_trees.Point\nimport com.insightfullogic.quad_trees.QuadTree\n\nobject TestAllKnnJoin {\n    def main(args: Array[String]): Unit = {\n\n        val startTime = System.currentTimeMillis()\n        var startTime2 = startTime\n\n        //        val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M(SparkKNN_Arguments())\n        val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M_No_Trip(SparkKNN_Arguments())\n        //        val clArgs = SparkKNN_Local_CLArgs.random_sample(SparkKNN_Arguments())\n        //        val clArgs = SparkKNN_Local_CLArgs.busPoint_busPointShift(SparkKNN_Arguments())\n        //        val clArgs = SparkKNN_Local_CLArgs.busPoint_taxiPoint(SparkKNN_Arguments())\n        //        val clArgs = SparkKNN_Local_CLArgs.tpepPoint_tpepPoint(SparkKNN_Arguments())\n        //        val clArgs = CLArgsParser(args, SparkKNN_Arguments())\n\n        val localMode = clArgs.getParamValueBoolean(SparkKNN_Arguments.local)\n        val firstSet = clArgs.getParamValueString(SparkKNN_Arguments.firstSet)\n        val firstSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.firstSetObjType)\n        val firstSetParser = RDD_Store.getLineParser(firstSetObjType)\n        val secondSet = clArgs.getParamValueString(SparkKNN_Arguments.secondSet)\n        val secondSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.secondSetObjType)\n        val secondSetParser = RDD_Store.getLineParser(secondSetObjType)\n\n        val kParam = clArgs.getParamValueInt(SparkKNN_Arguments.k)\n        val minPartitions = clArgs.getParamValueInt(SparkKNN_Arguments.minPartitions)\n        //        val sampleRate = clArgs.getParamValueDouble(SparkKNN_Arguments.sampleRate)\n        val outDir = clArgs.getParamValueString(SparkKNN_Arguments.outDir)\n\n        val sparkConf = new SparkConf()\n            .setAppName(this.getClass.getName)\n            .set(\"spark.serializer\", classOf[KryoSerializer].getName)\n            .registerKryoClasses(GeoMatch.getGeoMatchClasses)\n            .registerKryoClasses(SparkKNN.getSparkKNNClasses)\n\n        if (localMode)\n            sparkConf.setMaster(\"local[*]\")\n                .set(\"spark.local.dir\", LocalRunConsts.sparkWorkDir)\n\n        val sc = new SparkContext(sparkConf)\n\n        val rddLeft = RDD_Store.getRDDPlain(sc, firstSet, minPartitions)\n            .mapPartitions(_.map(firstSetParser))\n            .filter(_ != null)\n            .mapPartitions(_.map(row => {\n\n                val point = new Point(row._2._1.toDouble, row._2._2.toDouble)\n                point.userData = row._1\n\n                point\n            }))\n\n        val rddRight = RDD_Store.getRDDPlain(sc, secondSet, minPartitions)\n            .mapPartitions(_.map(secondSetParser))\n            .filter(_ != null)\n            .mapPartitions(_.map(row => {\n\n                val point = new Point(row._2._1.toDouble, row._2._2.toDouble)\n                point.userData = row._1\n\n                point\n            }))\n\n        val sparkKNN = SparkKNN(rddLeft, rddRight /*, sampleRate*/ , kParam)\n\n        // during local test runs\n        sparkKNN.minPartitions = minPartitions\n\n        val rddResult = sparkKNN.allKnnJoin()\n\n        // delete output dir if exists\n        Helper.delDirHDFS(rddResult.context, clArgs.getParamValueString(SparkKNN_Arguments.outDir))\n\n        rddResult.mapPartitions(_.map(row =>\n            \"%s,%.8f,%.8f;%s\".format(row._1.userData, row._1.x, row._1.y, row._2.map(matchInfo =>\n                \"%.8f,%s\".format(math.sqrt(matchInfo._1), matchInfo._2.userData)).mkString(\";\"))))\n            .saveAsTextFile(outDir, classOf[GzipCodec])\n\n        if (clArgs.getParamValueBoolean(SparkKNN_Arguments.local)) {\n\n            printf(\"Total Time: %,.2f Sec%n\", (System.currentTimeMillis() - startTime) / 1000.0)\n\n            println(outDir)\n        }\n    }\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Spark_kNN/src/test/scala/org/cusp/bdi/sknn/test/TestAllKNN.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ Spark_kNN/src/test/scala/org/cusp/bdi/sknn/test/TestAllKNN.scala	(date 1594963412841)
@@ -17,86 +17,87 @@
 import com.insightfullogic.quad_trees.QuadTree
 
 object TestAllKnnJoin {
-    def main(args: Array[String]): Unit = {
+  def main(args: Array[String]): Unit = {
 
-        val startTime = System.currentTimeMillis()
-        var startTime2 = startTime
+    val startTime = System.currentTimeMillis()
+    var startTime2 = startTime
 
-        //        val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M(SparkKNN_Arguments())
-        val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M_No_Trip(SparkKNN_Arguments())
-        //        val clArgs = SparkKNN_Local_CLArgs.random_sample(SparkKNN_Arguments())
-        //        val clArgs = SparkKNN_Local_CLArgs.busPoint_busPointShift(SparkKNN_Arguments())
-        //        val clArgs = SparkKNN_Local_CLArgs.busPoint_taxiPoint(SparkKNN_Arguments())
-        //        val clArgs = SparkKNN_Local_CLArgs.tpepPoint_tpepPoint(SparkKNN_Arguments())
-        //        val clArgs = CLArgsParser(args, SparkKNN_Arguments())
+    //        val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M(SparkKNN_Arguments())
+    val clArgs = SparkKNN_Local_CLArgs.taxi_taxi_1M_No_Trip(SparkKNN_Arguments())
+    //        val clArgs = SparkKNN_Local_CLArgs.random_sample(SparkKNN_Arguments())
+    //        val clArgs = SparkKNN_Local_CLArgs.busPoint_busPointShift(SparkKNN_Arguments())
+    //        val clArgs = SparkKNN_Local_CLArgs.busPoint_taxiPoint(SparkKNN_Arguments())
+    //        val clArgs = SparkKNN_Local_CLArgs.tpepPoint_tpepPoint(SparkKNN_Arguments())
+    //        val clArgs = CLArgsParser(args, SparkKNN_Arguments())
 
-        val localMode = clArgs.getParamValueBoolean(SparkKNN_Arguments.local)
-        val firstSet = clArgs.getParamValueString(SparkKNN_Arguments.firstSet)
-        val firstSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.firstSetObjType)
-        val firstSetParser = RDD_Store.getLineParser(firstSetObjType)
-        val secondSet = clArgs.getParamValueString(SparkKNN_Arguments.secondSet)
-        val secondSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.secondSetObjType)
-        val secondSetParser = RDD_Store.getLineParser(secondSetObjType)
+    val localMode = clArgs.getParamValueBoolean(SparkKNN_Arguments.local)
+    val firstSet = clArgs.getParamValueString(SparkKNN_Arguments.firstSet)
+    val firstSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.firstSetObjType)
+    val firstSetParser = RDD_Store.getLineParser(firstSetObjType)
+    val secondSet = clArgs.getParamValueString(SparkKNN_Arguments.secondSet)
+    val secondSetObjType = clArgs.getParamValueString(SparkKNN_Arguments.secondSetObjType)
+    val secondSetParser = RDD_Store.getLineParser(secondSetObjType)
 
-        val kParam = clArgs.getParamValueInt(SparkKNN_Arguments.k)
-        val minPartitions = clArgs.getParamValueInt(SparkKNN_Arguments.minPartitions)
-        //        val sampleRate = clArgs.getParamValueDouble(SparkKNN_Arguments.sampleRate)
-        val outDir = clArgs.getParamValueString(SparkKNN_Arguments.outDir)
+    val kParam = clArgs.getParamValueInt(SparkKNN_Arguments.k)
+    val minPartitions = clArgs.getParamValueInt(SparkKNN_Arguments.minPartitions)
+    //        val sampleRate = clArgs.getParamValueDouble(SparkKNN_Arguments.sampleRate)
+    val outDir = clArgs.getParamValueString(SparkKNN_Arguments.outDir)
 
-        val sparkConf = new SparkConf()
-            .setAppName(this.getClass.getName)
-            .set("spark.serializer", classOf[KryoSerializer].getName)
-            .registerKryoClasses(GeoMatch.getGeoMatchClasses)
-            .registerKryoClasses(SparkKNN.getSparkKNNClasses)
+    val sparkConf = new SparkConf()
+      .setAppName(this.getClass.getName)
+      .set("spark.serializer", classOf[KryoSerializer].getName)
+      .registerKryoClasses(GeoMatch.getGeoMatchClasses)
+      .registerKryoClasses(SparkKNN.getSparkKNNClasses)
 
-        if (localMode)
-            sparkConf.setMaster("local[*]")
-                .set("spark.local.dir", LocalRunConsts.sparkWorkDir)
+    if (localMode)
+      sparkConf.setMaster("local[*]")
+        .set("spark.local.dir", LocalRunConsts.sparkWorkDir)
 
-        val sc = new SparkContext(sparkConf)
+    val sc = new SparkContext(sparkConf)
 
-        val rddLeft = RDD_Store.getRDDPlain(sc, firstSet, minPartitions)
-            .mapPartitions(_.map(firstSetParser))
-            .filter(_ != null)
-            .mapPartitions(_.map(row => {
+    val rddLeft = RDD_Store.getRDDPlain(sc, firstSet, minPartitions)
+      .mapPartitions(_.map(firstSetParser))
+      .filter(_ != null)
+      .mapPartitions(_.map(row => {
 
-                val point = new Point(row._2._1.toDouble, row._2._2.toDouble)
-                point.userData = row._1
+        val point = new Point(row._2._1.toDouble, row._2._2.toDouble)
+        point.userData = row._1
 
-                point
-            }))
+        point
+      }))
 
-        val rddRight = RDD_Store.getRDDPlain(sc, secondSet, minPartitions)
-            .mapPartitions(_.map(secondSetParser))
-            .filter(_ != null)
-            .mapPartitions(_.map(row => {
+    val rddRight = RDD_Store.getRDDPlain(sc, secondSet, minPartitions)
+      .mapPartitions(_.map(secondSetParser))
+      .filter(_ != null)
+      .mapPartitions(_.map(row => {
 
-                val point = new Point(row._2._1.toDouble, row._2._2.toDouble)
-                point.userData = row._1
+        val point = new Point(row._2._1.toDouble, row._2._2.toDouble)
+        point.userData = row._1
 
-                point
-            }))
+        point
+      }))
 
-        val sparkKNN = SparkKNN(rddLeft, rddRight /*, sampleRate*/ , kParam)
+    val sparkKNN = SparkKNN(rddLeft, rddRight /*, sampleRate*/ , kParam)
 
-        // during local test runs
-        sparkKNN.minPartitions = minPartitions
+    // during local test runs
+    sparkKNN.minPartitions = minPartitions
 
-        val rddResult = sparkKNN.allKnnJoin()
+    //        val rddResult = sparkKNN.allKnnJoin()
+    val rddResult = sparkKNN.allKnnJoin()
 
-        // delete output dir if exists
-        Helper.delDirHDFS(rddResult.context, clArgs.getParamValueString(SparkKNN_Arguments.outDir))
+    // delete output dir if exists
+    Helper.delDirHDFS(rddResult.context, clArgs.getParamValueString(SparkKNN_Arguments.outDir))
 
-        rddResult.mapPartitions(_.map(row =>
-            "%s,%.8f,%.8f;%s".format(row._1.userData, row._1.x, row._1.y, row._2.map(matchInfo =>
-                "%.8f,%s".format(math.sqrt(matchInfo._1), matchInfo._2.userData)).mkString(";"))))
-            .saveAsTextFile(outDir, classOf[GzipCodec])
+    rddResult.mapPartitions(_.map(row =>
+      "%s,%.8f,%.8f;%s".format(row._1.userData, row._1.x, row._1.y, row._2.map(matchInfo =>
+        "%.8f,%s".format(math.sqrt(matchInfo._1), matchInfo._2.userData)).mkString(";"))))
+      .saveAsTextFile(outDir, classOf[GzipCodec])
 
-        if (clArgs.getParamValueBoolean(SparkKNN_Arguments.local)) {
+    if (clArgs.getParamValueBoolean(SparkKNN_Arguments.local)) {
 
-            printf("Total Time: %,.2f Sec%n", (System.currentTimeMillis() - startTime) / 1000.0)
+      printf("Total Time: %,.2f Sec%n", (System.currentTimeMillis() - startTime) / 1000.0)
 
-            println(outDir)
-        }
-    }
+      println(outDir)
+    }
+  }
 }
\ No newline at end of file
Index: .idea/uiDesigner.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/uiDesigner.xml	(date 1594793867570)
+++ .idea/uiDesigner.xml	(date 1594793867570)
@@ -0,0 +1,124 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="Palette2">
+    <group name="Swing">
+      <item class="com.intellij.uiDesigner.HSpacer" tooltip-text="Horizontal Spacer" icon="/com/intellij/uiDesigner/icons/hspacer.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="1" hsize-policy="6" anchor="0" fill="1" />
+      </item>
+      <item class="com.intellij.uiDesigner.VSpacer" tooltip-text="Vertical Spacer" icon="/com/intellij/uiDesigner/icons/vspacer.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="1" anchor="0" fill="2" />
+      </item>
+      <item class="javax.swing.JPanel" icon="/com/intellij/uiDesigner/icons/panel.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="3" hsize-policy="3" anchor="0" fill="3" />
+      </item>
+      <item class="javax.swing.JScrollPane" icon="/com/intellij/uiDesigner/icons/scrollPane.png" removable="false" auto-create-binding="false" can-attach-label="true">
+        <default-constraints vsize-policy="7" hsize-policy="7" anchor="0" fill="3" />
+      </item>
+      <item class="javax.swing.JButton" icon="/com/intellij/uiDesigner/icons/button.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="3" anchor="0" fill="1" />
+        <initial-values>
+          <property name="text" value="Button" />
+        </initial-values>
+      </item>
+      <item class="javax.swing.JRadioButton" icon="/com/intellij/uiDesigner/icons/radioButton.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="3" anchor="8" fill="0" />
+        <initial-values>
+          <property name="text" value="RadioButton" />
+        </initial-values>
+      </item>
+      <item class="javax.swing.JCheckBox" icon="/com/intellij/uiDesigner/icons/checkBox.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="3" anchor="8" fill="0" />
+        <initial-values>
+          <property name="text" value="CheckBox" />
+        </initial-values>
+      </item>
+      <item class="javax.swing.JLabel" icon="/com/intellij/uiDesigner/icons/label.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="0" anchor="8" fill="0" />
+        <initial-values>
+          <property name="text" value="Label" />
+        </initial-values>
+      </item>
+      <item class="javax.swing.JTextField" icon="/com/intellij/uiDesigner/icons/textField.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="8" fill="1">
+          <preferred-size width="150" height="-1" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JPasswordField" icon="/com/intellij/uiDesigner/icons/passwordField.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="8" fill="1">
+          <preferred-size width="150" height="-1" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JFormattedTextField" icon="/com/intellij/uiDesigner/icons/formattedTextField.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="8" fill="1">
+          <preferred-size width="150" height="-1" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JTextArea" icon="/com/intellij/uiDesigner/icons/textArea.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JTextPane" icon="/com/intellij/uiDesigner/icons/textPane.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JEditorPane" icon="/com/intellij/uiDesigner/icons/editorPane.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JComboBox" icon="/com/intellij/uiDesigner/icons/comboBox.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="0" hsize-policy="2" anchor="8" fill="1" />
+      </item>
+      <item class="javax.swing.JTable" icon="/com/intellij/uiDesigner/icons/table.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JList" icon="/com/intellij/uiDesigner/icons/list.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="2" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JTree" icon="/com/intellij/uiDesigner/icons/tree.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3">
+          <preferred-size width="150" height="50" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JTabbedPane" icon="/com/intellij/uiDesigner/icons/tabbedPane.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="3" hsize-policy="3" anchor="0" fill="3">
+          <preferred-size width="200" height="200" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JSplitPane" icon="/com/intellij/uiDesigner/icons/splitPane.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="3" hsize-policy="3" anchor="0" fill="3">
+          <preferred-size width="200" height="200" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JSpinner" icon="/com/intellij/uiDesigner/icons/spinner.png" removable="false" auto-create-binding="true" can-attach-label="true">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="8" fill="1" />
+      </item>
+      <item class="javax.swing.JSlider" icon="/com/intellij/uiDesigner/icons/slider.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="8" fill="1" />
+      </item>
+      <item class="javax.swing.JSeparator" icon="/com/intellij/uiDesigner/icons/separator.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="6" anchor="0" fill="3" />
+      </item>
+      <item class="javax.swing.JProgressBar" icon="/com/intellij/uiDesigner/icons/progressbar.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="0" fill="1" />
+      </item>
+      <item class="javax.swing.JToolBar" icon="/com/intellij/uiDesigner/icons/toolbar.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="6" anchor="0" fill="1">
+          <preferred-size width="-1" height="20" />
+        </default-constraints>
+      </item>
+      <item class="javax.swing.JToolBar$Separator" icon="/com/intellij/uiDesigner/icons/toolbarSeparator.png" removable="false" auto-create-binding="false" can-attach-label="false">
+        <default-constraints vsize-policy="0" hsize-policy="0" anchor="0" fill="1" />
+      </item>
+      <item class="javax.swing.JScrollBar" icon="/com/intellij/uiDesigner/icons/scrollbar.png" removable="false" auto-create-binding="true" can-attach-label="false">
+        <default-constraints vsize-policy="6" hsize-policy="0" anchor="0" fill="2" />
+      </item>
+    </group>
+  </component>
+</project>
\ No newline at end of file
Index: Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.cusp.bdi.sknn\n\nimport scala.collection.mutable.ListBuffer\nimport scala.util.Random\n\nimport org.apache.spark.Partitioner\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.util.SizeEstimator\nimport org.cusp.bdi.sknn.util.AssignToPartitions\nimport org.cusp.bdi.sknn.util.GridOperation\nimport org.cusp.bdi.sknn.util.QuadTreeDigestOperations\nimport org.cusp.bdi.sknn.util.QuadTreeInfo\nimport org.cusp.bdi.sknn.util.QuadTreeOperations\nimport org.cusp.bdi.sknn.util.SortSetObj\nimport org.cusp.bdi.util.Helper\n\nimport com.insightfullogic.quad_trees.Box\nimport com.insightfullogic.quad_trees.Point\nimport com.insightfullogic.quad_trees.QuadTree\nimport com.insightfullogic.quad_trees.QuadTreeDigest\n\ncase class QTPartId(uniqueIdentifier: Int, totalPoints: Long) {\n    var assignedPart = -1\n\n    override def toString() =\n        \"%d\\t%d\\t%d\".format(assignedPart, uniqueIdentifier, totalPoints)\n}\n\nobject SparkKNN {\n\n    def getSparkKNNClasses(): Array[Class[_]] =\n        Array(classOf[QuadTree],\n              classOf[QuadTreeInfo],\n              classOf[GridOperation],\n              QuadTreeDigestOperations.getClass,\n              QuadTreeOperations.getClass,\n              Helper.getClass,\n              classOf[QuadTreeInfo],\n              classOf[SortSetObj],\n              classOf[Box],\n              classOf[Point],\n              classOf[QuadTree],\n              classOf[QuadTreeDigest])\n}\n\ncase class SparkKNN(rddLeft: RDD[Point], rddRight: RDD[Point], k: Int) {\n\n    // for testing, remove ...\n    var minPartitions = 0\n\n    def allKnnJoin(): RDD[(Point, Iterable[(Double, Point)])] =\n        knnJoin(rddLeft, rddRight, k).union(knnJoin(rddRight, rddLeft, k))\n\n    def knnJoin(): RDD[(Point, Iterable[(Double, Point)])] =\n        knnJoin(rddLeft, rddRight, k)\n\n    private def knnJoin(rddLeft: RDD[Point], rddRight: RDD[Point], k: Int): RDD[(Point, Iterable[(Double, Point)])] = {\n\n        var (execRowCapacity, totalRowCount, mbrDS1) = computeCapacity(rddRight, k)\n\n        //        execRowCapacity = 57702\n\n        //        println(\">>\" + execRowCapacity)\n\n        var arrHorizDist = computePartitionRanges(rddRight, execRowCapacity)\n\n        val partitionerByX = new Partitioner() {\n\n            override def numPartitions = arrHorizDist.size\n\n            override def getPartition(key: Any): Int =\n                key match {\n                    case xCoord: Double => binarySearch(arrHorizDist, xCoord.toLong)\n                }\n        }\n\n        val rddQuadTree = rddRight.mapPartitions(_.map(point => (point.x, point)))\n            .repartitionAndSortWithinPartitions(partitionerByX) // places in containers along the x-axis and sort by x-coor\n            .mapPartitionsWithIndex((pIdx, iter) => {\n\n                val lstQT = ListBuffer[QuadTreeInfo]()\n                var qtInf: QuadTreeInfo = null\n\n                while (iter.hasNext) {\n\n                    val lstPoint = iter.take(execRowCapacity - 1).map(_._2).toList // -1 for the one in var point\n\n                    var (minX, minY, maxX, maxY) = computeMBR(lstPoint)\n\n                    minX = minX.toLong\n                    minY = minY.toLong\n                    maxX = maxX.toLong + 1\n                    maxY = maxY.toLong + 1\n\n                    val halfWidth = (maxX - minX) / 2.0\n                    val halfHeight = (maxY - minY) / 2.0\n\n                    qtInf = new QuadTreeInfo(Box(new Point(halfWidth + minX, halfHeight + minY), new Point(halfWidth, halfHeight)))\n\n                    qtInf.quadTree.insert(lstPoint)\n\n                    qtInf.uniqueIdentifier = Random.nextInt()\n\n                    lstQT.append(qtInf)\n                }\n\n                //                lstQT.foreach(qtInf => println(\">>%s\".format(qtInf.toString())))\n\n                lstQT.iterator\n            }, true)\n            .persist(StorageLevel.MEMORY_ONLY)\n\n        // (box#, Count)\n        var arrGridAndQTInf = rddQuadTree\n            .mapPartitionsWithIndex((pIdx, iter) => {\n\n                val gridOp = new GridOperation(mbrDS1, totalRowCount, k)\n\n                iter.map(qtInf => {\n\n                    val iterPoint = qtInf.quadTree.getAllPoints.iterator\n\n                    iterPoint.map(point => {\n\n                        //                        if (point.userData.toString().equalsIgnoreCase(\"taxi_b_918654\"))\n                        //                            println(gridOp.computeBoxXY(point.xy))\n\n                        if (iterPoint.hasNext)\n                            (gridOp.computeBoxXY(point.x, point.y), (1L, Set(qtInf.uniqueIdentifier), null, null))\n                        else\n                            (gridOp.computeBoxXY(point.x, point.y), (1L, Set(qtInf.uniqueIdentifier), ListBuffer((qtInf.uniqueIdentifier, qtInf.quadTree.getTotalPoints)), ListBuffer(qtInf.quadTree.getMBR)))\n                    })\n                })\n                    .flatMap(_.seq)\n            })\n            .reduceByKey((x, y) => (x._1 + y._1, x._2 ++ y._2, reducerListBuffer(x._3, y._3), reducerListBuffer(x._4, y._4)))\n            .collect\n\n        //        arrGridAndQTInf.foreach(row => \"<>\\t%d\\t%d\\t%d\".format(row._1._1, row._1._2, row._2._1))\n\n        arrHorizDist = null\n\n        val arrQTmbr = arrGridAndQTInf.map(_._2._4).filter(_ != null).flatMap(_.seq)\n\n        var arrAttrQT = arrGridAndQTInf\n            .map(_._2._3)\n            .filter(_ != null)\n            .flatMap(_.seq)\n            .map(row => QTPartId(row._1, row._2))\n\n        val actualPartitionCount = AssignToPartitions(arrAttrQT, execRowCapacity).getPartitionCount\n\n        //        arrAttrQT.foreach(qtdInf => println(\">>\\t%s\".format(qtdInf.toString())))\n\n        val bvMapPartAssign = rddLeft.context.broadcast(arrAttrQT.map(qtPId => (qtPId.uniqueIdentifier, qtPId.assignedPart)).toMap)\n\n        arrAttrQT = null\n\n        val gridOp = new GridOperation(mbrDS1, totalRowCount, k)\n\n        var leftBot = gridOp.computeBoxXY(mbrDS1._1, mbrDS1._2)\n        var rightTop = gridOp.computeBoxXY(mbrDS1._3, mbrDS1._4)\n\n        val halfWidth = ((rightTop._1 - leftBot._1).toLong + 1) / 2.0\n        val halfHeight = ((rightTop._2 - leftBot._2).toLong + 1) / 2.0\n\n        val qtGlobalIndex = new QuadTreeDigest(Box(new Point(halfWidth + leftBot._1, halfHeight + leftBot._2), new Point(halfWidth, halfHeight)))\n\n        arrGridAndQTInf.foreach(row => qtGlobalIndex.insert((row._1._1, row._1._2), row._2._1, row._2._2))\n\n        //        println(qtGlobalIndex)\n\n        //        mapGridSummary.foreach(x => println(\">>\\t%d\\t%d\\t%s\".format(x._1, x._2._1, x._2._2.toString)))\n\n        arrGridAndQTInf = null\n\n        //        println(SizeEstimator.estimate(qtGlobalIndex))\n\n        val bvQTGlobalIndex = rddLeft.context.broadcast(qtGlobalIndex)\n\n        var rddPoint = rddLeft\n            .mapPartitions(iter => {\n\n                val gridOp = new GridOperation(mbrDS1, totalRowCount, k)\n\n                iter.map(point => {\n\n                    //                    if (point.userData.toString().equalsIgnoreCase(\"yellow_1_b_548388\"))\n                    //                        println\n\n                    val lstUId = QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.x, point.y), k)\n                        .toList\n\n                    //                    if (lstUId.size >= 18)\n                    //                        println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.xy), k, gridOp.getBoxW, gridOp.getBoxH))\n\n                    val tuple: Any = (point, SortSetObj(k, false), lstUId)\n\n                    (bvMapPartAssign.value.get(lstUId.head).get, tuple)\n                })\n            })\n\n        //        println(\"<>\" + rddPoint.mapPartitions(_.map(_._2.asInstanceOf[(Point, SortSetObj, List[Int])]._3.size)).max)\n\n        //        println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(1013487.21, 134367.52), k))\n\n        val numRounds = arrQTmbr.map(mbr => {\n\n            val lowerLeft = (mbr._1, mbr._2)\n            val upperRight = (mbr._3, mbr._4)\n\n            //            if (QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapPartAssign.value.get(_).get).size >= 9)\n            //                println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapPartAssign.value.get(_).get))\n\n            List(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, lowerLeft._2), k).map(bvMapPartAssign.value.get(_).get).size,\n                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, lowerLeft._2,k).size,\n                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, lowerLeft._2), k).map(bvMapPartAssign.value.get(_).get).size,\n                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, upperRight._1, (upperRight._2 - lowerLeft._2) / 2,k,bvMapPartAssign.value).size,\n                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, upperRight._2), k).map(bvMapPartAssign.value.get(_).get).size,\n                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, upperRight._2,k).size,\n                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k).map(bvMapPartAssign.value.get(_).get).size).max\n        }).max\n\n        //        println(\"<>\" + numRounds)\n\n        (0 until numRounds).foreach(roundNumber => {\n\n            rddPoint = rddQuadTree\n                .mapPartitions(_.map(qtInf => {\n\n                    val tuple: Any = qtInf\n\n                    (bvMapPartAssign.value.get(qtInf.uniqueIdentifier).get, tuple)\n                }) /*, true*/ )\n                .union(rddPoint)\n                .partitionBy(new Partitioner() {\n\n                    override def numPartitions = actualPartitionCount\n                    override def getPartition(key: Any): Int =\n                        key match {\n                            case partId: Int => partId\n                        }\n                })\n                .mapPartitionsWithIndex((pIdx, iter) => {\n\n                    val lstPartQT = ListBuffer[QuadTreeInfo]()\n\n                    iter.map(row => {\n                        row._2 match {\n\n                            case qtInf: QuadTreeInfo => {\n\n                                lstPartQT += qtInf\n\n                                null\n                            }\n                            case _ => {\n\n                                val (point, sortSetSqDist, lstUId) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]\n\n                                //                                if (point.userData.toString().equalsIgnoreCase(\"taxi_b_601998\"))\n                                //                                    println(pIdx)\n\n                                if (!lstUId.isEmpty) {\n\n                                    // build a list of QT to check\n                                    val lstVisitQTInf = lstPartQT.filter(qtInf => lstUId.contains(qtInf.uniqueIdentifier))\n\n                                    //                                    if (point.userData.toString().equalsIgnoreCase(\"yellow_1_b_548388\"))\n                                    //                                        println(pIdx)\n\n                                    QuadTreeOperations.nearestNeighbor(lstVisitQTInf, point, sortSetSqDist, k)\n\n                                    val lstLeftQTInf = lstUId.filterNot(lstVisitQTInf.map(_.uniqueIdentifier).contains _)\n\n                                    val ret = (if (lstLeftQTInf.isEmpty) row._1 else bvMapPartAssign.value.get(lstLeftQTInf.head).get, (point, sortSetSqDist, lstLeftQTInf))\n\n                                    ret\n                                }\n                                else\n                                    row\n                            }\n                        }\n                    })\n                        .filter(_ != null)\n                })\n        })\n\n        rddPoint.mapPartitions(_.map(row => {\n\n            val (point, sortSetSqDist, _) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]\n\n            //            val point = row._2._1 match { case pt: Point => pt }\n            //            val sortSetSqDist = row._2._2\n\n            (point, sortSetSqDist.map(nd => (nd.distance, nd.data match { case pt: Point => pt })))\n        }))\n    }\n\n    private def reducerListBuffer[T](lst1: ListBuffer[T], lst2: ListBuffer[T]): ListBuffer[T] =\n        if (lst1 == null && lst2 == null) lst1\n        else if (lst1 == null) lst2\n        else if (lst2 == null) lst1\n        else\n            lst1 ++ lst2\n\n    private def binarySearch(arrHorizDist: Array[(Double, Double, Long)], pointX: Long): Int = {\n\n        var topIdx = 0\n        var botIdx = arrHorizDist.length - 1\n\n        while (botIdx >= topIdx) {\n\n            val midIdx = (topIdx + botIdx) / 2\n            val midRegion = arrHorizDist(midIdx)\n\n            if (pointX >= midRegion._1 && pointX <= midRegion._2)\n                return midIdx\n            else if (pointX < midRegion._1)\n                botIdx = midIdx - 1\n            else\n                topIdx = midIdx + 1\n        }\n\n        throw new Exception(\"binarySearch() for %,d failed in horizontal distribution %s\".format(pointX, arrHorizDist.toString))\n    }\n\n    private def getDS1Stats(iter: Iterator[Point]) = {\n\n        val (maxRowSize: Int, rowCount: Long, minX: Double, maxX: Double) = iter.fold(Int.MinValue, 0L, Double.MaxValue, Double.MinValue)((x, y) => {\n\n            val (a, b, c, d) = x.asInstanceOf[(Int, Long, Double, Double)]\n            val point = y match { case pt: Point => pt }\n\n            (math.max(a, point.userData.toString.length), b + 1, math.min(c, point.x), math.max(d, point.x))\n        })\n\n        (maxRowSize, rowCount, minX, maxX)\n    }\n\n    private def computeCapacity(rddRight: RDD[Point], k: Int) = {\n\n        // 7% reduction in memory to account for overhead operations\n        var execAvailableMemory = Helper.toByte(rddRight.context.getConf.get(\"spark.executor.memory\", rddRight.context.getExecutorMemoryStatus.map(_._2._1).max + \"B\")) // skips memory of core assigned for Hadoop daemon\n        // deduct yarn overhead\n        val exeOverheadMemory = math.ceil(math.max(384, 0.1 * execAvailableMemory)).toLong\n\n        val (maxRowSize, totalRowCount, minX, minY, maxX, maxY) = rddRight.mapPartitionsWithIndex((pIdx, iter) => {\n\n            val (maxRowSize: Int, totalRowCount: Long, minX: Double, minY: Double, maxX: Double, maxY: Double) = iter.fold(Int.MinValue, 0L, Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue)((tuple, point) => {\n\n                val (maxRowSize, totalRowCount, minX, minY, maxX, maxY) = tuple.asInstanceOf[(Int, Long, Double, Double, Double, Double)]\n                val pnt = point match { case pt: Point => pt }\n\n                (math.max(maxRowSize, pnt.userData.toString.length), totalRowCount + 1, math.min(minX, pnt.x), math.min(minY, pnt.y), math.max(maxX, pnt.x), math.max(maxY, pnt.y))\n            })\n\n            Iterator((maxRowSize, totalRowCount, minX, minY, maxX, maxY))\n        })\n            .fold((0, 0L, Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue))((t1, t2) => (math.max(t1._1, t2._1), t1._2 + t2._2, math.min(t1._3, t2._3), math.min(t1._4, t2._4), math.max(t1._5, t2._5), math.max(t1._6, t2._6)))\n\n        //        val gmGeomDummy = GMPoint((0 until maxRowSize).map(_ => \" \").mkString(\"\"), (0, 0))\n        val userData = Array.fill[Char](maxRowSize)(' ').toString\n        val pointDummy = new Point(0, 0, userData)\n        val quadTreeEmptyDummy = new QuadTreeInfo(Box(new Point(pointDummy), new Point(pointDummy)))\n        val sortSetDummy = SortSetObj(k, false)\n\n        val pointCost = SizeEstimator.estimate(pointDummy)\n        val sortSetCost = /* pointCost + */ SizeEstimator.estimate(sortSetDummy) + (k * pointCost)\n        val quadTreeCost = SizeEstimator.estimate(quadTreeEmptyDummy)\n\n        // exec mem cost = 1QT + 1Pt and matches\n        var execRowCapacity = (((execAvailableMemory - exeOverheadMemory - quadTreeCost) / pointCost) /*- (pointCost + sortSetCost)*/ ).toInt\n\n        var numParts = math.ceil(totalRowCount.toDouble / execRowCapacity).toInt\n\n        if (numParts == 1) {\n\n            numParts = rddRight.getNumPartitions\n            execRowCapacity = (totalRowCount / numParts).toInt\n        }\n\n        (execRowCapacity, totalRowCount, (minX, minY, maxX, maxY))\n    }\n\n    private def computeMBR(lstPoint: List[Point]) = {\n\n        val (minX: Double, minY: Double, maxX: Double, maxY: Double) = lstPoint.fold(Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue)((mbr, point) => {\n\n            val (a, b, c, d) = mbr.asInstanceOf[(Double, Double, Double, Double)]\n            val pt = point match { case pt: Point => pt }\n\n            (math.min(a, pt.x), math.min(b, pt.y), math.max(c, pt.x), math.max(d, pt.y))\n        })\n\n        (minX, minY, maxX, maxY)\n    }\n\n    private def computePartitionRanges(rddRight: RDD[Point], execRowCapacity: Int) = {\n\n        val arrRangesInfo = rddRight\n            .mapPartitions(_.map(point => ((point.x / execRowCapacity).toLong, 1L)))\n            .reduceByKey(_ + _)\n            .collect\n            .sortBy(_._1) // by bucket #\n            .map(row => {\n\n                val start = row._1 * execRowCapacity\n                val end = start + execRowCapacity - 1\n\n                (start, end, row._2)\n            })\n\n        val lstHorizRange = ListBuffer[(Double, Double, Long)]()\n        var start = arrRangesInfo(0)._1\n        var size = 0L\n        var idx = 0\n\n        while (idx < arrRangesInfo.size) {\n\n            var row = arrRangesInfo(idx)\n\n            while (size == 0 && row._3 >= execRowCapacity) {\n\n                val end = row._1 + math.ceil((row._2 - row._1) * (execRowCapacity / row._3.toFloat)).toLong\n\n                lstHorizRange.append((row._1, end, execRowCapacity))\n\n                start = end + 1\n                row = (start, row._2, row._3 - execRowCapacity)\n            }\n\n            val needCount = execRowCapacity - size\n            val availCount = if (needCount > row._3) row._3 else needCount\n\n            size += availCount\n\n            if (size == execRowCapacity || idx == arrRangesInfo.size - 1) {\n\n                if (availCount == row._3) {\n\n                    lstHorizRange.append((start, row._2, size))\n                    idx += 1\n                    if (idx < arrRangesInfo.size) start = arrRangesInfo(idx)._1\n                }\n                else {\n\n                    // val end = start + execRowCapacity - 1\n                    val end = row._1 + math.ceil((row._2 - row._1) * (availCount / row._3.toFloat)).toLong\n\n                    lstHorizRange.append((start, end, size))\n                    start = end + 1\n\n                    arrRangesInfo(idx) = (start, row._2, row._3 - availCount)\n                }\n\n                size = 0\n            }\n            else\n                idx += 1\n        }\n\n        lstHorizRange.toArray\n    }\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala	(date 1595046851009)
@@ -1,8 +1,8 @@
 package org.cusp.bdi.sknn
 
 import scala.collection.mutable.ListBuffer
+import scala.collection.mutable.HashMap
 import scala.util.Random
-
 import org.apache.spark.Partitioner
 import org.apache.spark.rdd.RDD
 import org.apache.spark.storage.StorageLevel
@@ -14,453 +14,505 @@
 import org.cusp.bdi.sknn.util.QuadTreeOperations
 import org.cusp.bdi.sknn.util.SortSetObj
 import org.cusp.bdi.util.Helper
-
 import com.insightfullogic.quad_trees.Box
 import com.insightfullogic.quad_trees.Point
 import com.insightfullogic.quad_trees.QuadTree
 import com.insightfullogic.quad_trees.QuadTreeDigest
 
-case class QTPartId(uniqueIdentifier: Int, totalPoints: Long) {
-    var assignedPart = -1
+import scala.collection.immutable
+
+case class PartitionInfo(minX: Double, minY: Double, maxX: Double, maxY: Double, uniqueIdentifier: Int, totalPoints: Long) {
+
+  var assignedPart = -1
 
-    override def toString() =
-        "%d\t%d\t%d".format(assignedPart, uniqueIdentifier, totalPoints)
+  override def toString() =
+    "%d\t%d\t%d".format(assignedPart, uniqueIdentifier, totalPoints)
 }
 
 object SparkKNN {
 
-    def getSparkKNNClasses(): Array[Class[_]] =
-        Array(classOf[QuadTree],
-              classOf[QuadTreeInfo],
-              classOf[GridOperation],
-              QuadTreeDigestOperations.getClass,
-              QuadTreeOperations.getClass,
-              Helper.getClass,
-              classOf[QuadTreeInfo],
-              classOf[SortSetObj],
-              classOf[Box],
-              classOf[Point],
-              classOf[QuadTree],
-              classOf[QuadTreeDigest])
+  def getSparkKNNClasses(): Array[Class[_]] =
+    Array(classOf[QuadTree],
+      classOf[QuadTreeInfo],
+      classOf[GridOperation],
+      QuadTreeDigestOperations.getClass,
+      QuadTreeOperations.getClass,
+      Helper.getClass,
+      classOf[QuadTreeInfo],
+      classOf[SortSetObj],
+      classOf[Box],
+      classOf[Point],
+      classOf[QuadTree],
+      classOf[QuadTreeDigest])
 }
 
 case class SparkKNN(rddLeft: RDD[Point], rddRight: RDD[Point], k: Int) {
 
-    // for testing, remove ...
-    var minPartitions = 0
+  // for testing, remove ...
+  var minPartitions = 0
 
-    def allKnnJoin(): RDD[(Point, Iterable[(Double, Point)])] =
-        knnJoin(rddLeft, rddRight, k).union(knnJoin(rddRight, rddLeft, k))
+  def allKnnJoin(): RDD[(Point, Iterable[(Double, Point)])] =
+    knnJoin(rddLeft, rddRight, k).union(knnJoin(rddRight, rddLeft, k))
 
-    def knnJoin(): RDD[(Point, Iterable[(Double, Point)])] =
-        knnJoin(rddLeft, rddRight, k)
+  def knnJoin(): RDD[(Point, Iterable[(Double, Point)])] =
+    knnJoin(rddLeft, rddRight, k)
 
-    private def knnJoin(rddLeft: RDD[Point], rddRight: RDD[Point], k: Int): RDD[(Point, Iterable[(Double, Point)])] = {
+  private def knnJoin(rddLeft: RDD[Point], rddRight: RDD[Point], k: Int): RDD[(Point, Iterable[(Double, Point)])] /*: RDD[(Point, Iterable[(Double, Point)])]*/ = {
 
-        var (execRowCapacity, totalRowCount, mbrDS1) = computeCapacity(rddRight, k)
+    var (execRowCapacity, totalRowCount) = computeCapacity(rddRight, k)
 
-        //        execRowCapacity = 57702
+    //        execRowCapacity = 57702
 
-        //        println(">>" + execRowCapacity)
+    //        println(">>" + execRowCapacity)
 
-        var arrHorizDist = computePartitionRanges(rddRight, execRowCapacity)
+    var arrPartRangeCount = computePartitionRanges(rddRight, execRowCapacity)
 
-        val partitionerByX = new Partitioner() {
+    var arrPartInf = rddRight.mapPartitions(_.map(point => (point.x, point.y)))
+      .repartitionAndSortWithinPartitions(new Partitioner() {
 
-            override def numPartitions = arrHorizDist.size
+        // places in containers along the x-axis and sort by x-coor
+        override def numPartitions = arrPartRangeCount.size
 
-            override def getPartition(key: Any): Int =
-                key match {
-                    case xCoord: Double => binarySearch(arrHorizDist, xCoord.toLong)
-                }
-        }
-
-        val rddQuadTree = rddRight.mapPartitions(_.map(point => (point.x, point)))
-            .repartitionAndSortWithinPartitions(partitionerByX) // places in containers along the x-axis and sort by x-coor
-            .mapPartitionsWithIndex((pIdx, iter) => {
+        override def getPartition(key: Any): Int =
+          key match {
+            case xCoord: Double => binarySearchArr(arrPartRangeCount, xCoord.toLong)
+          }
+      })
+      .mapPartitionsWithIndex((pIdx, iter) => {
 
-                val lstQT = ListBuffer[QuadTreeInfo]()
-                var qtInf: QuadTreeInfo = null
+        val lstPartitionRangeCount = ListBuffer[PartitionInfo]()
+
+        var (minX, minY) = iter.next
+        var (maxX, maxY) = (minX, minY)
+        var count = 1
 
-                while (iter.hasNext) {
+        while (iter.hasNext) {
 
-                    val lstPoint = iter.take(execRowCapacity - 1).map(_._2).toList // -1 for the one in var point
+          var xy = iter.next
+          count += 1
+
+          maxX = xy._1
+
+          if (xy._2 < minY) minY = xy._2
+          else if (xy._2 > maxY) maxY = xy._2
+
+          if (count == execRowCapacity || !iter.hasNext) {
+
+            lstPartitionRangeCount.append(PartitionInfo(minX, minY, maxX, maxY, Random.nextInt(), count))
+
+            if (iter.hasNext) {
+
+              xy = iter.next
+              minX = xy._1
+              minY = xy._2
+              maxX = minX
+              maxY = minY
+              count = 1
+            }
+          }
+        }
+
+        lstPartitionRangeCount.iterator
+      })
+      .collect
+      .sortBy(_.minX)
 
-                    var (minX, minY, maxX, maxY) = computeMBR(lstPoint)
+    arrPartRangeCount = null
 
-                    minX = minX.toLong
-                    minY = minY.toLong
-                    maxX = maxX.toLong + 1
-                    maxY = maxY.toLong + 1
+    val actualPartitionCount = AssignToPartitions(arrPartInf, execRowCapacity).getPartitionCount
+
+    //    arrPartInf.foreach(pInf => println(">1>\t%.8f\t%.8f\t%.8f\t%.8f\t%d\t%d\t%d".format(pInf.minX, pInf.minY, pInf.maxX, pInf.maxY, pInf.totalPoints, pInf.assignedPart, pInf.uniqueIdentifier)))
+
+    val rddSpIdx = rddRight
+      .mapPartitions(_.map(point => (point, 0.toByte)))
+      .partitionBy(new Partitioner() {
+        override def numPartitions: Int = actualPartitionCount
+
+        override def getPartition(key: Any): Int = key match {
+          case point: Point => binarySearchPartInf(arrPartInf, point.x).assignedPart
+        }
+      })
+      .mapPartitionsWithIndex((pIdx, iter) => {
+
+        val mapSpIdx = HashMap[Int, QuadTreeInfo]()
+        var qtInf: QuadTreeInfo = null
+
+        iter.foreach(row => {
+
+          val partInf = binarySearchPartInf(arrPartInf, row._1.x)
+
+          val spIdx = mapSpIdx.getOrElse(partInf.uniqueIdentifier, {
+
+            val minX = partInf.minX.toLong
+            val minY = partInf.minY.toLong
+            val maxX = partInf.maxX.toLong + 1
+            val maxY = partInf.maxY.toLong + 1
 
-                    val halfWidth = (maxX - minX) / 2.0
-                    val halfHeight = (maxY - minY) / 2.0
+            val halfWidth = (maxX - minX) / 2.0
+            val halfHeight = (maxY - minY) / 2.0
 
-                    qtInf = new QuadTreeInfo(Box(new Point(halfWidth + minX, halfHeight + minY), new Point(halfWidth, halfHeight)))
+            val newIdx = new QuadTreeInfo(Box(new Point(halfWidth + minX, halfHeight + minY), new Point(halfWidth, halfHeight)))
+            newIdx.uniqueIdentifier = partInf.uniqueIdentifier
 
-                    qtInf.quadTree.insert(lstPoint)
+            mapSpIdx += (partInf.uniqueIdentifier -> newIdx)
 
-                    qtInf.uniqueIdentifier = Random.nextInt()
+            newIdx
+          })
 
-                    lstQT.append(qtInf)
-                }
+          spIdx.quadTree.insert(row._1)
+        })
 
-                //                lstQT.foreach(qtInf => println(">>%s".format(qtInf.toString())))
-
-                lstQT.iterator
-            }, true)
-            .persist(StorageLevel.MEMORY_ONLY)
+        mapSpIdx.valuesIterator
+      }, true)
+      .persist(StorageLevel.MEMORY_ONLY)
 
-        // (box#, Count)
-        var arrGridAndQTInf = rddQuadTree
-            .mapPartitionsWithIndex((pIdx, iter) => {
+    //    rddSpIdx.foreach(qtInf => println(">2>\t%s".format(qtInf.toString())))
+
+    val mbrDS1 = arrPartInf.map(partInf => (partInf.minX, partInf.minY, partInf.maxX, partInf.maxY))
+      .fold((Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue))((mbr1, mbr2) => (math.min(mbr1._1, mbr2._1), math.min(mbr1._2, mbr2._2), math.max(mbr1._3, mbr2._3), math.max(mbr1._4, mbr2._4)))
+
+    // (box#, Count)
+    var arrGridAndSpIdxInf = rddSpIdx
+      .mapPartitionsWithIndex((pIdx, iter) => {
 
-                val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
+        val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
 
-                iter.map(qtInf => {
-
-                    val iterPoint = qtInf.quadTree.getAllPoints.iterator
-
-                    iterPoint.map(point => {
-
-                        //                        if (point.userData.toString().equalsIgnoreCase("taxi_b_918654"))
-                        //                            println(gridOp.computeBoxXY(point.xy))
-
-                        if (iterPoint.hasNext)
-                            (gridOp.computeBoxXY(point.x, point.y), (1L, Set(qtInf.uniqueIdentifier), null, null))
-                        else
-                            (gridOp.computeBoxXY(point.x, point.y), (1L, Set(qtInf.uniqueIdentifier), ListBuffer((qtInf.uniqueIdentifier, qtInf.quadTree.getTotalPoints)), ListBuffer(qtInf.quadTree.getMBR)))
-                    })
-                })
-                    .flatMap(_.seq)
-            })
-            .reduceByKey((x, y) => (x._1 + y._1, x._2 ++ y._2, reducerListBuffer(x._3, y._3), reducerListBuffer(x._4, y._4)))
-            .collect
+        iter.map(qtInf =>
+          qtInf.quadTree.getAllPoints.iterator.map(point =>
+            (gridOp.computeBoxXY(point.x, point.y), (1L, Set(qtInf.uniqueIdentifier)))
+          )
+        )
+          .flatMap(_.seq)
+      })
+      .reduceByKey((x, y) => (x._1 + y._1, x._2 ++ y._2))
+      .collect
 
-        //        arrGridAndQTInf.foreach(row => "<>\t%d\t%d\t%d".format(row._1._1, row._1._2, row._2._1))
-
-        arrHorizDist = null
-
-        val arrQTmbr = arrGridAndQTInf.map(_._2._4).filter(_ != null).flatMap(_.seq)
-
-        var arrAttrQT = arrGridAndQTInf
-            .map(_._2._3)
-            .filter(_ != null)
-            .flatMap(_.seq)
-            .map(row => QTPartId(row._1, row._2))
-
-        val actualPartitionCount = AssignToPartitions(arrAttrQT, execRowCapacity).getPartitionCount
-
-        //        arrAttrQT.foreach(qtdInf => println(">>\t%s".format(qtdInf.toString())))
-
-        val bvMapPartAssign = rddLeft.context.broadcast(arrAttrQT.map(qtPId => (qtPId.uniqueIdentifier, qtPId.assignedPart)).toMap)
-
-        arrAttrQT = null
-
-        val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
+    val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
 
-        var leftBot = gridOp.computeBoxXY(mbrDS1._1, mbrDS1._2)
-        var rightTop = gridOp.computeBoxXY(mbrDS1._3, mbrDS1._4)
+    var leftBot = gridOp.computeBoxXY(mbrDS1._1, mbrDS1._2)
+    var rightTop = gridOp.computeBoxXY(mbrDS1._3, mbrDS1._4)
 
-        val halfWidth = ((rightTop._1 - leftBot._1).toLong + 1) / 2.0
-        val halfHeight = ((rightTop._2 - leftBot._2).toLong + 1) / 2.0
+    val halfWidth = ((rightTop._1 - leftBot._1).toLong + 1) / 2.0
+    val halfHeight = ((rightTop._2 - leftBot._2).toLong + 1) / 2.0
 
-        val qtGlobalIndex = new QuadTreeDigest(Box(new Point(halfWidth + leftBot._1, halfHeight + leftBot._2), new Point(halfWidth, halfHeight)))
-
-        arrGridAndQTInf.foreach(row => qtGlobalIndex.insert((row._1._1, row._1._2), row._2._1, row._2._2))
+    val globalIndex = new QuadTreeDigest(Box(new Point(halfWidth + leftBot._1, halfHeight + leftBot._2), new Point(halfWidth, halfHeight)))
 
-        //        println(qtGlobalIndex)
+    arrGridAndSpIdxInf.foreach(row => globalIndex.insert((row._1._1, row._1._2), row._2._1, row._2._2))
 
-        //        mapGridSummary.foreach(x => println(">>\t%d\t%d\t%s".format(x._1, x._2._1, x._2._2.toString)))
 
-        arrGridAndQTInf = null
+    val bvQTGlobalIndex = rddLeft.context.broadcast(globalIndex)
 
-        //        println(SizeEstimator.estimate(qtGlobalIndex))
-
-        val bvQTGlobalIndex = rddLeft.context.broadcast(qtGlobalIndex)
+    val bvMapUIdPartId = rddLeft.context.broadcast(arrPartInf.map(partInf => (partInf.uniqueIdentifier, partInf.assignedPart)).toMap)
 
-        var rddPoint = rddLeft
-            .mapPartitions(iter => {
+    var rddPoint = rddLeft
+      .mapPartitions(iter => {
 
-                val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
+        val gridOp = new GridOperation(mbrDS1, totalRowCount, k)
 
-                iter.map(point => {
+        iter.map(point => {
 
-                    //                    if (point.userData.toString().equalsIgnoreCase("yellow_1_b_548388"))
-                    //                        println
+          //                    if (point.userData.toString().equalsIgnoreCase("yellow_1_b_548388"))
+          //                        println
 
-                    val lstUId = QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.x, point.y), k)
-                        .toList
+          val lstUId = QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.x, point.y), k)
+            .toList
 
-                    //                    if (lstUId.size >= 18)
-                    //                        println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.xy), k, gridOp.getBoxW, gridOp.getBoxH))
+          //  if (lstUId.size >= 18)
+          //    println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(point.xy), k, gridOp.getBoxW, gridOp.getBoxH))
 
-                    val tuple: Any = (point, SortSetObj(k, false), lstUId)
+          val tuple: Any = (point, SortSetObj(k, false), lstUId)
 
-                    (bvMapPartAssign.value.get(lstUId.head).get, tuple)
-                })
-            })
+          (bvMapUIdPartId.value.get(lstUId.head).get, tuple)
+        })
+      })
 
-        //        println("<>" + rddPoint.mapPartitions(_.map(_._2.asInstanceOf[(Point, SortSetObj, List[Int])]._3.size)).max)
-
-        //        println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(1013487.21, 134367.52), k))
+    //        println("<>" + rddPoint.mapPartitions(_.map(_._2.asInstanceOf[(Point, SortSetObj, List[Int])]._3.size)).max)
+    //        println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(1013487.21, 134367.52), k))
 
-        val numRounds = arrQTmbr.map(mbr => {
+    val numRounds = arrPartInf
+      .map(partInf => (partInf.minX, partInf.minY, partInf.maxX, partInf.maxY))
+      .map(mbr => {
 
-            val lowerLeft = (mbr._1, mbr._2)
-            val upperRight = (mbr._3, mbr._4)
+        val lowerLeft = (mbr._1, mbr._2)
+        val upperRight = (mbr._3, mbr._4)
 
-            //            if (QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapPartAssign.value.get(_).get).size >= 9)
-            //                println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapPartAssign.value.get(_).get))
+        //            if (QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapUIdPartId.value.get(_).get).size >= 9)
+        //                println(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k, gridOp.getBoxW, gridOp.getBoxH).map(bvMapUIdPartId.value.get(_).get))
 
-            List(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, lowerLeft._2), k).map(bvMapPartAssign.value.get(_).get).size,
-                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, lowerLeft._2,k).size,
-                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, lowerLeft._2), k).map(bvMapPartAssign.value.get(_).get).size,
-                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, upperRight._1, (upperRight._2 - lowerLeft._2) / 2,k,bvMapPartAssign.value).size,
-                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, upperRight._2), k).map(bvMapPartAssign.value.get(_).get).size,
-                 //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, upperRight._2,k).size,
-                 QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k).map(bvMapPartAssign.value.get(_).get).size).max
-        }).max
+        List(QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, lowerLeft._2), k).map(bvMapUIdPartId.value.get(_).get).size,
+          //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, lowerLeft._2,k).size,
+          QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, lowerLeft._2), k).map(bvMapUIdPartId.value.get(_).get).size,
+          //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, upperRight._1, (upperRight._2 - lowerLeft._2) / 2,k,bvMapUIdPartId.value).size,
+          QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(upperRight._1, upperRight._2), k).map(bvMapUIdPartId.value.get(_).get).size,
+          //                 getPartitionsInRange(bvGlobalIndex.value, allQTMBR, (upperRight._1 - lowerLeft._1) / 2, upperRight._2,k).size,
+          QuadTreeDigestOperations.getPartitionsInRange(bvQTGlobalIndex.value, gridOp.computeBoxXY(lowerLeft._1, upperRight._2), k).map(bvMapUIdPartId.value.get(_).get).size).max
+      }).max
 
-        //        println("<>" + numRounds)
+    arrPartInf = null
+
+    //        println("<>" + numRounds)
 
-        (0 until numRounds).foreach(roundNumber => {
+    (0 until numRounds).foreach(roundNumber => {
 
-            rddPoint = rddQuadTree
-                .mapPartitions(_.map(qtInf => {
+      rddPoint = rddSpIdx
+        .mapPartitions(_.map(qtInf => {
 
-                    val tuple: Any = qtInf
+          val tuple: Any = qtInf
 
-                    (bvMapPartAssign.value.get(qtInf.uniqueIdentifier).get, tuple)
-                }) /*, true*/ )
-                .union(rddPoint)
-                .partitionBy(new Partitioner() {
+          (bvMapUIdPartId.value.get(qtInf.uniqueIdentifier).get, tuple)
+        }) /*, true*/)
+        .union(rddPoint)
+        .partitionBy(new Partitioner() {
 
-                    override def numPartitions = actualPartitionCount
-                    override def getPartition(key: Any): Int =
-                        key match {
-                            case partId: Int => partId
-                        }
-                })
-                .mapPartitionsWithIndex((pIdx, iter) => {
+          override def numPartitions = actualPartitionCount
+
+          override def getPartition(key: Any): Int =
+            key match {
+              case partId: Int => partId
+            }
+        })
+        .mapPartitionsWithIndex((pIdx, iter) => {
 
-                    val lstPartQT = ListBuffer[QuadTreeInfo]()
+          val lstPartQT = ListBuffer[QuadTreeInfo]()
 
-                    iter.map(row => {
-                        row._2 match {
+          iter.map(row => {
+            row._2 match {
 
-                            case qtInf: QuadTreeInfo => {
+              case qtInf: QuadTreeInfo => {
 
-                                lstPartQT += qtInf
+                lstPartQT += qtInf
 
-                                null
-                            }
-                            case _ => {
+                null
+              }
+              case _ => {
 
-                                val (point, sortSetSqDist, lstUId) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]
+                val (point, sortSetSqDist, lstUId) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]
 
-                                //                                if (point.userData.toString().equalsIgnoreCase("taxi_b_601998"))
-                                //                                    println(pIdx)
+                //                                if (point.userData.toString().equalsIgnoreCase("taxi_b_601998"))
+                //                                    println(pIdx)
 
-                                if (!lstUId.isEmpty) {
+                if (!lstUId.isEmpty) {
 
-                                    // build a list of QT to check
-                                    val lstVisitQTInf = lstPartQT.filter(qtInf => lstUId.contains(qtInf.uniqueIdentifier))
+                  // build a list of QT to check
+                  val lstVisitQTInf = lstPartQT.filter(qtInf => lstUId.contains(qtInf.uniqueIdentifier))
 
-                                    //                                    if (point.userData.toString().equalsIgnoreCase("yellow_1_b_548388"))
-                                    //                                        println(pIdx)
+                  //  if (point.userData.toString().equalsIgnoreCase("yellow_1_b_548388"))
+                  //    println(pIdx)
 
-                                    QuadTreeOperations.nearestNeighbor(lstVisitQTInf, point, sortSetSqDist, k)
+                  QuadTreeOperations.nearestNeighbor(lstVisitQTInf, point, sortSetSqDist, k)
 
-                                    val lstLeftQTInf = lstUId.filterNot(lstVisitQTInf.map(_.uniqueIdentifier).contains _)
+                  val lstLeftQTInf = lstUId.filterNot(lstVisitQTInf.map(_.uniqueIdentifier).contains _)
 
-                                    val ret = (if (lstLeftQTInf.isEmpty) row._1 else bvMapPartAssign.value.get(lstLeftQTInf.head).get, (point, sortSetSqDist, lstLeftQTInf))
+                  val ret = (if (lstLeftQTInf.isEmpty) row._1 else bvMapUIdPartId.value.get(lstLeftQTInf.head).get, (point, sortSetSqDist, lstLeftQTInf))
 
-                                    ret
-                                }
-                                else
-                                    row
-                            }
-                        }
-                    })
-                        .filter(_ != null)
-                })
-        })
+                  ret
+                }
+                else
+                  row
+              }
+            }
+          })
+            .filter(_ != null)
+        })
+    })
 
-        rddPoint.mapPartitions(_.map(row => {
+    rddPoint.mapPartitions(_.map(row => {
 
-            val (point, sortSetSqDist, _) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]
+      val (point, sortSetSqDist, _) = row._2.asInstanceOf[(Point, SortSetObj, List[Int])]
 
-            //            val point = row._2._1 match { case pt: Point => pt }
-            //            val sortSetSqDist = row._2._2
+      //            val point = row._2._1 match { case pt: Point => pt }
+      //            val sortSetSqDist = row._2._2
 
-            (point, sortSetSqDist.map(nd => (nd.distance, nd.data match { case pt: Point => pt })))
-        }))
-    }
+      (point, sortSetSqDist.map(nd => (nd.distance, nd.data match {
+        case pt: Point => pt
+      })))
+    }))
+  }
 
-    private def reducerListBuffer[T](lst1: ListBuffer[T], lst2: ListBuffer[T]): ListBuffer[T] =
-        if (lst1 == null && lst2 == null) lst1
-        else if (lst1 == null) lst2
-        else if (lst2 == null) lst1
-        else
-            lst1 ++ lst2
-
-    private def binarySearch(arrHorizDist: Array[(Double, Double, Long)], pointX: Long): Int = {
+  private def binarySearchArr(arrHorizDist: Array[(Double, Double)], pointX: Long): Int = {
 
-        var topIdx = 0
-        var botIdx = arrHorizDist.length - 1
+    var topIdx = 0
+    var botIdx = arrHorizDist.length - 1
 
-        while (botIdx >= topIdx) {
+    while (botIdx >= topIdx) {
 
-            val midIdx = (topIdx + botIdx) / 2
-            val midRegion = arrHorizDist(midIdx)
+      val midIdx = (topIdx + botIdx) / 2
+      val midRegion = arrHorizDist(midIdx)
 
-            if (pointX >= midRegion._1 && pointX <= midRegion._2)
-                return midIdx
-            else if (pointX < midRegion._1)
-                botIdx = midIdx - 1
-            else
-                topIdx = midIdx + 1
-        }
+      if (pointX >= midRegion._1 && pointX <= midRegion._2)
+        return midIdx
+      else if (pointX < midRegion._1)
+        botIdx = midIdx - 1
+      else
+        topIdx = midIdx + 1
+    }
+
+    throw new Exception("binarySearchArr() for %,d failed in horizontal distribution %s".format(pointX, arrHorizDist.toString))
+  }
+
+  private def binarySearchPartInf(arrPartInf: Array[PartitionInfo], pointX: Double): PartitionInfo = {
+
+    var topIdx = 0
+    var botIdx = arrPartInf.length - 1
+
+    while (botIdx >= topIdx) {
+
+      val midIdx = (topIdx + botIdx) / 2
+      val midRegion = arrPartInf(midIdx)
+
+      if (pointX >= midRegion.minX && pointX <= midRegion.maxX)
+        return midRegion
+      else if (pointX < midRegion.minX)
+        botIdx = midIdx - 1
+      else
+        topIdx = midIdx + 1
+    }
 
-        throw new Exception("binarySearch() for %,d failed in horizontal distribution %s".format(pointX, arrHorizDist.toString))
-    }
+    throw new Exception("binarySearchPartInf() for %,d failed in horizontal distribution %s".format(pointX, arrPartInf.toString))
+  }
 
-    private def getDS1Stats(iter: Iterator[Point]) = {
-
-        val (maxRowSize: Int, rowCount: Long, minX: Double, maxX: Double) = iter.fold(Int.MinValue, 0L, Double.MaxValue, Double.MinValue)((x, y) => {
-
-            val (a, b, c, d) = x.asInstanceOf[(Int, Long, Double, Double)]
-            val point = y match { case pt: Point => pt }
-
-            (math.max(a, point.userData.toString.length), b + 1, math.min(c, point.x), math.max(d, point.x))
-        })
-
-        (maxRowSize, rowCount, minX, maxX)
-    }
+  //  private def getDS1Stats(iter: Iterator[Point]) = {
+  //
+  //    val (maxRowSize: Int, rowCount: Long, minX: Double, maxX: Double) = iter.fold(Int.MinValue, 0L, Double.MaxValue, Double.MinValue)((x, y) => {
+  //
+  //      val (a, b, c, d) = x.asInstanceOf[(Int, Long, Double, Double)]
+  //      val point = y match {
+  //        case pt: Point => pt
+  //      }
+  //
+  //      (math.max(a, point.userData.toString.length), b + 1, math.min(c, point.x), math.max(d, point.x))
+  //    })
+  //
+  //    (maxRowSize, rowCount, minX, maxX)
+  //  }
 
-    private def computeCapacity(rddRight: RDD[Point], k: Int) = {
+  private def computeCapacity(rddRight: RDD[Point], k: Int) = {
 
-        // 7% reduction in memory to account for overhead operations
-        var execAvailableMemory = Helper.toByte(rddRight.context.getConf.get("spark.executor.memory", rddRight.context.getExecutorMemoryStatus.map(_._2._1).max + "B")) // skips memory of core assigned for Hadoop daemon
-        // deduct yarn overhead
-        val exeOverheadMemory = math.ceil(math.max(384, 0.1 * execAvailableMemory)).toLong
+    // 7% reduction in memory to account for overhead operations
+    var execAvailableMemory = Helper.toByte(rddRight.context.getConf.get("spark.executor.memory", rddRight.context.getExecutorMemoryStatus.map(_._2._1).max + "B")) // skips memory of core assigned for Hadoop daemon
+    // deduct yarn overhead
+    val exeOverheadMemory = math.ceil(math.max(384, 0.1 * execAvailableMemory)).toLong
 
-        val (maxRowSize, totalRowCount, minX, minY, maxX, maxY) = rddRight.mapPartitionsWithIndex((pIdx, iter) => {
+    val (maxRowSize, totalRowCount) = rddRight.mapPartitionsWithIndex((pIdx, iter) => {
 
-            val (maxRowSize: Int, totalRowCount: Long, minX: Double, minY: Double, maxX: Double, maxY: Double) = iter.fold(Int.MinValue, 0L, Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue)((tuple, point) => {
+      val (maxRowSize: Int, totalRowCount: Long) = iter.fold(0, 0L)((tuple, point) => {
 
-                val (maxRowSize, totalRowCount, minX, minY, maxX, maxY) = tuple.asInstanceOf[(Int, Long, Double, Double, Double, Double)]
-                val pnt = point match { case pt: Point => pt }
+        val (maxRowSize, totalRowCount) = tuple.asInstanceOf[(Int, Long)]
+        val pnt = point match {
+          case pt: Point => pt
+        }
 
-                (math.max(maxRowSize, pnt.userData.toString.length), totalRowCount + 1, math.min(minX, pnt.x), math.min(minY, pnt.y), math.max(maxX, pnt.x), math.max(maxY, pnt.y))
-            })
+        (math.max(maxRowSize, pnt.userData.toString.length), totalRowCount + 1)
+      })
 
-            Iterator((maxRowSize, totalRowCount, minX, minY, maxX, maxY))
-        })
-            .fold((0, 0L, Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue))((t1, t2) => (math.max(t1._1, t2._1), t1._2 + t2._2, math.min(t1._3, t2._3), math.min(t1._4, t2._4), math.max(t1._5, t2._5), math.max(t1._6, t2._6)))
+      Iterator((maxRowSize, totalRowCount))
+    })
+      .fold((0, 0L))((t1, t2) => (math.max(t1._1, t2._1), t1._2 + t2._2))
 
-        //        val gmGeomDummy = GMPoint((0 until maxRowSize).map(_ => " ").mkString(""), (0, 0))
-        val userData = Array.fill[Char](maxRowSize)(' ').toString
-        val pointDummy = new Point(0, 0, userData)
-        val quadTreeEmptyDummy = new QuadTreeInfo(Box(new Point(pointDummy), new Point(pointDummy)))
-        val sortSetDummy = SortSetObj(k, false)
+    //        val gmGeomDummy = GMPoint((0 until maxRowSize).map(_ => " ").mkString(""), (0, 0))
+    val userData = Array.fill[Char](maxRowSize)(' ').toString
+    val pointDummy = new Point(0, 0, userData)
+    val quadTreeEmptyDummy = new QuadTreeInfo(Box(new Point(pointDummy), new Point(pointDummy)))
+    val sortSetDummy = SortSetObj(k, false)
 
-        val pointCost = SizeEstimator.estimate(pointDummy)
-        val sortSetCost = /* pointCost + */ SizeEstimator.estimate(sortSetDummy) + (k * pointCost)
-        val quadTreeCost = SizeEstimator.estimate(quadTreeEmptyDummy)
+    val pointCost = SizeEstimator.estimate(pointDummy)
+    val sortSetCost = /* pointCost + */ SizeEstimator.estimate(sortSetDummy) + (k * pointCost)
+    val quadTreeCost = SizeEstimator.estimate(quadTreeEmptyDummy)
 
-        // exec mem cost = 1QT + 1Pt and matches
-        var execRowCapacity = (((execAvailableMemory - exeOverheadMemory - quadTreeCost) / pointCost) /*- (pointCost + sortSetCost)*/ ).toInt
+    // exec mem cost = 1QT + 1Pt and matches
+    var execRowCapacity = (((execAvailableMemory - exeOverheadMemory - quadTreeCost) / pointCost)).toInt
 
-        var numParts = math.ceil(totalRowCount.toDouble / execRowCapacity).toInt
+    var numParts = math.ceil(totalRowCount.toDouble / execRowCapacity).toInt
 
-        if (numParts == 1) {
+    if (numParts == 1) {
 
-            numParts = rddRight.getNumPartitions
-            execRowCapacity = (totalRowCount / numParts).toInt
-        }
+      numParts = rddRight.getNumPartitions
+      execRowCapacity = (totalRowCount / numParts).toInt
+    }
 
-        (execRowCapacity, totalRowCount, (minX, minY, maxX, maxY))
-    }
+    (execRowCapacity, totalRowCount)
+  }
 
-    private def computeMBR(lstPoint: List[Point]) = {
+  private def computeMBR(lstPoint: List[Point]) = {
 
-        val (minX: Double, minY: Double, maxX: Double, maxY: Double) = lstPoint.fold(Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue)((mbr, point) => {
+    val (minX: Double, minY: Double, maxX: Double, maxY: Double) = lstPoint.fold(Double.MaxValue, Double.MaxValue, Double.MinValue, Double.MinValue)((mbr, point) => {
 
-            val (a, b, c, d) = mbr.asInstanceOf[(Double, Double, Double, Double)]
-            val pt = point match { case pt: Point => pt }
+      val (a, b, c, d) = mbr.asInstanceOf[(Double, Double, Double, Double)]
+      val pt = point match {
+        case pt: Point => pt
+      }
 
-            (math.min(a, pt.x), math.min(b, pt.y), math.max(c, pt.x), math.max(d, pt.y))
-        })
+      (math.min(a, pt.x), math.min(b, pt.y), math.max(c, pt.x), math.max(d, pt.y))
+    })
 
-        (minX, minY, maxX, maxY)
-    }
+    (minX, minY, maxX, maxY)
+  }
 
-    private def computePartitionRanges(rddRight: RDD[Point], execRowCapacity: Int) = {
+  private def computePartitionRanges(rddRight: RDD[Point], execRowCapacity: Int) = {
 
-        val arrRangesInfo = rddRight
-            .mapPartitions(_.map(point => ((point.x / execRowCapacity).toLong, 1L)))
-            .reduceByKey(_ + _)
-            .collect
-            .sortBy(_._1) // by bucket #
-            .map(row => {
+    val arrContainerRangeAndCount = rddRight
+      .mapPartitions(_.map(point => ((point.x / execRowCapacity).toLong, 1L)))
+      .reduceByKey(_ + _)
+      .collect
+      .sortBy(_._1) // by bucket #
+      .map(row => {
 
-                val start = row._1 * execRowCapacity
-                val end = start + execRowCapacity - 1
+        val start = row._1 * execRowCapacity
+        val end = start + execRowCapacity - 1
 
-                (start, end, row._2)
-            })
+        (start, end, row._2)
+      })
 
-        val lstHorizRange = ListBuffer[(Double, Double, Long)]()
-        var start = arrRangesInfo(0)._1
-        var size = 0L
-        var idx = 0
+    val lstPartRangeCount = ListBuffer[(Double, Double, Long)]()
+    val lastIdx = arrContainerRangeAndCount.size - 1
+    var idx = 0
+    var row = arrContainerRangeAndCount(idx)
+    var start = row._1
+    var totalFound = 0L
+
+    //        arrContainerRangeAndCount.foreach(row => println(">3>\t%d\t%d\t%d".format(row._1, row._2, row._3)))
 
-        while (idx < arrRangesInfo.size) {
+    while (idx <= lastIdx) {
 
-            var row = arrRangesInfo(idx)
+      val borrowCount = execRowCapacity - totalFound
 
-            while (size == 0 && row._3 >= execRowCapacity) {
+      if (borrowCount > row._3 && idx != lastIdx) {
 
-                val end = row._1 + math.ceil((row._2 - row._1) * (execRowCapacity / row._3.toFloat)).toLong
+        totalFound += row._3
+        idx += 1
+        row = arrContainerRangeAndCount(idx)
+      }
+      else {
 
-                lstHorizRange.append((row._1, end, execRowCapacity))
+        val percent = if (borrowCount > row._3.toDouble) 1 else borrowCount / row._3.toDouble
 
-                start = end + 1
-                row = (start, row._2, row._3 - execRowCapacity)
-            }
+        val end = row._1 + math.ceil((row._2 - row._1) * percent).toLong
 
-            val needCount = execRowCapacity - size
-            val availCount = if (needCount > row._3) row._3 else needCount
+        lstPartRangeCount.append((start, end, execRowCapacity))
 
-            size += availCount
+        if (idx == lastIdx)
+          idx += 1
+        else {
 
-            if (size == execRowCapacity || idx == arrRangesInfo.size - 1) {
+          totalFound = 0
 
-                if (availCount == row._3) {
+          if (borrowCount == row._3) {
 
-                    lstHorizRange.append((start, row._2, size))
-                    idx += 1
-                    if (idx < arrRangesInfo.size) start = arrRangesInfo(idx)._1
-                }
-                else {
+            idx += 1
+            row = arrContainerRangeAndCount(idx)
+            start = row._1
+          }
+          else {
 
-                    // val end = start + execRowCapacity - 1
-                    val end = row._1 + math.ceil((row._2 - row._1) * (availCount / row._3.toFloat)).toLong
-
-                    lstHorizRange.append((start, end, size))
-                    start = end + 1
-
-                    arrRangesInfo(idx) = (start, row._2, row._3 - availCount)
-                }
-
-                size = 0
-            }
-            else
-                idx += 1
-        }
+            start = end + 1
+            row = (start, row._2, row._3 - borrowCount)
+          }
+        }
+      }
+    }
 
-        lstHorizRange.toArray
-    }
+    //    lstPartRangeCount.foreach(row => println(">4>\t%.8f\t%.8f\t%d".format(row._1, row._2, row._3)))
+
+    lstPartRangeCount.map(row => (row._1, row._2)).toArray
+  }
 }
\ No newline at end of file
Index: FW_GeoMatch/FW_GeoMatch.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule=\"true\" type=\"JAVA_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\" LANGUAGE_LEVEL=\"JDK_1_8\">\n    <output url=\"file://$MODULE_DIR$/target/classes\" />\n    <output-test url=\"file://$MODULE_DIR$/target/test-classes\" />\n    <content url=\"file://$MODULE_DIR$\">\n      <sourceFolder url=\"file://$MODULE_DIR$/src/main/java\" isTestSource=\"false\" />\n      <sourceFolder url=\"file://$MODULE_DIR$/src/main/resources\" type=\"java-resource\" />\n      <sourceFolder url=\"file://$MODULE_DIR$/src/test/java\" isTestSource=\"true\" />\n      <sourceFolder url=\"file://$MODULE_DIR$/src/main/scala\" isTestSource=\"false\" />\n      <excludeFolder url=\"file://$MODULE_DIR$/target\" />\n    </content>\n    <orderEntry type=\"inheritedJdk\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n    <orderEntry type=\"library\" name=\"scala-sdk-2.11.11\" level=\"application\" />\n    <orderEntry type=\"module\" module-name=\"GeoMatch\" />\n    <orderEntry type=\"module\" module-name=\"Common\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-core_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro:1.8.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-core-asl:1.9.13\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-mapper-asl:1.9.13\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.thoughtworks.paranamer:paranamer:2.7\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-compress:1.8.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.tukaani:xz:1.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro-mapred:hadoop2:1.8.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro-ipc:1.8.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-codec:commons-codec:1.9\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.twitter:chill_2.11:0.9.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.esotericsoftware:kryo-shaded:4.0.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.esotericsoftware:minlog:1.3.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.objenesis:objenesis:2.5.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.twitter:chill-java:0.9.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.xbean:xbean-asm6-shaded:4.8\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-client:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-common:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-cli:commons-cli:1.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: xmlenc:xmlenc:0.52\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-httpclient:commons-httpclient:3.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-io:commons-io:2.4\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-collections:commons-collections:3.2.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-lang:commons-lang:2.6\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-configuration:commons-configuration:1.6\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-digester:commons-digester:1.8\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-beanutils:commons-beanutils:1.7.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-beanutils:commons-beanutils-core:1.8.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.google.protobuf:protobuf-java:2.5.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.google.code.gson:gson:2.2.4\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-auth:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.httpcomponents:httpclient:4.2.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.httpcomponents:httpcore:4.2.4\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.server:apacheds-i18n:2.0.0-M15\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.api:api-asn1-api:1.0.0-M20\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.api:api-util:1.0.0-M20\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-client:2.6.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.htrace:htrace-core:3.0.4\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-hdfs:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.mortbay.jetty:jetty-util:6.1.26\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: xerces:xercesImpl:2.9.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: xml-apis:xml-apis:1.3.04\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-app:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-common:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-client:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-server-common:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-shuffle:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-api:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-core:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-common:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.xml.bind:jaxb-api:2.2.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.xml.stream:stax-api:1.0-2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-jaxrs:1.9.13\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-xc:1.9.13\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-jobclient:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-annotations:2.6.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-launcher_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-kvstore_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.fusesource.leveldbjni:leveldbjni-all:1.8\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-core:2.6.7\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-annotations:2.6.7\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-network-common_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-network-shuffle_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-unsafe_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.activation:activation:1.1.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-recipes:2.6.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-framework:2.6.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.google.guava:guava:16.0.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.zookeeper:zookeeper:3.4.6\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.servlet:javax.servlet-api:3.1.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-lang3:3.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-math3:3.4.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.google.code.findbugs:jsr305:1.3.9\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.slf4j:slf4j-api:1.7.16\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.slf4j:jul-to-slf4j:1.7.16\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.slf4j:jcl-over-slf4j:1.7.16\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: log4j:log4j:1.2.17\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.slf4j:slf4j-log4j12:1.7.16\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.ning:compress-lzf:1.0.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.xerial.snappy:snappy-java:1.1.7.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.lz4:lz4-java:1.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.github.luben:zstd-jni:1.3.2-2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.roaringbitmap:RoaringBitmap:0.5.11\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: commons-net:commons-net:3.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scala-library:2.11.12\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-jackson_2.11:3.5.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-core_2.11:3.5.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-ast_2.11:3.5.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-scalap_2.11:3.5.3\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.scala-lang.modules:scala-xml_2.11:1.0.6\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-client:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.ws.rs:javax.ws.rs-api:2.0.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-api:2.4.0-b34\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-utils:2.4.0-b34\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2.external:aopalliance-repackaged:2.4.0-b34\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2.external:javax.inject:2.4.0-b34\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-locator:2.4.0-b34\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.javassist:javassist:3.18.1-GA\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-common:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.annotation:javax.annotation-api:1.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.bundles.repackaged:jersey-guava:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:osgi-resource-locator:1.0.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-server:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.media:jersey-media-jaxb:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: javax.validation:validation-api:1.1.0.Final\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.containers:jersey-container-servlet:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.containers:jersey-container-servlet-core:2.22.2\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.netty:netty-all:4.1.17.Final\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.netty:netty:3.9.9.Final\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.clearspring.analytics:stream:2.7.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-core:3.1.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-jvm:3.1.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-json:3.1.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-graphite:3.1.5\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-databind:2.6.7.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.module:jackson-module-scala_2.11:2.6.7.1\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scala-reflect:2.11.8\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.module:jackson-module-paranamer:2.7.9\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.ivy:ivy:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: oro:oro:2.0.8\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: net.razorvine:pyrolite:4.13\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: net.sf.py4j:py4j:0.10.7\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-tags_2.11:2.4.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-crypto:1.0.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.spark-project.spark:unused:1.0.0\" level=\"project\" />\n    <orderEntry type=\"library\" name=\"Maven: org.locationtech.jts:jts-core:1.16.1\" level=\"project\" />\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- FW_GeoMatch/FW_GeoMatch.iml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ FW_GeoMatch/FW_GeoMatch.iml	(date 1594705299121)
@@ -4,15 +4,13 @@
     <output url="file://$MODULE_DIR$/target/classes" />
     <output-test url="file://$MODULE_DIR$/target/test-classes" />
     <content url="file://$MODULE_DIR$">
-      <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />
-      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
-      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
       <sourceFolder url="file://$MODULE_DIR$/src/main/scala" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
+      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
       <excludeFolder url="file://$MODULE_DIR$/target" />
     </content>
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
-    <orderEntry type="library" name="scala-sdk-2.11.11" level="application" />
     <orderEntry type="module" module-name="GeoMatch" />
     <orderEntry type="module" module-name="Common" />
     <orderEntry type="library" name="Maven: org.apache.spark:spark-core_2.11:2.4.0" level="project" />
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/vcs.xml	(date 1594704491324)
+++ .idea/vcs.xml	(date 1594704491324)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: SpatialBenchmark/SpatialBenchmark.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/SpatialBenchmark.iml	(date 1594757152686)
+++ SpatialBenchmark/SpatialBenchmark.iml	(date 1594757152686)
@@ -0,0 +1,146 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">
+  <component name="NewModuleRootManager" LANGUAGE_LEVEL="JDK_1_8">
+    <output url="file://$MODULE_DIR$/target/classes" />
+    <output-test url="file://$MODULE_DIR$/target/test-classes" />
+    <content url="file://$MODULE_DIR$">
+      <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
+      <sourceFolder url="file://$MODULE_DIR$/src/main/scala" isTestSource="false" />
+      <excludeFolder url="file://$MODULE_DIR$/target" />
+    </content>
+    <orderEntry type="inheritedJdk" />
+    <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="library" name="scala-sdk-2.11.11" level="application" />
+    <orderEntry type="module" module-name="Common" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-core_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.avro:avro:1.8.2" level="project" />
+    <orderEntry type="library" name="Maven: org.codehaus.jackson:jackson-core-asl:1.9.13" level="project" />
+    <orderEntry type="library" name="Maven: org.codehaus.jackson:jackson-mapper-asl:1.9.13" level="project" />
+    <orderEntry type="library" name="Maven: com.thoughtworks.paranamer:paranamer:2.7" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.commons:commons-compress:1.8.1" level="project" />
+    <orderEntry type="library" name="Maven: org.tukaani:xz:1.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.avro:avro-mapred:hadoop2:1.8.2" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.avro:avro-ipc:1.8.2" level="project" />
+    <orderEntry type="library" name="Maven: commons-codec:commons-codec:1.9" level="project" />
+    <orderEntry type="library" name="Maven: com.twitter:chill_2.11:0.9.3" level="project" />
+    <orderEntry type="library" name="Maven: com.esotericsoftware:kryo-shaded:4.0.2" level="project" />
+    <orderEntry type="library" name="Maven: com.esotericsoftware:minlog:1.3.0" level="project" />
+    <orderEntry type="library" name="Maven: org.objenesis:objenesis:2.5.1" level="project" />
+    <orderEntry type="library" name="Maven: com.twitter:chill-java:0.9.3" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.xbean:xbean-asm6-shaded:4.8" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-client:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-common:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: commons-cli:commons-cli:1.2" level="project" />
+    <orderEntry type="library" name="Maven: xmlenc:xmlenc:0.52" level="project" />
+    <orderEntry type="library" name="Maven: commons-httpclient:commons-httpclient:3.1" level="project" />
+    <orderEntry type="library" name="Maven: commons-io:commons-io:2.4" level="project" />
+    <orderEntry type="library" name="Maven: commons-collections:commons-collections:3.2.2" level="project" />
+    <orderEntry type="library" name="Maven: commons-lang:commons-lang:2.6" level="project" />
+    <orderEntry type="library" name="Maven: commons-configuration:commons-configuration:1.6" level="project" />
+    <orderEntry type="library" name="Maven: commons-digester:commons-digester:1.8" level="project" />
+    <orderEntry type="library" name="Maven: commons-beanutils:commons-beanutils:1.7.0" level="project" />
+    <orderEntry type="library" name="Maven: commons-beanutils:commons-beanutils-core:1.8.0" level="project" />
+    <orderEntry type="library" name="Maven: com.google.protobuf:protobuf-java:2.5.0" level="project" />
+    <orderEntry type="library" name="Maven: com.google.code.gson:gson:2.2.4" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-auth:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.httpcomponents:httpclient:4.2.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.httpcomponents:httpcore:4.2.4" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.directory.server:apacheds-i18n:2.0.0-M15" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.directory.api:api-asn1-api:1.0.0-M20" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.directory.api:api-util:1.0.0-M20" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.curator:curator-client:2.6.0" level="project" />
+    <orderEntry type="library" name="Maven: org.htrace:htrace-core:3.0.4" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-hdfs:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.mortbay.jetty:jetty-util:6.1.26" level="project" />
+    <orderEntry type="library" name="Maven: xerces:xercesImpl:2.9.1" level="project" />
+    <orderEntry type="library" name="Maven: xml-apis:xml-apis:1.3.04" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-mapreduce-client-app:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-mapreduce-client-common:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-yarn-client:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-yarn-server-common:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-mapreduce-client-shuffle:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-yarn-api:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-mapreduce-client-core:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-yarn-common:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: javax.xml.bind:jaxb-api:2.2.2" level="project" />
+    <orderEntry type="library" name="Maven: javax.xml.stream:stax-api:1.0-2" level="project" />
+    <orderEntry type="library" name="Maven: org.codehaus.jackson:jackson-jaxrs:1.9.13" level="project" />
+    <orderEntry type="library" name="Maven: org.codehaus.jackson:jackson-xc:1.9.13" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-mapreduce-client-jobclient:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.hadoop:hadoop-annotations:2.6.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-launcher_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-kvstore_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.fusesource.leveldbjni:leveldbjni-all:1.8" level="project" />
+    <orderEntry type="library" name="Maven: com.fasterxml.jackson.core:jackson-core:2.6.7" level="project" />
+    <orderEntry type="library" name="Maven: com.fasterxml.jackson.core:jackson-annotations:2.6.7" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-network-common_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-network-shuffle_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-unsafe_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: javax.activation:activation:1.1.1" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.curator:curator-recipes:2.6.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.curator:curator-framework:2.6.0" level="project" />
+    <orderEntry type="library" name="Maven: com.google.guava:guava:16.0.1" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.zookeeper:zookeeper:3.4.6" level="project" />
+    <orderEntry type="library" name="Maven: javax.servlet:javax.servlet-api:3.1.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.commons:commons-lang3:3.5" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.commons:commons-math3:3.4.1" level="project" />
+    <orderEntry type="library" name="Maven: com.google.code.findbugs:jsr305:1.3.9" level="project" />
+    <orderEntry type="library" name="Maven: org.slf4j:slf4j-api:1.7.16" level="project" />
+    <orderEntry type="library" name="Maven: org.slf4j:jul-to-slf4j:1.7.16" level="project" />
+    <orderEntry type="library" name="Maven: org.slf4j:jcl-over-slf4j:1.7.16" level="project" />
+    <orderEntry type="library" name="Maven: log4j:log4j:1.2.17" level="project" />
+    <orderEntry type="library" name="Maven: org.slf4j:slf4j-log4j12:1.7.16" level="project" />
+    <orderEntry type="library" name="Maven: com.ning:compress-lzf:1.0.3" level="project" />
+    <orderEntry type="library" name="Maven: org.xerial.snappy:snappy-java:1.1.7.1" level="project" />
+    <orderEntry type="library" name="Maven: org.lz4:lz4-java:1.4.0" level="project" />
+    <orderEntry type="library" name="Maven: com.github.luben:zstd-jni:1.3.2-2" level="project" />
+    <orderEntry type="library" name="Maven: org.roaringbitmap:RoaringBitmap:0.5.11" level="project" />
+    <orderEntry type="library" name="Maven: commons-net:commons-net:3.1" level="project" />
+    <orderEntry type="library" name="Maven: org.scala-lang:scala-library:2.11.12" level="project" />
+    <orderEntry type="library" name="Maven: org.json4s:json4s-jackson_2.11:3.5.3" level="project" />
+    <orderEntry type="library" name="Maven: org.json4s:json4s-core_2.11:3.5.3" level="project" />
+    <orderEntry type="library" name="Maven: org.json4s:json4s-ast_2.11:3.5.3" level="project" />
+    <orderEntry type="library" name="Maven: org.json4s:json4s-scalap_2.11:3.5.3" level="project" />
+    <orderEntry type="library" name="Maven: org.scala-lang.modules:scala-xml_2.11:1.0.6" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.core:jersey-client:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: javax.ws.rs:javax.ws.rs-api:2.0.1" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2:hk2-api:2.4.0-b34" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2:hk2-utils:2.4.0-b34" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2.external:aopalliance-repackaged:2.4.0-b34" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2.external:javax.inject:2.4.0-b34" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2:hk2-locator:2.4.0-b34" level="project" />
+    <orderEntry type="library" name="Maven: org.javassist:javassist:3.18.1-GA" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.core:jersey-common:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: javax.annotation:javax.annotation-api:1.2" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.bundles.repackaged:jersey-guava:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.hk2:osgi-resource-locator:1.0.1" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.core:jersey-server:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.media:jersey-media-jaxb:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: javax.validation:validation-api:1.1.0.Final" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.containers:jersey-container-servlet:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: org.glassfish.jersey.containers:jersey-container-servlet-core:2.22.2" level="project" />
+    <orderEntry type="library" name="Maven: io.netty:netty-all:4.1.17.Final" level="project" />
+    <orderEntry type="library" name="Maven: io.netty:netty:3.9.9.Final" level="project" />
+    <orderEntry type="library" name="Maven: com.clearspring.analytics:stream:2.7.0" level="project" />
+    <orderEntry type="library" name="Maven: io.dropwizard.metrics:metrics-core:3.1.5" level="project" />
+    <orderEntry type="library" name="Maven: io.dropwizard.metrics:metrics-jvm:3.1.5" level="project" />
+    <orderEntry type="library" name="Maven: io.dropwizard.metrics:metrics-json:3.1.5" level="project" />
+    <orderEntry type="library" name="Maven: io.dropwizard.metrics:metrics-graphite:3.1.5" level="project" />
+    <orderEntry type="library" name="Maven: com.fasterxml.jackson.core:jackson-databind:2.6.7.1" level="project" />
+    <orderEntry type="library" name="Maven: com.fasterxml.jackson.module:jackson-module-scala_2.11:2.6.7.1" level="project" />
+    <orderEntry type="library" name="Maven: org.scala-lang:scala-reflect:2.11.8" level="project" />
+    <orderEntry type="library" name="Maven: com.fasterxml.jackson.module:jackson-module-paranamer:2.7.9" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.ivy:ivy:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: oro:oro:2.0.8" level="project" />
+    <orderEntry type="library" name="Maven: net.razorvine:pyrolite:4.13" level="project" />
+    <orderEntry type="library" name="Maven: net.sf.py4j:py4j:0.10.7" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.spark:spark-tags_2.11:2.4.0" level="project" />
+    <orderEntry type="library" name="Maven: org.apache.commons:commons-crypto:1.0.0" level="project" />
+    <orderEntry type="library" name="Maven: org.spark-project.spark:unused:1.0.0" level="project" />
+    <orderEntry type="library" name="Maven: org.locationtech.jts:jts-core:1.16.1" level="project" />
+    <orderEntry type="module" module-name="LocalRunFiles" />
+  </component>
+</module>
\ No newline at end of file
Index: SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/HelperObjects.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/HelperObjects.scala	(date 1576181304115)
+++ SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/HelperObjects.scala	(date 1576181304115)
@@ -0,0 +1,47 @@
+package org.cusp.bdi.sb.examples
+
+import org.cusp.bdi.util.LocalRunArgs
+import org.cusp.bdi.util.LocalRunConsts
+
+object SB_Arguments {
+
+    val local = ("local", "Boolean", false, "(T=local, F=cluster)")
+    val debug = ("debug", "Boolean", false, "(T=show_debug, F=no_debug)")
+    val outDir = ("outDir", "String", null, "File location to write benchmark results")
+    val classificationCount = ("classificationCount", "Int", 3, "Number of matches that can be out-of-order when classifying the output. i.e. 3 means positions 0,1,2 can appear as 1,0,2 or 2,0,1 ...")
+    val keyMatchInFile = ("keyMatchInFile", "String", null, "File location of key")
+    val keyMatchInFileParser = ("keyMatchInFileParser", "String", null, "The full class name of the key match result file specified in keyMatchInFile. Pass in the complete class name (package.className); the class should extend the class org.cusp.bdi.sb.BenchmarkInputFileParser. Reflection will be used to instanciate the class")
+    val testFWInFile = ("testFWInFile", "String", null, "Use if the framework's results should be obtained from an input file. Pass the full path of the file and specify the parser class.")
+    val testFWInFileParser = ("testFWInFileParser", "String", null, "The full class name of the framework's result file specified in testFWInFile. Pass in the complete class name (package.className); the class should extend the class org.cusp.bdi.sb.BenchmarkInputFileParser")
+
+    def apply() =
+        List(local, debug, outDir, classificationCount, keyMatchInFile, keyMatchInFileParser, testFWInFile, testFWInFileParser)
+}
+
+object SB_CLArgs {
+
+    /* Key(Param name), Type, Default value, Description */
+
+    val SKNN_RandomPoint_RandomPoint = SB_CLArgs(LocalRunConsts.pathSparkKNN_FW_Output_1, SB_KeyMatchInputFileParser_RandomPoints.getClass.getName, LocalRunConsts.pathSparkKNN_FW_Output_2, SB_KeyMatchInputFileParser_RandomPoints.getClass.getName)
+    //    val SKNN_RandomPoint_RandomPoint = SB_CLArgs(LocalRunConsts.pathKM_RandomPointsNonUniform, SB_KeyMatchInputFileParser_RandomPoints.getClass.getName, LocalRunConsts.pathSparkKNN_FW_Output, SB_KeyMatchInputFileParser_RandomPoints.getClass.getName)
+    //    val SKNN_BusPoint_BusPointShift = SB_CLArgs(LocalRunConsts.pathKM_Bus_SMALL, SB_KeyMatchInputFileParser_Bus_Small.getClass.getName, LocalRunConsts.pathSparkKNN_FW_Output, SB_KeyMatchInputFileParser_Bus_Small.getClass.getName)
+    val GM_LionTPEP = SB_CLArgs(LocalRunConsts.pathKM_TPEP, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP", LocalRunConsts.pathGM_TPEP, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP")
+    val GM_LionTaxi = SB_CLArgs(LocalRunConsts.pathKM_Taxi, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi", LocalRunConsts.pathGM_Taxi, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi")
+    val GS_LionTPEP = SB_CLArgs(LocalRunConsts.pathKM_TPEP, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP", LocalRunConsts.pathGS_TPEP, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP")
+    val GS_LionTaxi = SB_CLArgs(LocalRunConsts.pathKM_Taxi, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi", LocalRunConsts.pathGS_Taxi, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi")
+    val LS_LionTPEP = SB_CLArgs(LocalRunConsts.pathKM_TPEP_WGS84, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP_wgs84", LocalRunConsts.pathLS_wgs_TPEP, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_TPEP_wgs84")
+    val LS_LionTaxi = SB_CLArgs(LocalRunConsts.pathKM_Taxi_WGS84, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi_wgs84", LocalRunConsts.pathLS_wgs_Taxi, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Taxi_wgs84")
+    val LS_LionBus = SB_CLArgs(LocalRunConsts.pathKM_Bus_WGS84, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Bus_wgs84", LocalRunConsts.pathLS_wgs_Bus, "org.cusp.bdi.sb.examples.SB_KeyMatchInputFileParser_Bus_wgs84")
+
+    private def apply(keyMatchInFile: String, kmInputFileParser: String, testFWInFile: String, testFWInFileParser: String) = {
+
+        val additionalParams = StringBuilder.newBuilder
+            .append(" -classificationCount 10")
+            .append(" -keyMatchInFile ").append(keyMatchInFile)
+            .append(" -keyMatchInFileParser ").append(kmInputFileParser)
+            .append(" -testFWInFile ").append(testFWInFile)
+            .append(" -testFWInFileParser ").append(testFWInFileParser)
+
+        LocalRunArgs("", "", "", "", additionalParams, SB_Arguments())
+    }
+}
\ No newline at end of file
Index: SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/SB_KeyMatch_InputFileParser.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/SB_KeyMatch_InputFileParser.scala	(date 1574831528250)
+++ SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/SB_KeyMatch_InputFileParser.scala	(date 1574831528250)
@@ -0,0 +1,109 @@
+package org.cusp.bdi.sb.examples
+
+import org.cusp.bdi.util.Helper
+
+trait BenchmarkInputFileParser extends Serializable {
+
+    def parseLine(line: String): (String, Array[String])
+
+    // "matchNthComma" is the number of the comma separating the record from the street matches
+    // e.g. matchNthComma=11, then the 11th comma is the one separating the point from the street matches
+    protected def commonParseLine = (line: String, latLonNthComma: Int, matchNthComma: Int) => {
+
+        var arrStreetMatches: Array[String] = null
+
+        val idxStreet = Helper.indexOf(line, ",", matchNthComma)
+        var recordLine = StringBuilder.newBuilder.append(line)
+
+        if (idxStreet != -1) {
+
+            arrStreetMatches = line.substring(idxStreet + 1).split(',')
+            recordLine.delete(idxStreet, recordLine.length)
+        }
+
+        val idxCoord = Helper.indexOf(recordLine, ",", latLonNthComma)
+
+        if (idxCoord != -1) {
+
+            // remove lon/lat from line
+            recordLine.delete(idxCoord, Helper.indexOf(recordLine, ",", 2, idxCoord + 1))
+
+        }
+
+        (recordLine.toString(), arrStreetMatches)
+    }
+}
+
+case class SB_KeyMatchInputFileParser_Bus() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 0, 11)
+}
+
+case class SB_KeyMatchInputFileParser_Taxi() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 0, 18)
+}
+
+case class SB_KeyMatchInputFileParser_TPEP_wgs84() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 2, 5)
+}
+
+case class SB_KeyMatchInputFileParser_Taxi_wgs84() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 5, 18)
+}
+
+case class SB_KeyMatchInputFileParser_Bus_wgs84() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 1, 11)
+}
+
+case class SB_KeyMatchInputFileParser_TPEP() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) =
+        commonParseLine(line, 0, 5)
+}
+
+case class SB_KeyMatchInputFileParser_RandomPoints() extends BenchmarkInputFileParser {
+
+    def parseLine(line: String) = {
+
+        val arr = line.split(';')
+
+        (arr(0), arr.slice(1, arr.length))
+    }
+}
+
+//case class SB_KeyMatchInputFileParser_RandomPoints() extends SB_KeyMatchInputFileParser_RandomPoints_Common with BenchmarkInputFileParser {
+//
+//    def parseLine(line: String) =
+//        super.parseLine(line, false)
+//}
+
+//case class SB_KeyMatchInputFileParser_RandomPoints_RemoveFirstMatch() extends SB_KeyMatchInputFileParser_RandomPoints_Common with BenchmarkInputFileParser {
+//
+//    def parseLine(line: String) =
+//        super.parseLine(line, true)
+//}
+
+case class SB_KeyMatchInputFileParser_Bus_Small() extends BenchmarkInputFileParser {
+
+    def parseLine(line: String) = {
+
+        val arr = line.split(';')
+
+        (arr(0), arr.slice(1, arr.length).map(x => x.substring(x.indexOf(',') + 1)))
+    }
+}
+
+case class KM_InputFileParser_OSM_Buildings() extends BenchmarkInputFileParser {
+    def parseLine(line: String): (String, Array[String]) = {
+
+        val idx = Helper.indexOf(line, "))")
+
+        if (idx + 3 == line.size)
+            (line, null)
+        else
+            (line.substring(0, idx + 3), line.substring(idx + 4).split(','))
+    }
+}
\ No newline at end of file
Index: LocalRunFiles/src/main/scala/org/cusp/bdi/util/LocalRunConsts.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.cusp.bdi.util\n\nobject LocalRunConsts {\n\n    val pathOutput = \"/media/cusp/Data/GeoMatch_Files/OutputFiles/\"\n    val sparkWorkDir = \"/media/cusp/Data/GeoMatch_Files/spark_work_dir/\"\n\n    val DS_CODE_TPEP_POINT = \"TPEP_Point\"\n    val DS_CODE_TPEP_POINT_WGS84 = \"TPEP_Point_WGS84\"\n    val DS_CODE_OSM_POINT_WGS84 = \"OSM_Point_WGS84\"\n    val DS_CODE_TAXI_POINT = \"Taxi_Point\"\n    val DS_CODE_THREE_PART_LINE = \"Three_Part_Line\"\n    val DS_CODE_BUS_POINT = \"Bus_Point\"\n    val DS_CODE_BUS_POINT_SHIFTED = \"Bus_Point_shifted\"\n    val DS_CODE_RAND_POINT = \"Rand_Point\"\n\n    val pathLION_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/LION_NYC_Streets_NAD83_Enumerated_WKT.csv\"\n    val pathLION_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/LION_NYC_Streets_Lat_Lon_Enumerated_WKT.csv\"\n    val pathTPEP_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_TPEP_NAD83.csv\" // \"/media/cusp/Data/GeoMatch_Files/InputFiles/TBD.csv\"\n    val pathTPEP_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_TPEP_WGS84.csv\"\n    val pathTaxi_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_Taxi_NAD83.csv\"\n    val pathTaxi_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_Taxi_WGS84.csv\"\n    val pathBus_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_Bus_NAD83.csv\"\n    val pathBus_NAD83_SMALL = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Bus_TripRecod_NAD83_part-00000_SMALL.csv\"\n    val pathBus_NAD83_TINY = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Bus_TripRecod_NAD83_part-00000_TINY.csv\"\n    val pathBus_NAD83_SMALL_SHIFTED = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Bus_TripRecod_NAD83_part-00000_SMALL_shifted(100-300).csv\"\n    val pathBus_NAD83_TINY_SHIFTED = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Bus_TripRecod_NAD83_part-00000_TINY_shifted(100-300).csv\"\n    val pathBus_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_Bus_WGS84.csv\"\n    val pathRandomPointsNonUniformPart1 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/randomPoints_500K_nonUniform_part1.csv\"\n    //    val pathRandomPointsNonUniformPart2 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/randomPoints_500K_nonUniform_part2.csv\"\n    val pathRandomPointsNonUniformPart2 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/randomPoints_1M_nonUniform_part2.csv\"\n\n    val pathOSM_Point_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Trip_Record_OSM_Points.csv\"\n\n    val pathKM_Bus_SMALL = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Bus_TripRecod_NAD83_part-00000_SMALL_key_SORTED_k30.csv\"\n    val pathKM_TPEP = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_LION_LineString_TPEP_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv\"\n    val pathKM_TPEP_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_WGS84_LION_LineString_TPEP_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv\"\n    val pathKM_Taxi = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_LION_LineString_Taxi_Point_ErrCorr_MBRExp_150_MaxDist_1503.1K.csv\"\n    val pathKM_Taxi_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_WGS84_LION_LineString_Taxi_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv\"\n    val pathKM_Bus = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_LION_LineString_Bus_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv\"\n    val pathKM_Bus_WGS84 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/KM_WGS84_LION_LineString_Bus_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv\"\n    val pathKM_RandomPointsNonUniform = \"\"\n\n    val pathGM_TPEP = \"/media/cusp/Data/GeoMatch_Files/OutputFiles/698/\"\n    val pathSparkKNN_FW_Output_1 = pathGM_TPEP //\"/media/cusp/Data/GeoMatch_Files/OutputFiles/sKNN_k_10.txt\"\n//    val pathSparkKNN_FW_Output_2 = \"/media/cusp/Data/GeoMatch_Files/OutputFiles/148/\"\n    //    val pathSparkKNN_FW_Output_2 = \"/media/cusp/Data/GeoMatch_Files/OutputFiles/Simba_Taxi_K10\"\n        val pathSparkKNN_FW_Output_2 = \"/media/cusp/Data/GeoMatch_Files/OutputFiles/Simba_Taxi_K10\"\n\n    val pathGS_TPEP = \"/media/cusp/Data/GeoMatch_Files/InputFiles/GeoSpark_LION_TPEP.csv\"\n    val pathLS_wgs_TPEP = \"/media/cusp/Data/GeoMatch_Files/InputFiles/LocationSpark_LION_TPEP.csv\"\n    val pathLS_wgs_Taxi = \"/media/cusp/Data/GeoMatch_Files/InputFiles/LocationSpark_LION_Taxi.csv\"\n    val pathLS_wgs_Bus = \"/media/cusp/Data/GeoMatch_Files/InputFiles/LocationSpark_LION_Bus.csv\"\n    val pathGM_Taxi = \"/media/cusp/Data/GeoMatch_Files/InputFiles/GeoMatch_LION_Taxi.csv\"\n    val pathGS_Taxi = \"/media/cusp/Data/GeoMatch_Files/InputFiles/GeoSpark_LION_Taxi.csv\"\n\n    val pathTaxi1M_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Yellow_TLC_TripRecord_NAD83_1M.csv\"\n\n    val pathRandSample_A_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/RandomSamples/Yellow_2_A.csv\"\n    val pathRandSample_B_NAD83 = \"/media/cusp/Data/GeoMatch_Files/InputFiles/RandomSamples/Yellow_2_B.csv\"\n\n    val pathTaxi1M_No_Trip_NAD83_A = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Yellow_TLC_TripRecord_NAD83_1M_No_Trip_Info_A.csv\"\n    val pathTaxi1M_No_Trip_NAD83_B = \"/media/cusp/Data/GeoMatch_Files/InputFiles/Yellow_TLC_TripRecord_NAD83_1M_No_Trip_Info_B.csv\"\n}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- LocalRunFiles/src/main/scala/org/cusp/bdi/util/LocalRunConsts.scala	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ LocalRunFiles/src/main/scala/org/cusp/bdi/util/LocalRunConsts.scala	(date 1595047046545)
@@ -41,11 +41,11 @@
     val pathKM_Bus_WGS84 = "/media/cusp/Data/GeoMatch_Files/InputFiles/KM_WGS84_LION_LineString_Bus_Point_ErrCorr_MBRExp_150_MaxDist_150.1K.csv"
     val pathKM_RandomPointsNonUniform = ""
 
-    val pathGM_TPEP = "/media/cusp/Data/GeoMatch_Files/OutputFiles/698/"
+    val pathGM_TPEP = "/media/cusp/Data/GeoMatch_Files/OutputFiles/615/"
     val pathSparkKNN_FW_Output_1 = pathGM_TPEP //"/media/cusp/Data/GeoMatch_Files/OutputFiles/sKNN_k_10.txt"
 //    val pathSparkKNN_FW_Output_2 = "/media/cusp/Data/GeoMatch_Files/OutputFiles/148/"
     //    val pathSparkKNN_FW_Output_2 = "/media/cusp/Data/GeoMatch_Files/OutputFiles/Simba_Taxi_K10"
-        val pathSparkKNN_FW_Output_2 = "/media/cusp/Data/GeoMatch_Files/OutputFiles/Simba_Taxi_K10"
+        val pathSparkKNN_FW_Output_2 = "/media/cusp/Data/GeoMatch_Files/OutputFiles/207/"
 
     val pathGS_TPEP = "/media/cusp/Data/GeoMatch_Files/InputFiles/GeoSpark_LION_TPEP.csv"
     val pathLS_wgs_TPEP = "/media/cusp/Data/GeoMatch_Files/InputFiles/LocationSpark_LION_TPEP.csv"
Index: .idea/scala_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/scala_settings.xml	(date 1594758901688)
+++ .idea/scala_settings.xml	(date 1594758901688)
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ScalaProjectSettings">
+    <option name="intInjectionMapping">
+      <map>
+        <entry key="xml" value="XML" />
+      </map>
+    </option>
+  </component>
+</project>
\ No newline at end of file
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ProjectModuleManager\">\n    <modules>\n      <module fileurl=\"file://$PROJECT_DIR$/Common/Common.iml\" filepath=\"$PROJECT_DIR$/Common/Common.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml\" filepath=\"$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/FW_Simba/FW_Simba.iml\" filepath=\"$PROJECT_DIR$/FW_Simba/FW_Simba.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/FW_Simba_Source/FW_Simba_Source.iml\" filepath=\"$PROJECT_DIR$/FW_Simba_Source/FW_Simba_Source.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/GeoMatch/GeoMatch.iml\" filepath=\"$PROJECT_DIR$/GeoMatch/GeoMatch.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/LocalRunFiles/LocalRunFiles.iml\" filepath=\"$PROJECT_DIR$/LocalRunFiles/LocalRunFiles.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/.idea/Research.iml\" filepath=\"$PROJECT_DIR$/.idea/Research.iml\" />\n      <module fileurl=\"file://$PROJECT_DIR$/Spark_kNN/Spark_kNN.iml\" filepath=\"$PROJECT_DIR$/Spark_kNN/Spark_kNN.iml\" />\n    </modules>\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/modules.xml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ .idea/modules.xml	(date 1594757005328)
@@ -3,13 +3,13 @@
   <component name="ProjectModuleManager">
     <modules>
       <module fileurl="file://$PROJECT_DIR$/Common/Common.iml" filepath="$PROJECT_DIR$/Common/Common.iml" />
-      <module fileurl="file://$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml" filepath="$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml" />
       <module fileurl="file://$PROJECT_DIR$/FW_Simba/FW_Simba.iml" filepath="$PROJECT_DIR$/FW_Simba/FW_Simba.iml" />
       <module fileurl="file://$PROJECT_DIR$/FW_Simba_Source/FW_Simba_Source.iml" filepath="$PROJECT_DIR$/FW_Simba_Source/FW_Simba_Source.iml" />
       <module fileurl="file://$PROJECT_DIR$/GeoMatch/GeoMatch.iml" filepath="$PROJECT_DIR$/GeoMatch/GeoMatch.iml" />
       <module fileurl="file://$PROJECT_DIR$/LocalRunFiles/LocalRunFiles.iml" filepath="$PROJECT_DIR$/LocalRunFiles/LocalRunFiles.iml" />
       <module fileurl="file://$PROJECT_DIR$/.idea/Research.iml" filepath="$PROJECT_DIR$/.idea/Research.iml" />
       <module fileurl="file://$PROJECT_DIR$/Spark_kNN/Spark_kNN.iml" filepath="$PROJECT_DIR$/Spark_kNN/Spark_kNN.iml" />
+      <module fileurl="file://$PROJECT_DIR$/SpatialBenchmark/SpatialBenchmark.iml" filepath="$PROJECT_DIR$/SpatialBenchmark/SpatialBenchmark.iml" />
     </modules>
   </component>
 </project>
\ No newline at end of file
Index: SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/MatchDistAnalysis.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/MatchDistAnalysis.scala	(date 1582523951644)
+++ SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/MatchDistAnalysis.scala	(date 1582523951644)
@@ -0,0 +1,284 @@
+package org.cusp.bdi.sb
+
+import scala.collection.mutable.HashMap
+import scala.collection.mutable.ListBuffer
+import scala.collection.mutable.Map
+import scala.collection.mutable.SortedSet
+
+import org.apache.commons.logging.LogFactory
+import org.apache.spark.SparkConf
+import org.apache.spark.SparkContext
+import org.apache.spark.rdd.RDD
+import org.apache.spark.serializer.KryoSerializer
+import org.cusp.bdi.sb.examples.BenchmarkInputFileParser
+import org.cusp.bdi.sb.examples.SB_Arguments
+import org.cusp.bdi.sb.examples.SB_CLArgs
+import org.cusp.bdi.util.Helper
+import org.apache.hadoop.io.compress.GzipCodec
+
+object MatchDistAnalysis extends Serializable {
+
+    object Classifications {
+
+        //        var missMatchFlag = false
+
+        val percent = "(%)"
+        val recordsBothNoMatch = "Records both no match"
+        val recordsCount = "Total Number of Records"
+        val recordsSimilarMatch = "Records similarily matched"
+        val recordsInFirstOnly = "Records matched in the 1st file only"
+        val recordsInSecondOnly = "Records matched in the 2nd file only"
+        val recordsFirstBetter = "Records in 1st file with closer matches"
+        val recordsSecondBetter = "Records in 2nd file with closer matches"
+        val recordsFirstUnderMatched = "Records undermatched in 1st file"
+        val recordsSecondUnderMatched = "Records undermatched in 2nd file"
+        val recordsFirstOverMatched = "Records over matched in 1st file"
+        val recordsSecondOverMatched = "Records over matched in 2nd file"
+
+        def getMapClassifications = HashMap(Classifications.recordsBothNoMatch -> 0L,
+            Classifications.recordsCount -> 0L,
+            Classifications.recordsSimilarMatch -> 0L,
+            Classifications.recordsInFirstOnly -> 0L,
+            Classifications.recordsInSecondOnly -> 0L,
+            Classifications.recordsFirstBetter -> 0L,
+            Classifications.recordsSecondBetter -> 0L,
+            Classifications.recordsFirstUnderMatched -> 0L,
+            Classifications.recordsSecondUnderMatched -> 0L,
+            Classifications.recordsFirstOverMatched -> 0L,
+            Classifications.recordsSecondOverMatched -> 0L)
+
+        def incrementInMap(row: String, arr1: Array[(Double, String)], arr2: Array[(Double, String)], mapClassifications: HashMap[String, Long], key: String) {
+
+            //            if (key == recordsCount)
+            //                missMatchFlag = false
+            //            else if (key != recordsSimilarMatch)
+            //                missMatchFlag = true
+
+            mapClassifications.update(key, mapClassifications.get(key).get + 1)
+            if (row.length() > 0)
+                println(">>%s%n\t%s\n\t%s".format(row, arr1.mkString(","), arr2.mkString(",")))
+        }
+    }
+
+    private val LOGGER = LogFactory.getLog(this.getClass())
+
+    def main(args: Array[String]): Unit = {
+
+        val startTime = System.currentTimeMillis()
+
+        //        val clArgs = SB_CLArgs.GM_LionTPEP
+        //        val clArgs = SB_CLArgs.GM_LionTaxi
+        //        val clArgs = SB_CLArgs.GS_LionTPEP
+        //        val clArgs = SB_CLArgs.GS_LionTaxi
+        //        val clArgs = SB_CLArgs.LS_LionTaxi
+        //        val clArgs = SB_CLArgs.LS_LionBus
+        //        val clArgs = SB_CLArgs.LS_LionTPEP
+        //        val clArgs = SB_CLArgs.SKNN_BusPoint_BusPointShift
+        val clArgs = SB_CLArgs.SKNN_RandomPoint_RandomPoint
+        //        val clArgs = CLArgsParser(args, SB_Arguments())
+
+        val classificationCount = clArgs.getParamValueInt(SB_Arguments.classificationCount)
+        val testFWFileParser_1 = instansiateClass[BenchmarkInputFileParser](clArgs.getParamValueString(SB_Arguments.keyMatchInFileParser))
+        val testFWFileParser_2 = instansiateClass[BenchmarkInputFileParser](clArgs.getParamValueString(SB_Arguments.testFWInFileParser))
+
+        val sparkConf = new SparkConf().setAppName("Spatial Benchmark")
+
+        if (clArgs.getParamValueBoolean(SB_Arguments.local))
+            sparkConf.setMaster("local[*]")
+
+        sparkConf.set("spark.serializer", classOf[KryoSerializer].getName);
+        sparkConf.registerKryoClasses(Array(classOf[String]))
+
+        val sparkContext = new SparkContext(sparkConf)
+
+        // delete output dir if exists
+        Helper.delDirHDFS(sparkContext, clArgs.getParamValueString(SB_Arguments.outDir))
+
+        val rdd1 = sparkContext.textFile(clArgs.getParamValueString(SB_Arguments.keyMatchInFile))
+        val rdd2 = sparkContext.textFile(clArgs.getParamValueString(SB_Arguments.testFWInFile))
+
+        var rddUnique1: RDD[(String, (Array[(Double, String)], Array[(Double, String)]))] =
+            filterDuplicateRows(classificationCount, rdd1, testFWFileParser_1.parseLine)
+                .mapPartitions(_.map(row => (row._1, (row._2, null))))
+
+        var rddUnique2: RDD[(String, (Array[(Double, String)], Array[(Double, String)]))] =
+            filterDuplicateRows(classificationCount, rdd2, testFWFileParser_2.parseLine)
+                .mapPartitions(_.map(row => (row._1, (null, row._2))))
+
+        val arrResults = rddUnique1.union(rddUnique2)
+            .reduceByKey((x, y) => {
+
+                var arrDS1 = x._1
+                var arrDS2 = x._2
+
+                if ((x._1 == null && x._2 == null) || y._1 == null && y._2 == null)
+                    throw new Exception("both arrays cannot be null") // shouldn't happen
+
+                if (arrDS1 == null)
+                    arrDS1 = y._1
+
+                if (arrDS2 == null)
+                    arrDS2 = y._2
+
+                (arrDS1, arrDS2)
+            })
+            .mapPartitions(iter => {
+
+                val mapClassifications = Classifications.getMapClassifications
+
+                while (iter.hasNext) {
+
+                    val row = iter.next
+                    val (arr1, arr2) = (if (row._2._1 == null) Array[(Double, String)]() else row._2._1, if (row._2._2 == null) Array[(Double, String)]() else row._2._2)
+
+                    Classifications.incrementInMap("", arr1, arr2, mapClassifications, Classifications.recordsCount)
+
+                    //                    if (arr1 != null && arr2 == null)
+                    //                        Classifications.incrementInMap(mapClassifications, Classifications.recordsInFirstOnly)
+                    //                    else if (arr1 == null && arr2 != null)
+                    //                        Classifications.incrementInMap(mapClassifications, Classifications.recordsInSecondOnly)
+                    //                    else {
+
+                    if (arr1.length == 0 && arr2.length == 0)
+                        Classifications.incrementInMap("", arr1, arr2, mapClassifications, Classifications.recordsBothNoMatch)
+                    else if (arr1.length > 0 && arr2.length == 0)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsInFirstOnly)
+                    else if (arr1.length == 0 && arr2.length > 0)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsInSecondOnly)
+                    else if (arr1.length < arr2.length)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsFirstUnderMatched)
+                    else if (arr1.length > arr2.length)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsSecondUnderMatched)
+
+                    if (arr1.length > classificationCount && arr1.length > arr2.length)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsFirstOverMatched)
+                    if (arr2.length > classificationCount && arr2.length > arr1.length)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsSecondOverMatched)
+
+                    val agreeCount = (0 until math.min(arr1.length, arr2.length)).map(i => {
+
+                        // (countBoth, countInFirst, countInSecond)
+                        if (arr1(i)._1 == arr2(i)._1)
+                            (1, 0, 0)
+                        else if (arr1(i)._1 < arr2(i)._1)
+                            (0, 1, 0)
+                        else
+                            (0, 0, 1)
+                    })
+                        .fold((0, 0, 0))((x, y) => (x._1 + y._1, x._2 + y._2, x._3 + y._3))
+
+                    if (agreeCount._1 == classificationCount)
+                        Classifications.incrementInMap("", arr1, arr2, mapClassifications, Classifications.recordsSimilarMatch)
+                    if (agreeCount._2 > 0)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsFirstBetter)
+                    if (agreeCount._3 > 0)
+                        Classifications.incrementInMap(row._1, arr1, arr2, mapClassifications, Classifications.recordsSecondBetter)
+
+                    //                    if (Classifications.missMatchFlag)
+                    //                        println(">>%s%n\t%s\n\t%s".format(row._1, arr1.mkString(","), arr2.mkString(",")))
+                }
+                //                }
+
+                mapClassifications.iterator
+            })
+            .reduceByKey(_ + _)
+            .collect()
+
+        // add percentages
+        val mapResults = HashMap[String, Long]() ++ arrResults
+
+        val recordsCount = mapResults.get(Classifications.recordsCount).get.toString().toDouble
+        val recordsBothNoMatch = mapResults.get(Classifications.recordsBothNoMatch).get.toString().toDouble
+        val recordsSimilarMatch = mapResults.get(Classifications.recordsSimilarMatch).get.toString().toDouble
+        val recordsInFirstOnly = mapResults.get(Classifications.recordsInFirstOnly).get.toString().toDouble
+        val recordsInSecondOnly = mapResults.get(Classifications.recordsInSecondOnly).get.toString().toDouble
+        val recordsFirstBetter = mapResults.get(Classifications.recordsFirstBetter).get.toString().toDouble
+        val recordsSecondBetter = mapResults.get(Classifications.recordsSecondBetter).get.toString().toDouble
+        val recordsFirstUnderMatched = mapResults.get(Classifications.recordsFirstUnderMatched).get.toString().toDouble
+        val recordsSecondUnderMatched = mapResults.get(Classifications.recordsSecondUnderMatched).get.toString().toDouble
+        val recordsFirstOverMatched = mapResults.get(Classifications.recordsFirstOverMatched).get.toString().toDouble
+        val recordsSecondOverMatched = mapResults.get(Classifications.recordsSecondOverMatched).get.toString().toDouble
+
+        // list to display in a specific order
+        val lstResults = ListBuffer(getFormatted(mapResults, Classifications.recordsCount, recordsCount),
+                                    getFormatted(mapResults, Classifications.recordsBothNoMatch, recordsBothNoMatch),
+                                    getFormatted(mapResults, Classifications.percent, (recordsBothNoMatch / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsSimilarMatch, recordsSimilarMatch),
+                                    getFormatted(mapResults, Classifications.percent, (recordsSimilarMatch / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsInFirstOnly, recordsInFirstOnly),
+                                    getFormatted(mapResults, Classifications.percent, (recordsInFirstOnly / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsInSecondOnly, recordsInSecondOnly),
+                                    getFormatted(mapResults, Classifications.percent, (recordsInSecondOnly / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsFirstBetter, recordsFirstBetter),
+                                    getFormatted(mapResults, Classifications.percent, (recordsFirstBetter / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsSecondBetter, recordsSecondBetter),
+                                    getFormatted(mapResults, Classifications.percent, (recordsSecondBetter / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsFirstUnderMatched, recordsFirstUnderMatched),
+                                    getFormatted(mapResults, Classifications.percent, (recordsFirstUnderMatched / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsSecondUnderMatched, recordsSecondUnderMatched),
+                                    getFormatted(mapResults, Classifications.percent, (recordsSecondUnderMatched / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsFirstOverMatched, recordsFirstOverMatched),
+                                    getFormatted(mapResults, Classifications.percent, (recordsFirstOverMatched / recordsCount * 100)),
+
+                                    getFormatted(mapResults, Classifications.recordsSecondOverMatched, recordsSecondOverMatched),
+                                    getFormatted(mapResults, Classifications.percent, (recordsSecondOverMatched / recordsCount * 100)))
+
+        sparkContext.parallelize(lstResults, 1)
+            .saveAsTextFile(clArgs.getParamValueString(SB_Arguments.outDir), classOf[GzipCodec])
+
+        if (clArgs.getParamValueBoolean(SB_Arguments.local)) {
+
+            println("Output idr: " + clArgs.getParamValueString(SB_Arguments.outDir))
+            lstResults.foreach(println)
+        }
+    }
+
+    private def getFormatted(mapResults: Map[String, Long], key: String, default: Any) = {
+
+        val opt = mapResults.get(key)
+        val value = if (opt == None) default else opt.get
+
+        // Long data type assumed a count
+        // Double data type assumed a percentage
+        value match {
+            case _: Long => "%41s".format(key) + ": " + "%,d".format(value)
+            case _: Double => "%41s".format(key) + ": " + "%.4f%%".format(value)
+            case _ => "%41s".format(key) + ": " + value
+        }
+    }
+
+    private def filterDuplicateRows(classificationCount: Int, rdd: RDD[String], fileParser: (String => (String, Array[String]))) =
+        rdd.mapPartitions(_.map(fileParser).filter(_ != null))
+            .mapPartitions(_.map(row => {
+
+                val arr = row._2.map(row => {
+                    val arr = row.split(',')
+                    (arr(0).toDouble, arr(1))
+                })
+
+                (row._1.toLowerCase(), arr)
+            }))
+            .reduceByKey((arr1, arr2) => extractOneArray(classificationCount, arr1, arr2))
+            .mapPartitions(_.map(row => (row._1, row._2.asInstanceOf[Array[(Double, String)]])))
+
+    private def instansiateClass[T](className: String) = {
+
+        var loadClass = className
+
+        if (className.endsWith("$"))
+            loadClass = className.substring(0, className.length() - 1)
+
+        Class.forName(loadClass).getConstructor().newInstance().asInstanceOf[T]
+    }
+
+    private def extractOneArray(classificationCount: Int, arr1: Array[(Double, String)], arr2: Array[(Double, String)]) =
+        (arr1.to[SortedSet] ++ arr2.to[SortedSet]).toArray.take(classificationCount)
+}
\ No newline at end of file
Index: SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/OutputsComapre.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/OutputsComapre.scala	(date 1594761398061)
+++ SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/OutputsComapre.scala	(date 1594761398061)
@@ -0,0 +1,253 @@
+
+package org.cusp.bdi.sb
+
+import scala.collection.mutable.HashMap
+import scala.collection.mutable.ListBuffer
+
+import org.apache.spark.Partitioner
+import org.apache.spark.rdd.RDD
+import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
+import org.cusp.bdi.util.Helper
+import org.cusp.bdi.sb.examples.BenchmarkInputFileParser
+import org.apache.commons.lang3.builder.HashCodeBuilder
+
+object OutputsComapre extends Serializable {
+
+  object Classifications {
+
+    val percent = "(%)"
+    val recordsBothNoMatch = "Records both no match"
+    val recordsCount = "Total Number of Records"
+    val recordsFWCorrectMatched = "Records correctly matched"
+    val recordsFWFailedToMatch = "Records framework failed to match"
+    val recordsFWMismatch = "Records framework incorrectly matched"
+    val recordsFWOnlyMatched = "Records framework only matched"
+    val recordsFWOverMatched = "Records framework overmatched"
+    val recordsFWUnderMatched = "Records framework undermatched"
+    val recordsInFWOnly = "Records appeared in framework only"
+    val recordsInKMOnly = "Records appeared in key match only"
+  }
+
+  private def rddUnique(rdd: RDD[String], fileParser: (String => (String, Array[String]))) =
+    rdd.mapPartitions(_.map(fileParser).filter(_ != null))
+      .mapPartitions(_.map(x => {
+
+        val arr = if (x._2 == null) Array[String]() else x._2.filter(x => !Helper.isNullOrEmpty(x))
+
+        (x._1.toLowerCase(), arr)
+      }))
+      .reduceByKey((x, y) => extractOneArray(x, y))
+
+  def apply(classificationCount: Int, rddKeyMatch: RDD[String], keyMatchFileParser: BenchmarkInputFileParser, rddTestFW: RDD[String], testFWFileParser: BenchmarkInputFileParser) = {
+
+    var rddKeyMatchUnique = rddUnique(rddKeyMatch, keyMatchFileParser.parseLine)
+      .mapPartitions(_.map(x => {
+
+        val arr: Array[String] = null
+
+        (x._1, (x._2, arr))
+      }))
+
+    var rddTestFWUnique = rddUnique(rddTestFW, testFWFileParser.parseLine)
+      .mapPartitions(_.map(x => {
+
+        val arr: Array[String] = null
+
+        (x._1, (arr, x._2))
+      }))
+
+    val arrResults = rddKeyMatchUnique.union(rddTestFWUnique)
+      .reduceByKey((x, y) => {
+
+        var arr1 = x._1
+        var arr2 = x._2
+
+        if (x._1 == null)
+          arr1 = y._1
+        else if (y._1 != null)
+          arr1 = x._1 ++ y._1
+
+        if (x._2 == null)
+          arr2 = y._2
+        else if (y._2 != null)
+          arr1 = x._2 ++ y._2
+
+        (arr1, arr2)
+      })
+      .mapPartitions(iter => {
+
+        val mapRowLevelClassify = HashMap(Classifications.recordsBothNoMatch -> 0L,
+          Classifications.recordsCount -> 0L,
+          Classifications.recordsFWCorrectMatched -> 0L,
+          Classifications.recordsFWFailedToMatch -> 0L,
+          Classifications.recordsFWMismatch -> 0L,
+          Classifications.recordsFWOnlyMatched -> 0L,
+          Classifications.recordsFWOverMatched -> 0L,
+          Classifications.recordsFWUnderMatched -> 0L,
+          Classifications.recordsInFWOnly -> 0L,
+          Classifications.recordsInKMOnly -> 0L)
+
+        def incrementInMap(classificationKey: String, row: (String, (Array[String], Array[String]))) = {
+
+          if (!(classificationKey.equals(Classifications.recordsCount) || classificationKey.equals(Classifications.recordsBothNoMatch) || classificationKey.equals(Classifications.recordsFWCorrectMatched)))
+            println(">>\t%s: %s\n\t\t\t%s\n\t\t\t%s".format(classificationKey, row._1, row._2._1.mkString(","), row._2._2.mkString(",")))
+
+          mapRowLevelClassify.update(classificationKey, mapRowLevelClassify.get(classificationKey).get + 1)
+        }
+
+        iter.foreach(row => {
+
+          if (row._2._1 != null)
+            incrementInMap(Classifications.recordsCount, row)
+
+          var done = false
+
+          if (row._2._1 == null && row._2._2 != null) {
+
+            incrementInMap(Classifications.recordsInFWOnly, row)
+            done = true
+          }
+          else if (row._2._1 != null && row._2._2 == null) {
+
+            incrementInMap(Classifications.recordsInKMOnly, row)
+            done = true
+          }
+
+          if (!done) {
+
+            val arrKMSize = row._2._1.size
+            val arrFWSize = row._2._2.size
+            val arrMatchIdxs = row._2._2.map(x => row._2._1.indexOf(x))
+
+            if (arrFWSize == 0 && arrKMSize == 0) {
+              incrementInMap(Classifications.recordsBothNoMatch, row)
+              incrementInMap(Classifications.recordsFWCorrectMatched, row)
+            }
+            else if (arrFWSize > 0 && arrKMSize == 0)
+              incrementInMap(Classifications.recordsFWOnlyMatched, row)
+            else if (arrFWSize == 0 && arrKMSize > 0)
+              incrementInMap(Classifications.recordsFWFailedToMatch, row)
+            else if (arrFWSize > arrKMSize)
+              incrementInMap(Classifications.recordsFWOverMatched, row)
+            else if (arrFWSize < classificationCount && arrFWSize < arrKMSize)
+              incrementInMap(Classifications.recordsFWUnderMatched, row)
+
+            var correctStreetCount = 0
+
+            correctStreetCount = arrMatchIdxs.take(classificationCount)
+              .seq
+              .groupBy(identity)
+              .mapValues(_.size)
+              .toArray
+              .sortBy(_._1)
+              .map(tuple => {
+
+                if ((0 until classificationCount).contains(tuple._1))
+                  1
+                else
+                  0
+              }).sum
+
+            // add # "Records both no match" to # of "Records correctly matched" since they are correctly classified
+            if (correctStreetCount == 0)
+              incrementInMap(Classifications.recordsFWMismatch, row)
+            else if (correctStreetCount > 0 && correctStreetCount <= classificationCount)
+              incrementInMap(Classifications.recordsFWCorrectMatched, row)
+          }
+        })
+
+        mapRowLevelClassify.iterator
+      })
+      .reduceByKey(_ + _)
+      .collect()
+
+    // add percentages
+    val mapResults = HashMap[String, Long]()
+
+    arrResults.foreach(x => mapResults += x._1 -> x._2)
+
+    val recordsCount = mapResults.get(Classifications.recordsCount).get.toString().toDouble
+    val recordsBothNoMatch = mapResults.get(Classifications.recordsBothNoMatch).get.toString().toDouble
+    val recordsFWOnlyMatched = mapResults.get(Classifications.recordsFWOnlyMatched).get.toString().toDouble
+    val recordsFWCorrectMatched = mapResults.get(Classifications.recordsFWCorrectMatched).get.toString().toDouble
+    val recordsFWFailedToMatch = mapResults.get(Classifications.recordsFWFailedToMatch).get.toString().toDouble
+    val recordsFWOverMatched = mapResults.get(Classifications.recordsFWOverMatched).get.toString().toDouble
+    val recordsFWUnderMatched = mapResults.get(Classifications.recordsFWUnderMatched).get.toString().toDouble
+    val recordsInFWOnly = mapResults.get(Classifications.recordsInFWOnly).get.toString().toDouble
+    val recordsInKMOnly = mapResults.get(Classifications.recordsInKMOnly).get.toString().toDouble
+    val recordsFWMismatch = mapResults.get(Classifications.recordsFWMismatch).get.toString().toDouble
+
+    // list to display in a specific order
+    ListBuffer(getFormatted(mapResults, Classifications.recordsCount, recordsCount),
+      getFormatted(mapResults, Classifications.recordsFWCorrectMatched, recordsFWCorrectMatched),
+      getFormatted(mapResults, Classifications.percent, (recordsFWCorrectMatched / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsBothNoMatch, recordsBothNoMatch),
+      getFormatted(mapResults, Classifications.percent, (recordsBothNoMatch / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsFWOnlyMatched, recordsFWOnlyMatched),
+      getFormatted(mapResults, Classifications.percent, (recordsFWOnlyMatched / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsFWFailedToMatch, recordsFWFailedToMatch),
+      getFormatted(mapResults, Classifications.percent, (recordsFWFailedToMatch / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsFWOverMatched, recordsFWOverMatched),
+      getFormatted(mapResults, Classifications.percent, (recordsFWOverMatched / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsFWUnderMatched, recordsFWUnderMatched),
+      getFormatted(mapResults, Classifications.percent, (recordsFWUnderMatched / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsInFWOnly, recordsInFWOnly),
+      getFormatted(mapResults, Classifications.percent, (recordsInFWOnly / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsInKMOnly, recordsInKMOnly),
+      getFormatted(mapResults, Classifications.percent, (recordsInKMOnly / recordsCount * 100)),
+      getFormatted(mapResults, Classifications.recordsFWMismatch, recordsFWMismatch),
+      getFormatted(mapResults, Classifications.percent, (recordsFWMismatch / recordsCount * 100)))
+  }
+
+  private def getFormatted(mapResults: HashMap[String, Long], key: String, default: Any) = {
+
+    val opt = mapResults.get(key)
+    val value = if (opt == None) default else opt.get
+
+    // Long data type assumed a count
+    // Double data type assumed a percentage
+    value match {
+      case _: Long => "%41s".format(key) + ": " + "%,d".format(value)
+      case _: Double => "%41s".format(key) + ": " + "%.4f%%".format(value)
+      case _ => "%41s".format(key) + ": " + value
+    }
+  }
+
+  private def extractOneArray(arr0: Array[String], arr1: Array[String]) = {
+
+    val arrFilter0 = arr0.filter(x => !Helper.isNullOrEmpty(x))
+    val arrFilter1 = arr1.filter(x => !Helper.isNullOrEmpty(x))
+
+    if (Helper.isNullOrEmpty(arrFilter0))
+      arrFilter1
+    else if (Helper.isNullOrEmpty(arrFilter1))
+      arrFilter0
+    else {
+
+      val lst = arrFilter0.to[ListBuffer]
+      var idx = 0
+
+      arrFilter1.foreach(x => lst.append(x))
+
+      //            while (lst.size < classificationCount && idx < arrFilter1.size) {
+      //
+      //                val str = arrFilter1(idx)
+      //
+      //                if (!lst.contains(str))
+      //                    lst.append(str)
+      //
+      //                idx += 1
+      //            }
+
+      lst.distinct.toArray
+    }
+  }
+}
+
+//class KeyPartitioner(_numPartitions: Int) extends Partitioner {
+//
+//    def numPartitions = _numPartitions
+//
+//    def getPartition(key: Any): Int =
+//        math.abs(key.toString().toLowerCase().hashCode()) % numPartitions
+//}
\ No newline at end of file
Index: SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/SpatialBenchmark.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/SpatialBenchmark.scala	(date 1594758463531)
+++ SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/SpatialBenchmark.scala	(date 1594758463531)
@@ -0,0 +1,76 @@
+package org.cusp.bdi.sb
+
+import org.apache.commons.logging.LogFactory
+import org.apache.hadoop.io.compress.GzipCodec
+import org.apache.spark.SparkConf
+import org.apache.spark.SparkContext
+import org.apache.spark.serializer.KryoSerializer
+import org.cusp.bdi.sb.examples.BenchmarkInputFileParser
+import org.cusp.bdi.util.CLArgsParser
+import org.cusp.bdi.util.Helper
+import org.cusp.bdi.sb.examples.SB_Arguments
+import org.cusp.bdi.sb.examples.SB_CLArgs
+
+object SpatialBenchmark extends Serializable {
+
+    private val LOGGER = LogFactory.getLog(this.getClass())
+
+    def main(args: Array[String]): Unit = {
+
+        val startTime = System.currentTimeMillis()
+
+        //        val clArgs = SB_CLArgs.GM_LionTPEP
+        //        val clArgs = SB_CLArgs.GM_LionTaxi
+        //        val clArgs = SB_CLArgs.GS_LionTPEP
+        //        val clArgs = SB_CLArgs.GS_LionTaxi
+        //        val clArgs = SB_CLArgs.LS_LionTaxi
+        //        val clArgs = SB_CLArgs.LS_LionBus
+        //        val clArgs = SB_CLArgs.LS_LionTPEP
+        //        val clArgs = SB_CLArgs.SKNN_BusPoint_BusPointShift
+        val clArgs = SB_CLArgs.SKNN_RandomPoint_RandomPoint
+        //        val clArgs = CLArgsParser(args, SB_Arguments())
+
+        val keyMatchInFileParser = instansiateClass[BenchmarkInputFileParser](clArgs.getParamValueString(SB_Arguments.keyMatchInFileParser))
+        val testFWInFileParser = instansiateClass[BenchmarkInputFileParser](clArgs.getParamValueString(SB_Arguments.testFWInFileParser))
+
+        val sparkConf = new SparkConf().setAppName("Spatial Benchmark")
+
+        if (clArgs.getParamValueBoolean(SB_Arguments.local))
+            sparkConf.setMaster("local[*]")
+
+        sparkConf.set("spark.serializer", classOf[KryoSerializer].getName);
+        sparkConf.registerKryoClasses(Array(classOf[String]))
+
+        val sparkContext = new SparkContext(sparkConf)
+
+        // delete output dir if exists
+        Helper.delDirHDFS(sparkContext, clArgs.getParamValueString(SB_Arguments.outDir))
+
+        val rddKeyMatch = sparkContext.textFile(clArgs.getParamValueString(SB_Arguments.keyMatchInFile))
+
+        val rddTestFW = sparkContext.textFile(clArgs.getParamValueString(SB_Arguments.testFWInFile))
+
+        val compareResults = OutputsComapre(clArgs.getParamValueInt(SB_Arguments.classificationCount), rddKeyMatch, keyMatchInFileParser, rddTestFW, testFWInFileParser)
+
+        compareResults.append("Total Runtime: " + "%,d".format(System.currentTimeMillis() - startTime) + " ms")
+
+        sparkContext.parallelize(compareResults, 1)
+            .saveAsTextFile(clArgs.getParamValueString(SB_Arguments.outDir), classOf[GzipCodec])
+
+        if (clArgs.getParamValueBoolean(SB_Arguments.local)) {
+
+            println("Output idr: " + clArgs.getParamValueString(SB_Arguments.outDir))
+            compareResults.foreach(println)
+        }
+    }
+
+    def instansiateClass[T](className: String) = {
+
+        var loadClass = className
+
+        if (className.endsWith("$"))
+            loadClass = className.substring(0, className.length() - 1)
+
+        Class.forName(loadClass).getConstructor().newInstance().asInstanceOf[T]
+    }
+}
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"MavenProjectsManager\">\n    <option name=\"originalFiles\">\n      <list>\n        <option value=\"$PROJECT_DIR$/FW_Simba_Source/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/FW_Simba/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/Common/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/LocalRunFiles/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/Spark_kNN/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/GeoMatch/pom.xml\" />\n        <option value=\"$PROJECT_DIR$/FW_GeoMatch/pom.xml\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"ProjectRootManager\" version=\"2\" languageLevel=\"JDK_1_8\" project-jdk-name=\"1.8\" project-jdk-type=\"JavaSDK\">\n    <output url=\"file://$PROJECT_DIR$/out\" />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ .idea/misc.xml	(date 1594757022645)
@@ -9,11 +9,11 @@
         <option value="$PROJECT_DIR$/LocalRunFiles/pom.xml" />
         <option value="$PROJECT_DIR$/Spark_kNN/pom.xml" />
         <option value="$PROJECT_DIR$/GeoMatch/pom.xml" />
-        <option value="$PROJECT_DIR$/FW_GeoMatch/pom.xml" />
+        <option value="$PROJECT_DIR$/SpatialBenchmark/pom.xml" />
       </list>
     </option>
   </component>
-  <component name="ProjectRootManager" version="2" languageLevel="JDK_1_8" project-jdk-name="1.8" project-jdk-type="JavaSDK">
+  <component name="ProjectRootManager" version="2" languageLevel="JDK_1_8" default="false" project-jdk-name="1.8" project-jdk-type="JavaSDK">
     <output url="file://$PROJECT_DIR$/out" />
   </component>
 </project>
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"ee761a9d-eff2-4086-b83a-8096819a4548\" name=\"Default Changelist\" comment=\"\" />\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"CodeStyleSettingsInfer\">\n    <option name=\"done\" value=\"true\" />\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"ProjectCodeStyleSettingsMigration\">\n    <option name=\"version\" value=\"1\" />\n  </component>\n  <component name=\"ProjectId\" id=\"1efAr5YcOxluhMg5E3rnT7ELqH3\" />\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"autoscrollFromSource\" value=\"true\" />\n    <option name=\"flattenModules\" value=\"true\" />\n    <option name=\"flattenPackages\" value=\"true\" />\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n    <option name=\"showVisibilityIcons\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">\n    <property name=\"RunOnceActivity.OpenProjectViewOnStart\" value=\"true\" />\n    <property name=\"RunOnceActivity.ShowReadmeOnStart\" value=\"true\" />\n    <property name=\"last_opened_file_path\" value=\"$PROJECT_DIR$/FW_GeoMatch/src\" />\n    <property name=\"project.structure.last.edited\" value=\"Modules\" />\n    <property name=\"project.structure.proportion\" value=\"0.15\" />\n    <property name=\"project.structure.side.proportion\" value=\"0.34280476\" />\n    <property name=\"settings.editor.selected.configurable\" value=\"project.propVCSSupport.Mappings\" />\n  </component>\n  <component name=\"RecentsManager\">\n    <key name=\"CopyFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/FW_GeoMatch/src\" />\n      <recent name=\"$PROJECT_DIR$/GeoMatch/src\" />\n      <recent name=\"$PROJECT_DIR$/Spark_kNN/src\" />\n      <recent name=\"$PROJECT_DIR$/LocalRunFiles/src\" />\n      <recent name=\"$PROJECT_DIR$/Common/src\" />\n    </key>\n  </component>\n  <component name=\"RunManager\">\n    <configuration name=\"Scala REPL\" type=\"ScalaScriptConsoleRunConfiguration\" factoryName=\"Scala Console\" temporary=\"true\" show_console_on_std_err=\"false\" show_console_on_std_out=\"false\">\n      <module name=\"FW_Simba_Source\" />\n      <option name=\"allowRunningInParallel\" value=\"false\" />\n      <option name=\"javaOptions\" value=\"-Djline.terminal=NONE\" />\n      <option name=\"myConsoleArgs\" value=\"\" />\n      <option name=\"workingDirectory\" value=\"$PROJECT_DIR$\" />\n      <method v=\"2\">\n        <option name=\"Make\" enabled=\"true\" />\n      </method>\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Scala REPL.Scala REPL\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SvnConfiguration\">\n    <configuration />\n  </component>\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"ee761a9d-eff2-4086-b83a-8096819a4548\" name=\"Default Changelist\" comment=\"\" />\n      <created>1594702468860</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1594702468860</updated>\n    </task>\n    <servers />\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <ignored-roots>\n      <path value=\"$PROJECT_DIR$\" />\n    </ignored-roots>\n  </component>\n  <component name=\"WindowStateProjectService\">\n    <state x=\"705\" y=\"173\" width=\"1284\" height=\"1060\" key=\"#Project_Structure\" timestamp=\"1594703806160\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"705\" y=\"173\" width=\"1284\" height=\"1060\" key=\"#Project_Structure/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594703806160\" />\n    <state x=\"1018\" y=\"402\" width=\"575\" height=\"661\" key=\"#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog\" timestamp=\"1594703870846\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"1018\" y=\"402\" width=\"575\" height=\"661\" key=\"#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594703870846\" />\n    <state x=\"935\" y=\"476\" width=\"735\" height=\"513\" key=\"FileChooserDialogImpl\" timestamp=\"1594702737552\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"935\" y=\"476\" width=\"735\" height=\"513\" key=\"FileChooserDialogImpl/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594702737552\" />\n    <state x=\"1105\" y=\"653\" key=\"Github.CreateGistDialog\" timestamp=\"1594704073076\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"1105\" y=\"653\" key=\"Github.CreateGistDialog/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704073076\" />\n    <state x=\"1024\" y=\"448\" width=\"674\" height=\"328\" key=\"Github.ShareDialog\" timestamp=\"1594704214744\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"1024\" y=\"448\" width=\"674\" height=\"328\" key=\"Github.ShareDialog/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704214744\" />\n    <state x=\"937\" y=\"413\" width=\"743\" height=\"639\" key=\"Github.UntrackedFilesDialog\" timestamp=\"1594704224220\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"937\" y=\"413\" width=\"743\" height=\"639\" key=\"Github.UntrackedFilesDialog/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704224220\" />\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.bottom\" timestamp=\"1594704278772\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.bottom/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704278772\" />\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.center\" timestamp=\"1594704278772\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.center/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704278772\" />\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.left\" timestamp=\"1594704278771\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.left/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704278771\" />\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.right\" timestamp=\"1594704278772\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state width=\"2447\" height=\"397\" key=\"GridCell.Tab.0.right/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704278772\" />\n    <state x=\"802\" y=\"367\" key=\"SettingsEditor\" timestamp=\"1594704421418\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"802\" y=\"367\" key=\"SettingsEditor/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594704421418\" />\n    <state x=\"622\" y=\"459\" width=\"1372\" height=\"547\" key=\"new project wizard\" timestamp=\"1594703860458\">\n      <screen x=\"67\" y=\"27\" width=\"2493\" height=\"1413\" />\n    </state>\n    <state x=\"622\" y=\"459\" width=\"1372\" height=\"547\" key=\"new project wizard/67.27.2493.1413@67.27.2493.1413\" timestamp=\"1594703860458\" />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/workspace.xml	(revision f45e12eed633286d06e7b11ba95d0a67ecabbe0f)
+++ .idea/workspace.xml	(date 1595050493922)
@@ -1,7 +1,118 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
+  <component name="BranchesTreeState">
+    <expand>
+      <path>
+        <item name="ROOT" type="e8cecc67:BranchNodeDescriptor" />
+        <item name="LOCAL_ROOT" type="e8cecc67:BranchNodeDescriptor" />
+      </path>
+      <path>
+        <item name="ROOT" type="e8cecc67:BranchNodeDescriptor" />
+        <item name="REMOTE_ROOT" type="e8cecc67:BranchNodeDescriptor" />
+      </path>
+      <path>
+        <item name="ROOT" type="e8cecc67:BranchNodeDescriptor" />
+        <item name="REMOTE_ROOT" type="e8cecc67:BranchNodeDescriptor" />
+        <item name="GROUP_NODE:origin" type="e8cecc67:BranchNodeDescriptor" />
+      </path>
+    </expand>
+    <select />
+  </component>
   <component name="ChangeListManager">
-    <list default="true" id="ee761a9d-eff2-4086-b83a-8096819a4548" name="Default Changelist" comment="" />
+    <list default="true" id="ee761a9d-eff2-4086-b83a-8096819a4548" name="Default Changelist" comment="Initial Commit">
+      <change afterPath="$PROJECT_DIR$/.idea/scala_settings.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/uiDesigner.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/HelperObjects.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/FW_Simba/src/test/scala/org/cusp/bdi/fw/simba/SIM_Example.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/SpatialBenchmark.iml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/pom.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/MatchDistAnalysis.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/OutputsComapre.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/SpatialBenchmark.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/HelperObjects.scala" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SpatialBenchmark/src/main/scala/org/cusp/bdi/sb/examples/SB_KeyMatch_InputFileParser.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/Research.iml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/Research.iml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/compiler.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/compiler.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/misc.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/modules.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/modules.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml" beforeDir="false" afterPath="$PROJECT_DIR$/FW_GeoMatch/FW_GeoMatch.iml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/FW_GeoMatch/src/main/scala/GitHub_Example.scala" beforeDir="false" afterPath="$PROJECT_DIR$/FW_GeoMatch/src/main/scala/GitHub_Example.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/FW_GeoMatch/src/main/scala/org/cusp/bdi/gm/examples/GM_Example.scala" beforeDir="false" afterPath="$PROJECT_DIR$/FW_GeoMatch/src/main/scala/org/cusp/bdi/gm/examples/GM_Example.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/LocalRunFiles/src/main/scala/org/cusp/bdi/util/LocalRunConsts.scala" beforeDir="false" afterPath="$PROJECT_DIR$/LocalRunFiles/src/main/scala/org/cusp/bdi/util/LocalRunConsts.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/LocalRunFiles/target/classes/org/cusp/bdi/util/LocalRunConsts$.class" beforeDir="false" afterPath="$PROJECT_DIR$/LocalRunFiles/target/classes/org/cusp/bdi/util/LocalRunConsts$.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/util/AssignToPartitions.scala" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/util/AssignToPartitions.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/src/test/scala/org/cusp/bdi/sknn/test/TestAllKNN.scala" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/src/test/scala/org/cusp/bdi/sknn/test/TestAllKNN.scala" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/QTPartId$.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/QTPartId.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anon$1.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anon$1.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$1.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$1.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$10.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$10.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$11.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$11.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$12.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$12.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$13.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$13.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$14.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$14.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$15.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$15.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$16$$anonfun$apply$9.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$16.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$16.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$17$$anonfun$apply$1.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$17$$anonfun$apply$2.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$17$$anonfun$apply$3.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$17$$anonfun$apply$4.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$17.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$2.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$2.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$23$$anonfun$apply$13$$anonfun$apply$14.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$23$$anonfun$apply$13.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$23.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$23.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$24.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$24.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$25.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$26$$anonfun$27.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$26.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$26.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$28.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$28.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$29.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$29.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$3$$anonfun$apply$5.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$3$$anonfun$apply$5.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$3.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$3.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$30$$anonfun$apply$15.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$30.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$30.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$31.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$32.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$4$$anonfun$5.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$4.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$4.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6$$anonfun$apply$6$$anonfun$apply$7.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6$$anonfun$apply$6.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6$$anonfun$apply$6.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6$$anonfun$apply$8.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$6.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$7.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$7.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$8.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$9.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$9.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anon$2.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$18$$anonfun$apply$10.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$18.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$18.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19$$anonfun$apply$11$$anonfun$20.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19$$anonfun$apply$11$$anonfun$21.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19$$anonfun$apply$11$$anonfun$22.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19$$anonfun$apply$11.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19$$anonfun$apply$12.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1$$anonfun$19.class" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$1.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$2.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$$anonfun$knnJoin$2.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN$.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/SparkKNN.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$1.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$1.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$2.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$2.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$3.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$3.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$4.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$4.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$5.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$5.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$6.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$$anonfun$6.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions$.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/classes/org/cusp/bdi/sknn/util/AssignToPartitions.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1$$anonfun$apply$3$$anonfun$apply$4.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1$$anonfun$apply$3$$anonfun$apply$4.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1$$anonfun$apply$3.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1$$anonfun$apply$3.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$$anonfun$main$1.class" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$.class" beforeDir="false" afterPath="$PROJECT_DIR$/Spark_kNN/target/test-classes/org/cusp/bdi/sknn/test/TestAllKnnJoin$.class" afterDir="false" />
+    </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
@@ -11,6 +122,11 @@
     <option name="done" value="true" />
   </component>
   <component name="Git.Settings">
+    <option name="RECENT_BRANCH_BY_REPOSITORY">
+      <map>
+        <entry key="$PROJECT_DIR$" value="master" />
+      </map>
+    </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
   <component name="ProjectCodeStyleSettingsMigration">
@@ -29,22 +145,64 @@
   <component name="PropertiesComponent">
     <property name="RunOnceActivity.OpenProjectViewOnStart" value="true" />
     <property name="RunOnceActivity.ShowReadmeOnStart" value="true" />
-    <property name="last_opened_file_path" value="$PROJECT_DIR$/FW_GeoMatch/src" />
+    <property name="SHARE_PROJECT_CONFIGURATION_FILES" value="true" />
+    <property name="last_opened_file_path" value="$PROJECT_DIR$/SpatialBenchmark/src" />
     <property name="project.structure.last.edited" value="Modules" />
     <property name="project.structure.proportion" value="0.15" />
     <property name="project.structure.side.proportion" value="0.34280476" />
-    <property name="settings.editor.selected.configurable" value="project.propVCSSupport.Mappings" />
+    <property name="scala_project_settings_configurable.last_selected_tab_index" value="0" />
+    <property name="settings.editor.selected.configurable" value="preferences.keymap" />
   </component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
+      <recent name="$PROJECT_DIR$/SpatialBenchmark/src" />
+      <recent name="$PROJECT_DIR$/FW_Simba/src/test/scala" />
       <recent name="$PROJECT_DIR$/FW_GeoMatch/src" />
       <recent name="$PROJECT_DIR$/GeoMatch/src" />
       <recent name="$PROJECT_DIR$/Spark_kNN/src" />
-      <recent name="$PROJECT_DIR$/LocalRunFiles/src" />
-      <recent name="$PROJECT_DIR$/Common/src" />
     </key>
   </component>
-  <component name="RunManager">
+  <component name="RunManager" selected="Application.SpatialBenchmark">
+    <configuration name="SIM_Example" type="Application" factoryName="Application" temporary="true">
+      <option name="MAIN_CLASS_NAME" value="org.cusp.bdi.fw.simba.SIM_Example" />
+      <module name="FW_Simba" />
+      <extension name="coverage">
+        <pattern>
+          <option name="PATTERN" value="org.cusp.bdi.fw.simba.*" />
+          <option name="ENABLED" value="true" />
+        </pattern>
+      </extension>
+      <method v="2">
+        <option name="Make" enabled="true" />
+      </method>
+    </configuration>
+    <configuration name="SpatialBenchmark" type="Application" factoryName="Application" temporary="true">
+      <option name="MAIN_CLASS_NAME" value="org.cusp.bdi.sb.SpatialBenchmark" />
+      <module name="SpatialBenchmark" />
+      <extension name="coverage">
+        <pattern>
+          <option name="PATTERN" value="org.cusp.bdi.sb.*" />
+          <option name="ENABLED" value="true" />
+        </pattern>
+      </extension>
+      <method v="2">
+        <option name="Make" enabled="true" />
+      </method>
+    </configuration>
+    <configuration name="TestAllKnnJoin" type="Application" factoryName="Application" temporary="true">
+      <output_file path="/media/ayman/Data/GeoMatch_Files/OutputFiles/intelliJ_output_TestAllKnnJoin" is_save="true" />
+      <option name="MAIN_CLASS_NAME" value="org.cusp.bdi.sknn.test.TestAllKnnJoin" />
+      <module name="Spark_kNN" />
+      <extension name="coverage">
+        <pattern>
+          <option name="PATTERN" value="org.cusp.bdi.sknn.test.*" />
+          <option name="ENABLED" value="true" />
+        </pattern>
+      </extension>
+      <method v="2">
+        <option name="Make" enabled="true" />
+      </method>
+    </configuration>
     <configuration name="Scala REPL" type="ScalaScriptConsoleRunConfiguration" factoryName="Scala Console" temporary="true" show_console_on_std_err="false" show_console_on_std_out="false">
       <module name="FW_Simba_Source" />
       <option name="allowRunningInParallel" value="false" />
@@ -55,12 +213,28 @@
         <option name="Make" enabled="true" />
       </method>
     </configuration>
+    <list>
+      <item itemvalue="Application.SIM_Example" />
+      <item itemvalue="Application.SpatialBenchmark" />
+      <item itemvalue="Application.TestAllKnnJoin" />
+      <item itemvalue="Scala REPL.Scala REPL" />
+    </list>
     <recent_temporary>
       <list>
+        <item itemvalue="Application.SpatialBenchmark" />
+        <item itemvalue="Application.TestAllKnnJoin" />
         <item itemvalue="Scala REPL.Scala REPL" />
+        <item itemvalue="Application.SIM_Example" />
       </list>
     </recent_temporary>
   </component>
+  <component name="ScalaProjectSettings">
+    <option name="intInjectionMapping">
+      <map>
+        <entry key="xml" value="XML" />
+      </map>
+    </option>
+  </component>
   <component name="SvnConfiguration">
     <configuration />
   </component>
@@ -74,59 +248,158 @@
     </task>
     <servers />
   </component>
+  <component name="Vcs.Log.Tabs.Properties">
+    <option name="TAB_STATES">
+      <map>
+        <entry key="MAIN">
+          <value>
+            <State>
+              <option name="CUSTOM_BOOLEAN_PROPERTIES">
+                <map>
+                  <entry key="Show.Git.Branches" value="false" />
+                </map>
+              </option>
+            </State>
+          </value>
+        </entry>
+      </map>
+    </option>
+  </component>
   <component name="VcsManagerConfiguration">
     <ignored-roots>
       <path value="$PROJECT_DIR$" />
     </ignored-roots>
+    <MESSAGE value="Initial Commit" />
+    <option name="LAST_COMMIT_MESSAGE" value="Initial Commit" />
   </component>
   <component name="WindowStateProjectService">
-    <state x="705" y="173" width="1284" height="1060" key="#Project_Structure" timestamp="1594703806160">
+    <state x="1183" y="599" width="811" height="531" key="#Notifications" timestamp="1594705139893">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="1183" y="599" width="811" height="531" key="#Notifications/67.27.2493.1413@67.27.2493.1413" timestamp="1594705139893" />
+    <state x="705" y="173" width="1284" height="1060" key="#Project_Structure" timestamp="1594757152304">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="705" y="173" width="1284" height="1060" key="#Project_Structure/67.27.2493.1413@67.27.2493.1413" timestamp="1594757152304" />
+    <state x="771" y="390" key="#com.intellij.execution.impl.EditConfigurationsDialog" timestamp="1594792004747">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="771" y="390" key="#com.intellij.execution.impl.EditConfigurationsDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594792004747" />
+    <state x="1029" y="649" width="559" height="167" key="#com.intellij.framework.addSupport.AddSupportForSingleFrameworkDialog" timestamp="1594705151507">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="1029" y="649" width="559" height="167" key="#com.intellij.framework.addSupport.AddSupportForSingleFrameworkDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594705151507" />
+    <state x="1121" y="426" width="374" height="613" key="#com.intellij.ide.util.MemberChooser" timestamp="1594875751099">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="1121" y="426" width="374" height="613" key="#com.intellij.ide.util.MemberChooser/67.27.2493.1413@67.27.2493.1413" timestamp="1594875751099" />
+    <state x="1018" y="402" width="575" height="661" key="#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog" timestamp="1594757013387">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="705" y="173" width="1284" height="1060" key="#Project_Structure/67.27.2493.1413@67.27.2493.1413" timestamp="1594703806160" />
-    <state x="1018" y="402" width="575" height="661" key="#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog" timestamp="1594703870846">
+    <state x="1018" y="402" width="575" height="661" key="#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594757013387" />
+    <state width="533" height="506" key="DebuggerActiveHint" timestamp="1595046050903">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="1018" y="402" width="575" height="661" key="#com.intellij.ide.util.frameworkSupport.AddFrameworkSupportDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594703870846" />
-    <state x="935" y="476" width="735" height="513" key="FileChooserDialogImpl" timestamp="1594702737552">
+    <state width="533" height="506" key="DebuggerActiveHint/67.27.2493.1413@67.27.2493.1413" timestamp="1595046050903" />
+    <state x="935" y="476" width="735" height="513" key="FileChooserDialogImpl" timestamp="1594705267447">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="935" y="476" width="735" height="513" key="FileChooserDialogImpl/67.27.2493.1413@67.27.2493.1413" timestamp="1594702737552" />
+    <state x="935" y="476" width="735" height="513" key="FileChooserDialogImpl/67.27.2493.1413@67.27.2493.1413" timestamp="1594705267447" />
     <state x="1105" y="653" key="Github.CreateGistDialog" timestamp="1594704073076">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
     <state x="1105" y="653" key="Github.CreateGistDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594704073076" />
-    <state x="1024" y="448" width="674" height="328" key="Github.ShareDialog" timestamp="1594704214744">
+    <state x="1024" y="448" width="674" height="328" key="Github.ShareDialog" timestamp="1594704480708">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="1024" y="448" width="674" height="328" key="Github.ShareDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594704480708" />
+    <state x="937" y="413" width="743" height="639" key="Github.UntrackedFilesDialog" timestamp="1594704490168">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="937" y="413" width="743" height="639" key="Github.UntrackedFilesDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594704490168" />
+    <state width="2447" height="396" key="GridCell.Tab.0.bottom" timestamp="1595050233951">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state width="2447" height="396" key="GridCell.Tab.0.bottom/67.27.2493.1413@67.27.2493.1413" timestamp="1595050233951" />
+    <state width="2447" height="396" key="GridCell.Tab.0.center" timestamp="1595050233949">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state width="2447" height="396" key="GridCell.Tab.0.center/67.27.2493.1413@67.27.2493.1413" timestamp="1595050233949" />
+    <state width="2447" height="396" key="GridCell.Tab.0.left" timestamp="1595050233948">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state width="2447" height="396" key="GridCell.Tab.0.left/67.27.2493.1413@67.27.2493.1413" timestamp="1595050233948" />
+    <state width="2447" height="396" key="GridCell.Tab.0.right" timestamp="1595050233950">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state width="2447" height="396" key="GridCell.Tab.0.right/67.27.2493.1413@67.27.2493.1413" timestamp="1595050233950" />
+    <state width="2447" height="487" key="GridCell.Tab.1.bottom" timestamp="1595046541444">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state width="2447" height="487" key="GridCell.Tab.1.bottom/67.27.2493.1413@67.27.2493.1413" timestamp="1595046541444" />
+    <state width="2447" height="487" key="GridCell.Tab.1.center" timestamp="1595046541443">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="1024" y="448" width="674" height="328" key="Github.ShareDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594704214744" />
-    <state x="937" y="413" width="743" height="639" key="Github.UntrackedFilesDialog" timestamp="1594704224220">
+    <state width="2447" height="487" key="GridCell.Tab.1.center/67.27.2493.1413@67.27.2493.1413" timestamp="1595046541443" />
+    <state width="2447" height="487" key="GridCell.Tab.1.left" timestamp="1595046541442">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="937" y="413" width="743" height="639" key="Github.UntrackedFilesDialog/67.27.2493.1413@67.27.2493.1413" timestamp="1594704224220" />
-    <state width="2447" height="397" key="GridCell.Tab.0.bottom" timestamp="1594704278772">
+    <state width="2447" height="487" key="GridCell.Tab.1.left/67.27.2493.1413@67.27.2493.1413" timestamp="1595046541442" />
+    <state width="2447" height="487" key="GridCell.Tab.1.right" timestamp="1595046541443">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state width="2447" height="397" key="GridCell.Tab.0.bottom/67.27.2493.1413@67.27.2493.1413" timestamp="1594704278772" />
-    <state width="2447" height="397" key="GridCell.Tab.0.center" timestamp="1594704278772">
+    <state width="2447" height="487" key="GridCell.Tab.1.right/67.27.2493.1413@67.27.2493.1413" timestamp="1595046541443" />
+    <state x="899" y="479" width="819" height="507" key="NewModule_or_Project.wizard" timestamp="1594705270398">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state width="2447" height="397" key="GridCell.Tab.0.center/67.27.2493.1413@67.27.2493.1413" timestamp="1594704278772" />
-    <state width="2447" height="397" key="GridCell.Tab.0.left" timestamp="1594704278771">
+    <state x="899" y="479" width="819" height="507" key="NewModule_or_Project.wizard/67.27.2493.1413@67.27.2493.1413" timestamp="1594705270398" />
+    <state x="152" y="296" width="1410" height="995" key="SettingsEditor" timestamp="1594882954495">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state width="2447" height="397" key="GridCell.Tab.0.left/67.27.2493.1413@67.27.2493.1413" timestamp="1594704278771" />
-    <state width="2447" height="397" key="GridCell.Tab.0.right" timestamp="1594704278772">
+    <state x="152" y="296" width="1410" height="995" key="SettingsEditor/67.27.2493.1413@67.27.2493.1413" timestamp="1594882954495" />
+    <state x="908" y="468" width="800" height="528" key="Vcs.Push.Dialog.v2" timestamp="1595050193121">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state width="2447" height="397" key="GridCell.Tab.0.right/67.27.2493.1413@67.27.2493.1413" timestamp="1594704278772" />
-    <state x="802" y="367" key="SettingsEditor" timestamp="1594704421418">
+    <state x="908" y="468" width="800" height="528" key="Vcs.Push.Dialog.v2/67.27.2493.1413@67.27.2493.1413" timestamp="1595050193121" />
+    <state x="167" y="125" width="2293" height="1215" key="dock-window-1" timestamp="1594783317173">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="802" y="367" key="SettingsEditor/67.27.2493.1413@67.27.2493.1413" timestamp="1594704421418" />
-    <state x="622" y="459" width="1372" height="547" key="new project wizard" timestamp="1594703860458">
+    <state x="167" y="125" width="2293" height="1215" key="dock-window-1/67.27.2493.1413@67.27.2493.1413" timestamp="1594783317173" />
+    <state x="292" y="314" width="2293" height="1215" key="dock-window-2" timestamp="1594783317173">
       <screen x="67" y="27" width="2493" height="1413" />
     </state>
-    <state x="622" y="459" width="1372" height="547" key="new project wizard/67.27.2493.1413@67.27.2493.1413" timestamp="1594703860458" />
+    <state x="292" y="314" width="2293" height="1215" key="dock-window-2/67.27.2493.1413@67.27.2493.1413" timestamp="1594783317173" />
+    <state x="974" y="339" width="1109" height="836" key="find.popup" timestamp="1594884004409">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="974" y="339" width="1109" height="836" key="find.popup/67.27.2493.1413@67.27.2493.1413" timestamp="1594884004409" />
+    <state x="622" y="459" width="1372" height="547" key="new project wizard" timestamp="1594757004227">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="622" y="459" width="1372" height="547" key="new project wizard/67.27.2493.1413@67.27.2493.1413" timestamp="1594757004227" />
+    <state x="977" y="343" key="run.anything.popup" timestamp="1595028742522">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="977" y="343" key="run.anything.popup/67.27.2493.1413@67.27.2493.1413" timestamp="1595028742522" />
+    <state x="976" y="347" width="672" height="678" key="search.everywhere.popup" timestamp="1595043495899">
+      <screen x="67" y="27" width="2493" height="1413" />
+    </state>
+    <state x="976" y="347" width="672" height="678" key="search.everywhere.popup/67.27.2493.1413@67.27.2493.1413" timestamp="1595043495899" />
+  </component>
+  <component name="XDebuggerManager">
+    <breakpoint-manager>
+      <breakpoints>
+        <line-breakpoint enabled="true" type="scala-line">
+          <url>file://$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala</url>
+          <line>254</line>
+          <option name="timeStamp" value="20" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" type="scala-line">
+          <url>file://$PROJECT_DIR$/Spark_kNN/src/main/scala/org/cusp/bdi/sknn/SparkKNN.scala</url>
+          <line>474</line>
+          <option name="timeStamp" value="22" />
+        </line-breakpoint>
+      </breakpoints>
+    </breakpoint-manager>
   </component>
 </project>
\ No newline at end of file
